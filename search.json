[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis for Economists",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "1  Time Series Data",
    "section": "",
    "text": "Raw data: The raw values without any transformation. We are not so interested in the raw data, as it is hard to read information from it. Take the GDP plot as an example (Figure 1.1, upper-left subplot). There is an overall upward trend. But we are more interested in: how much does the economy grow this year? Is it better or worse than last year? The answers are not obvious from the raw data. Besides, there are obvious seasonal fluctuations. Usually the first quarter has the lowest value in a whole year, due to the Spring Festival, which significantly reduces the working days in the first quarter. The seasonal fluctuations prohibit us from sensibly comparing two consecutive values.\nGrowth rate: The headline GDP growth is usually derived by comparing the current quarter with the same quarter from last year. \\(g=\\frac{x_t - x_{t-4}}{x_{t-4}}\\times 100.\\) This makes sense. As mentioned above, due to seasonal patterns, comparing two consecutive quarters directly does not make sense. The year-on-year growth rate directly tells us how fast the economy grows. However, by dividing the past values, it loses the absolute level information. For instance, it is hard to tell after the pandemic, whether or not the economy recovers from its pre-pandemic output level. Besides, it is sensitive to the values of last year. For example, due to the pandemic, the GDP for 2020 is exceptionally low, which makes growth rate for 2021 exceptionally high. This is undesirable, because it does not mean the economy in 2021 is actually good. We would like a growth rate that shirks off past burdens.\nThat’s why we sometimes prefer (annualized) quarterly growth rate. \\(g=\\frac{x_t-x_{t-1}}{x_{t-1}}\\times 400.\\) Due to seasonally patterns, two consecutive quarters are not comparable directly. A first quarter value is usually much lower than the fourth quarter of last year due to holidays, which does not necessarily mean the economy condition is getting worse. Since this pattern is the same every year, it is possible to remove the seasonal fluctuations. This is called seasonally adjustment. We won’t cover seasonally adjustment in detail, but the next section will give some intuitions on how this can possibly be done. After seasonally adjusting the time series, we can calculate the growth rate based on two consecutive values (annualized by multiplying \\(4\\)). The bottom-right panel of Figure 1.1 is the seasonally-adjusted quarterly growth. Note that it is no longer biased upward in 2021 as the YoY growth.\nSeasonally-adjusted series: This is usually the data format we prefer in time series analysis. FRED reports both seasonally-adjusted and non-seasonally-adjusted series. Seasonal adjustment algorithm is a science in itself. Popular algorithms include X-13-ARIMA developed by the United States Census Bureau, TRAMO/SEATS developed by the Bank of Spain, and so on.\n\n\n\n\n\nFigure 1.1: Quarterly GDP Time Series (Unit: RMB Billion or %)\n\n\n\n\nLog levels and log growth rates: We like to work with log levels. A lot of economic time series exhibit exponential growth, such as GDP. Taking logs convert them to linear. Another amazing thing about logs is the difference of two log values can be interpreted as percentage growth. We know from Taylor expansion that for small values of \\(\\Delta x\\) : \\(\\ln(\\Delta x +1) \\approx \\Delta x\\). Therefore,\n\\[\n\\ln x_t - \\ln x_{t-1} = \\ln\\left(\\frac{x_t}{x_{t-1}}\\right) = \\ln\\left(\\frac{x_t-x_{t-1}}{x_{t-1}}+1\\right) \\approx \\frac{x_t-x_{t-1}}{x_{t-1}}.\n\\]\nSo it is very handy to just difference the log levels to get the growth rates. Log difference can also be interpreted as the continuously compounded rate of change, if assuming\n\\[\n\\frac{x_t}{x_{t-1}}=e^g \\implies g = \\ln x_t - \\ln x_{t-1}.\n\\]\nLog difference also has the property of summability: summing up a series of log differences gives the log level provided the initial level. It is not as handy if you want to recover the level values from a series of percentage growth.\n\\[\n\\ln x_t = x_0 + \\sum_{j=1}^{t} (\\ln x_j - \\ln x_{j-1}).\n\\]\n\n\n\n\n\n\nTip\n\n\n\nBuying vs. renting a home, which is better? Compute the NPV:\n\\[\n\\text{NPV}=\\sum_{t=0}^T \\frac{C_t}{(1+r)^t}=\\int_0^T C(t) e^{-rt} dt.\n\\]"
  },
  {
    "objectID": "decomp.html#time-series-components",
    "href": "decomp.html#time-series-components",
    "title": "2  Decomposition",
    "section": "2.1 Time Series Components",
    "text": "2.1 Time Series Components\nIt is helpful to think about a time series as composed of different components: a trend component, a seasonal component, and a remainder.\n\\[x_t = T_t + S_t + R_t.\\]\nThe formula assumes the “additive” composition. This assumption is appropriate if the magnitude of the fluctuations does not vary with the absolute levels of the time series. If the magnitude of fluctuations is proportional to the absolute levels, a “multiplicative” decomposition is more appropriate:\n\\[\nx_t = T_t \\times S_t \\times R_t.\n\\]\nNote that a multiplicative decomposition of a time series is equivalent to an additive decomposition on its log levels:\n\\[\n\\ln x_t = \\ln T_t + \\ln S_t + \\ln R_t.\n\\]\nDecomposing a time series allows us to extract information that is not obvious from the original time series. It also allows us to manipulate the time series. For example, if the seasonal component can be estimated, we can remove it to obtain seasonally-adjusted series, \\(x_t^{SA} = x_t - S_t\\), or \\(x_t^{SA} = x_t/S_t\\). The question is how to estimate the components given a time series."
  },
  {
    "objectID": "decomp.html#moving-averages",
    "href": "decomp.html#moving-averages",
    "title": "2  Decomposition",
    "section": "2.2 Moving Averages",
    "text": "2.2 Moving Averages\nMoving averages turn out to be handy in estiming trend-cycles by averaging out noisy fluctuations. A moving average of order \\(m\\) (assuming \\(m\\) is an odd number) is defined as\n\\[\n\\text{MA}(x_t,m) = \\frac{1}{m}\\sum_{j=-k}^{k} x_{t+j},\n\\]\nwhere \\(m=2k + 1\\). For example, a moving average of order \\(3\\) is\n\\[\n\\text{MA}(x_t, 3) = \\frac{1}{3}(x_{t-1} + x_t + x_{t+1}).\n\\]\nNote that \\(x_t\\) is centered right in the middle and the average is symmetric. This also means, if we apply this formula to real data, the first and last observation will have to be discarded. If the order \\(m\\) is an even number, the formula will no longer be symmetric. To overcome this, we can estimate a moving average over another moving average. For example, we can estimate a moving average of order \\(4\\), followed by a moving average of order \\(2\\). This is denoted as \\(2 \\times 4\\)-MA. Mathematically,\n\\[\n\\begin{aligned}\n\\text{MA}(x_t, 2 \\times 4) &= \\frac{1}{2}[\\text{MA}(x_{t-1}, 4) + \\text{MA}(x_t, 4)] \\\\\n&= \\frac{1}{2}\\left[\\frac{1}{4}(x_{t-2} + x_{t-1} + x_t + x_{t+1}) + \\frac{1}{4}(x_{t-1} + x_t + x_{t+1} + x_{t+2})\\right] \\\\\n&= \\frac{1}{8}x_{t-2} + \\frac{1}{4}x_{t-1} + \\frac{1}{4}x_{t} + \\frac{1}{4}x_{t+1} + \\frac{1}{8}x_{t+2}.\n\\end{aligned}\n\\]\nNote that how the \\(2\\times4\\)-MA averages out the seasonality for time series with seasonal period \\(4\\), e.g. quarterly series. The formula puts equal weight on every quarter — the first and last terms refer the same quarter and their weights combined to \\(\\frac{1}{4}\\).\nIn general, we can use \\(m\\)-MA to estimate the trend if the seasonal period is an odd number, and use \\(2\\times m\\)-MA if the seasonal period is an even number.\n\ndata = readRDS(\"data/gdp.Rds\")  # a `zoo` object\ngdp2x4MA = ma(ma(data$GDP,4),2) # from `forecast` package\nts.plot(cbind(data$GDP, gdp2x4MA), col=1:2)\n\n\n\n\nFigure 2.1: Quarterly GDP with 2x4-MA estimate of the trend-cycle"
  },
  {
    "objectID": "decomp.html#classical-decomposition",
    "href": "decomp.html#classical-decomposition",
    "title": "2  Decomposition",
    "section": "2.3 Classical Decomposition",
    "text": "2.3 Classical Decomposition\nMoving averages give us everything we need to perform classical decomposition. Classical decomposition, invented 1920s, is the simplest method decompose a time series into trend, seasonality and remainder. It is outdated nowadays and has been replaced by more advanced algorithms. Nonetheless, it serves as a good example for introductory purpose on how time series decomposition could possibly be achieved.\nThe algorithm for additive decomposition is as follows.\n\nEstimate the trend component \\(T_t\\) by applying moving averages. If the seasonal period is an odd number, apply the \\(m\\)-th order MA. If the seasonal period is even, apply the \\(2\\times m\\) MA.\nCalculate the detrended series \\(x_t - T_t\\).\nCalculate the seasonal component \\(S_t\\) by averaging all the detrended values of the season. For example, for quarterly series, the value of \\(S_t\\) for Q1 would be the average of all values in Q1. This assumes the seasonal component is constant over time. \\(S_t\\) is then adjusted to ensure all values summed up to zero.\nSubtracting the seasonal component to get the remainder \\(R_t = x_t-T_t-S_t\\).\n\n\nlog(data$GDP) |> decompose() |> plot()\n\n\n\n\nFigure 2.2: Classical multiplicative decomposition of quarterly GDP\n\n\n\n\nThe example performs additive decomposition to the logged quarterly GDP series. Note how the constant seasonal component is removed, leaving the smooth and nice-looking up-growing trend. The remainder component tells us the irregular ups and downs of the economy around the trend-cycle. Isn’t it amazing that a simple decomposition of the time series tells us a lot about the economy?"
  },
  {
    "objectID": "decomp.html#seasonal-adjustment",
    "href": "decomp.html#seasonal-adjustment",
    "title": "2  Decomposition",
    "section": "2.4 Seasonal Adjustment",
    "text": "2.4 Seasonal Adjustment\nBy decomposing a time series into trend, seasonality and remainder, it readily gives us a method for seasonal adjustment. Simply subtracting the seasonal component from the original data, or equivalently, summing up the trend and the remainder components, would give us the seasonally-adjusted series.\nThe following example compares the seasonally-adjusted series using the classical decomposition with the state-of-the-art X-13ARIMA-SEATS algorithm. Despite the former is far more rudimentary than the latter, they look quite close if we simply eye-balling the plot. By taking first-order differences, we can see the series based on classical decomposition is more volatile, suggesting the classical decomposition is less robust to unusual values.\n\nlogdata = log(data) |> window(start=2000)\nseasadj = as.ts(logdata$GDP) - decompose(logdata$GDP)$seasonal\n\npar(mfrow=c(1,2), mar=rep(2,4))\nts.plot(cbind(seasadj, logdata$GDPSA), col=1:2)\nts.plot(diff(cbind(seasadj, logdata$GDPSA)), col=1:2)\n\n\n\n\nComparing classical decomposition and X-13"
  },
  {
    "objectID": "autocov.html#autocorrelation",
    "href": "autocov.html#autocorrelation",
    "title": "3  ACF and PACF",
    "section": "3.1 Autocorrelation",
    "text": "3.1 Autocorrelation\nThe temporal dependence is characterized by the correlation between \\(y_t\\) and its own lags \\(y_{t-k}\\).\n\nDefinition 3.1 The \\(k\\)-th order autocovariance of \\(y_t\\) is defined as\n\\[\\gamma_k = \\text{cov}(y_t, y_{t-k}).\\]\nThe \\(k\\)-th order autocorrelation is defined as\n\\[\\rho_k = \\frac{\\text{cov}(y_t, y_{t-k})}{\\text{var}(y_t)} = \\frac{\\gamma_k}{\\gamma_0}.\\]\n\nIf we plot the autocorrelation as a function of the lag length \\(k\\), we get the autocorrelation function (ACF). Here is an example of the ACF of China’s monthly export growth (log-difference). The lag on the horizontal axis is counted by seasonal period. Because it is monthly data, 1 period is 12 months. We can see the autocorrelation is the strongest for the first two lags. Longer lags are barely significant. There are spikes with 12-month and 24-month lags, indicating the seasonality is not fully removed from the series.\n\ndata = readRDS(\"data/md.Rds\")\nacf(data$Export, main='Autocorrelation')\n\n\n\n\nFigure 3.1: ACF for monthly export growth"
  },
  {
    "objectID": "autocov.html#partial-autocorrelation",
    "href": "autocov.html#partial-autocorrelation",
    "title": "3  ACF and PACF",
    "section": "3.2 Partial Autocorrelation",
    "text": "3.2 Partial Autocorrelation\nACF measures the correlation between \\(y_t\\) and \\(y_{t-k}\\) regardless of their relationships with the intermediate variables \\(y_{t-1},y_{t-2},\\dots,y_{t-k+1}\\). Even if \\(y_t\\) is only correlated with the first-order lag, it is automatically made correlated with the \\(k\\)-th order lag through intermediate variables. Sometime we are interested in the correlation between \\(y_t\\) and \\(y_{t-k}\\) partialling out the influence of intermediate variables.\n\nDefinition 3.2 The partial autocorrelation function (PACF) considers the correlation between the remaining parts in \\(y_t\\) and \\(y_{t-k}\\) after partialling out the intermediate effect of \\(y_{t-1},y_{t-2},\\dots,y_{t-k+1}\\).\n\\[\n\\phi_k = \\begin{cases}\n\\text{corr}(y_t, y_{t-1})=\\rho_{_1}, \\text{ if } k=1;\\\\\n\\text{corr}(r_{y_t|y_{t-1},\\dots,y_{t-k+1}}, r_{y_{t-k}|y_{t-1},\\dots,y_{t-k+1}}), \\text{ if } k\\geq 2;\n\\end{cases}\n\\]\nwhere \\(r_{y|x}\\) means the remainder in \\(y\\) after partialling out the intermediate effect of \\(x\\).\n\nIn practice, \\(\\phi_k\\) can be estimated by the regression\n\\[\ny_t = \\mu + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_k y_{t-k} + \\epsilon_t.\n\\]\nThe estimated coefficient \\(\\hat\\phi_k\\) is the partial autocorrelation after controlling the intermediate lags.\n\npacf(data$Export, main='Partial Autocorrelation')\n\n\n\n\nFigure 3.2: PACF for monthly export growth"
  },
  {
    "objectID": "stationary.html#stationary-process",
    "href": "stationary.html#stationary-process",
    "title": "4  Stationarity",
    "section": "4.1 Stationary Process",
    "text": "4.1 Stationary Process\n\nDefinition 4.1 A stochastic process is said to be strictly stationary if its properties are unaffected by a change of time origin. In other words, the joint distribution at any set of time is not affect by an arbitrary shift along the time axis.\n\n\nDefinition 4.2 A stochastic process is called covariance stationary (or weak stationary) if its means, variances, and covariances are independent of time. Formally, a process \\(\\{y_t\\}\\) is covariance stationary if for all \\(t\\) it holds that\n\n\\(\\mathbb{E}(y_t) = \\mu < \\infty\\);\n\\(\\text{var}(y_t) = \\gamma_{_0} < \\infty\\);\n\\(\\text{cov}(y_t,y_{t-k})=\\gamma_k\\), for \\(k=1,2,3,\\dots\\)\n\n\nStationarity is an important concept in time series analysis. It basically says the statistical properties of a time series are stable over time. Otherwise, if the statistical properties vary with time, statistics estimated from past values, such autocorrelations, would be much less meaningful. Strict stationarity requires the joint distribution being stable, that is moments of any order would be stable over time. In practice, mostly we only care about the first- and second-order moments, that is means and variances and covariances. Therefore, covariance stationary is sufficient.\nFigure 4.1 shows some examples of stationary and non-stationary time series. Only the first one is stationary (it is generated from \\(i.i.d\\) normal distribution). The second one is not stationary as its mean is not constant over time. The third one is not stationary as its variance is not constant. The last one is not stationary either, because its covariance is not constant.\n\n\n\n\n\nFigure 4.1: Stationary and non-stationary time series\n\n\n\n\nReal-life time series are rarely stationary. But they can be transformed to (quasi) stationary by differencing. Figure 4.2 shows some examples of the first-order (log) differences of real-life time series. They more or less exhibit some properties of stationarity, but not perfectly stationary. The series can be further “stationarized” by taking a second-order difference. But these examples are acceptable to be treated as stationary in our models. Even if they are not perfectly stationary, the model can be thought of being used to “extract” their stationary properties.\n\n\n\n\n\nFigure 4.2: Stationary and non-stationary time series (real life)\n\n\n\n\n\nProposition 4.1 For stationary series, it holds that \\(\\gamma_k = \\gamma_{-k}\\).\n\n\nProof. By definition,\n\\[\n\\gamma_k = \\mathbb{E}[(y_t-\\mu)(y_{t-k}-\\mu)],\n\\]\n\\[\n\\gamma_{-k} = \\mathbb{E}[(y_t-\\mu)(y_{t+k}-\\mu)].\n\\]\nSince \\(y_t\\) is stationary, \\(\\gamma_k\\) is invariant with time. Let \\(t'=t+k\\), we have\n\\[\n\\begin{aligned}\n\\gamma_{k} &= \\mathbb{E}[(y_{t'}-\\mu)(y_{t'-k}-\\mu)] \\\\\n&= \\mathbb{E}[(y_{t+k}-\\mu)(y_{t}-\\mu)] \\\\\n&= \\gamma_{-k}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "stationary.html#ergodicity",
    "href": "stationary.html#ergodicity",
    "title": "4  Stationarity",
    "section": "4.2 Ergodicity",
    "text": "4.2 Ergodicity\nTemporal dependence is an important feature of time series variables. This dependence is both a bless and a curve. Autocorrelation enables us to make predictions based on past experiences. However, as we will see in later chapters, it also invalidates theorems that usually require \\(iid\\) assumptions. Ideally, we would like the temporal dependence to be not too strong. This is the property of ergodicity.\n\nDefinition 4.3 A stationary process \\(\\{y_t\\}\\) is ergodic if\n\\[\n\\lim_{n\\to\\infty} |\\mathbb{E}[f(y_t...y_{t+k})g(y_{t+n}...y_{t+n+k})]|=|\\mathbb{E}[f(y_t...y_{t+k})]||\\mathbb{E}[g(y_{t+n}...y_{t+n+k})]|.\n\\]\n\nHeuristically, ergodicity means if two random variables are positioned far enough in the sequence, they become almost independent. In other words, ergodicity is a restriction on dependency. An ergodic process allows serial correlation, but the serial correlation disappears if the two observations are far apart. Ergodicity is important because as we will see in later chapters, the Law of Large Numbers or the Central Limit Theorem will not hold without it.\n\nTheorem 4.1 A stationary time series is ergodic if \\(\\sum_{k=0}^{\\infty} |\\gamma_k| < \\infty\\).\n\n\nProof. A rigorous proof is not necessary. It is enough to give an intuition why autocorrelation disappears for far apart variables. Note that \\(\\sum_{k=0}^{\\infty} |\\gamma_k|\\) is monotonic and increasing, it converges. Therefore, \\(\\gamma_k \\to 0\\) by Cauchy Criterion."
  },
  {
    "objectID": "stationary.html#white-noise",
    "href": "stationary.html#white-noise",
    "title": "4  Stationarity",
    "section": "4.3 White Noise",
    "text": "4.3 White Noise\nWhite noise is a special stationary process that is an important building block of many time series models.\n\nDefinition 4.4 A stochastic process \\(w_t\\) is called white noise if its has constant mean \\(0\\) and variance \\(\\sigma^2\\) and no serial correlation \\(\\text{cov}(w_t, w_{t-k})=0\\) for any \\(k \\neq 0\\). The white noise process is denoted as\n\\[\nw_t \\sim \\text{WN}(0, \\sigma^2).\n\\]\n\nThis is the weakest requirement for while noise. It only requires no serial correlation. We may impose further assumptions. If every \\(w_t\\) is independent, it becomes independent white noise \\(w_t \\sim \\perp\\text{WN}(0, \\sigma^2)\\). Independence does not imply identical distribution. If every \\(w_t\\) is independently and identically distributed, it is called \\(i.i.d\\) white noise, \\(w_t \\overset{iid}{\\sim} \\text{WN}(0, \\sigma^2)\\). If the distribution is normal, it becomes the most perfect white noise, that is \\(i.i.d\\) Gaussian white noise, \\(w_t \\overset{iid}{\\sim} N(0, \\sigma^2)\\). The first plot of Figure 4.1 is a demonstration of the \\(i.i.d\\) Gaussian white noise. In most cases, the weakest form of white noise is sufficient.\n\n\n\n\n\n\nExercise\n\n\n\nProve that a while noise process is stationary."
  },
  {
    "objectID": "models.html#classification",
    "href": "models.html#classification",
    "title": "5  Model vs Spec",
    "section": "5.1 Classification",
    "text": "5.1 Classification\nTime series models can be broadly sorted into four categories based on whether we are dealing with stationary or non-stationary time series, or whether the model involves only one variable or multiple variables.\n\nTime series model classification\n\n\n\nStationary\nNonstationary\n\n\n\n\nUnivariate\nARMA\nUnit root\n\n\nMultivariate\nVAR\nCointegration"
  },
  {
    "objectID": "models.html#model-vs-spec",
    "href": "models.html#model-vs-spec",
    "title": "5  Model vs Spec",
    "section": "5.2 Model vs Spec",
    "text": "5.2 Model vs Spec\nWe use the word “model” rather loosely in economics and econometrics. Anything that deals with the quantified relationships between variables can be called a model. A general equilibrium model is a model. A regression is also a model.\nTo make things less confusing, we would use the word “model” more restrictively in this chapter. We reserve the word model to those representing the data generating processes (DGPs). That is, when we write down a model in an equation, we literally mean it. If we say \\(y_t\\) follows an AR(1) model:\n\\[\n\\begin{aligned}\ny_t &= \\phi y_{t-1} + \\epsilon_t,\\\\\n\\epsilon_t &\\sim N(0,\\sigma^2).\n\\end{aligned}\n\\]\nWe literally mean \\(y_t\\) is determined by its previous value and an contemporary innovation drawn from a Gaussian distribution.\nA model is distinguished from a specification. Suppose \\(\\{y_t\\}\\) represent the GDP series, we can estimate a regression:\n\\[\ny_t = \\phi y_{t-1} + e_t\n\\]\nThis is a specification not a model. Because the DGP of GDP data is unknown, definitely not an AR(1). We can nontheless fit this spec with the data and get an estimated \\(\\hat\\phi\\). If \\(e_t\\) satisfies some nice properties, for example, uncorrelated with the regressor, then we know this \\(\\hat\\phi\\) is consistent.\nWhen we run regressions with real-life data, we are actually working with specifications. They are not the DGPs of the random variables. But they allow us to recover some useful information from the data when certain assumptions are met. Mostly we are interested in the relationships between variables. A specification describes this relationship, even though it does not describe the full DGP.\nThis chapter deals with models in the abstract sense. The next chapter will discuss how to fit a model or a spec with real data."
  },
  {
    "objectID": "ar.html#ar1-process",
    "href": "ar.html#ar1-process",
    "title": "6  AR Models",
    "section": "6.1 AR(1) Process",
    "text": "6.1 AR(1) Process\nWe start with the simplest time series model — autoregressive model, or AR model. The simplest from of AR model is AR(1), which involves only one lag,\n\\[\ny_t = \\mu + \\phi y_{t-1} + \\epsilon_t,\n\\tag{6.1}\\]\nwhere \\(\\epsilon_t \\sim \\text{WN}(0,\\sigma^2)\\). The model can be extended to include more lags. An AR(\\(p\\)) model is defined as\n\\[\ny_t = \\mu + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} +  \\epsilon_t.\n\\]\nWe focus on AR(1) first. The model states that the value of \\(y_t\\) is determined by a constant, its previous value, and a random innovation. We call the last term \\(\\epsilon_t\\) innovation, not an error term. It is not an error, it is a random contribution that is unknown until time \\(t\\). It should also not be confused with the so-called “structural shock”, which is attached with a structural meaning and will be discussed in later chapters.\nThe model is probabilistic, as oppose to deterministic, in the sense that some information is unknown or deliberately omitted, so that we do not know the deterministic outcome, but only a probability distribution.\n\n\n\n\n\n\nNote\n\n\n\nThink about tossing a coin: if every piece of information is incorporated in the model, including the initial speed and position, the air resistance, and so on; then we can figure out the exact outcome, whether the coin will land on its head or tail. But this is unrealistic. Omitting all these information, we can model the process as a Bernoulli distribution. The probability model will not give a deterministic outcome, but only a distribution with each possible value associated with a probability.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe assumption that a process is only determined by its past values and a white noise innovation seems very restrictive. But it is not. Think about the three assumptions for technical analysis of the stock market (there are still many investors believing this): (1) The market discounts everything, (2) prices move in trends and counter-trends, and (3) price action is repetitive, with certain patterns reoccurring. Effectively, it is saying we can predict the stock market by the past price patterns. If we were to write a model for the stock market based on these assumptions, AR(\\(p\\)) isn’t a bad choice at all.\n\n\nNote that the model can be rewritten as\n\\[\ny_t - \\frac{\\mu}{1-\\phi} = \\phi\\left(y_{t-1} - \\frac{\\mu}{1-\\phi}\\right) + \\epsilon_t,\n\\]\nassuming \\(\\phi\\neq 1\\). If we define \\(\\tilde{y_t}=y_t - \\frac{\\mu}{1-\\phi}\\), we can get rid of the constant term:\n\\[\n\\tilde{y}_t = \\phi\\tilde{y}_{t-1} + \\epsilon_t.\n\\tag{6.2}\\]\nIt can be easily shown, if \\(y_t\\) is stationary, \\(\\frac{\\mu}{1-\\phi}\\) is the stationary mean. Because this mechanical transformation can always be done to remove the constant. We can simply ignore the constant term without lost of generality.\n\n\n\n\n\n\nNote\n\n\n\nWorking with demeaned variables greatly simplify the notation. For example, assuming \\(\\mathbb{E}(y_t)=0\\), the variance is simply the second-order moment \\(\\mathbb{E}(y_t^2)\\); the covariance can be written as \\(\\mathbb{E}(y_ty_{t-k})\\).\n\n\nFor a constant-free AR(1) model, we can rewrite the model as follows:\n\\[\n\\begin{aligned}\ny_t &= \\phi y_{t-1} + \\epsilon_t \\\\\n&= \\phi( \\phi y_{t-2} + \\epsilon_{t-1}) + \\epsilon_t \\\\\n&= \\phi^2 y_{t-2} + \\phi\\epsilon_{t-1} + \\epsilon_t \\\\\n&= \\phi^2 (\\phi y_{t-3} + \\epsilon_{t-2}) + \\phi\\epsilon_{t-1} + \\epsilon_t \\\\\n&= \\phi^3 y_{t-3} + \\phi^2\\epsilon_{t-2} + \\phi\\epsilon_{t-1} + \\epsilon_t \\\\\n&\\;\\vdots \\\\\n&= \\phi^t y_0 + \\sum_{j=0}^{t-1} \\phi^j\\epsilon_{t-j} \\\\\n&= \\sum_{j=0}^{\\infty} \\phi^j\\epsilon_{t-j}.\n\\end{aligned}\n\\tag{6.3}\\]\nThe exercise shows an AR(1) process can be reduced to an MA process, which will be discussed in the next section. It says the value of \\(y_t\\) is determined by its initial value (if it has one) and the accumulated innovations in the past. It is our deeds in history that shapes our world today.\n\n\n\n\n\n\nNote\n\n\n\nThe property that an AR process can be rewritten as an infinite MA process with absolute summable coefficients \\(\\sum_{j=0}^{\\infty}|\\phi^j|<\\infty\\) is called causal. This must not be confused with the causal effect in econometrics (defines in the ceteris paribus sense). To avoid confusion, we avoid use this term as much as possible.\n\n\nNow we focus our attention on the critical parameter \\(\\phi\\). If \\(|\\phi|>1\\), the process is explosive. We are not interested in explosive processes. If a real-world time series grows exponentially, we take logarithm to transform it to linear. So in most of our discussions, we rule out the case of explosive behaviour.\nIf \\(|\\phi|<1\\), \\(\\phi^j\\to 0\\) as \\(j\\to\\infty\\). This means the influence of innovations far away in the past decays to zero. We will show that the series is stationary and ergodic.\nIf \\(|\\phi|=1\\), we have \\(y_t = \\sum_{j=0}^{\\infty} \\text{sgn}(\\phi)^j\\epsilon_{t-j} = \\sum_{j=0}^{\\infty}\\tilde{\\epsilon}_{t-j}\\). This means the influence of past innovations will not decay no matter how distant away they are. This is known as a unit root process, which will be covered in later chapters. But it is clear that the process is not stationary. Consider the variance of \\(y_t\\) conditioned on an initial value:\n\\[\n\\text{var}(y_t|y_0) = \\text{var}(\\sum_{j=0}^{t-1}\\epsilon_{t-j})=\\sum_{j=0}^{t-1}\\text{var}(\\epsilon_{t-j})=\\sum_{j=0}^{t-1}\\sigma^2=t\\sigma^2.\n\\]\nThe variance is increasing with time. It is not constant. Figure 6.1 simulates the AR(1) with \\(\\phi=0.5\\) and \\(\\phi=1\\) respectively.\n\ny = arima.sim(list(ar=0.5), n=1000)\nz = arima.sim(list(order=c(0,1,0)), n=1000)\nplot(cbind(y,z), plot.type=\"multiple\", nc=2, ann=F, \n     mar.multi=rep(2,4), oma.multi = rep(0,4))\n\n\n\n\nFigure 6.1: Simulation of AR(1) processes\n\n\n\n\n\nProposition 6.1 An AR(1) process with \\(|\\phi|<1\\) is covariance stationary.\n\n\nProof. Let’s compute the mean, variance and covariance for the AR(1) process.\n\\[\n\\mathbb{E}(y_t) = \\mathbb{E}\\left[\\sum_{j=0}^{\\infty} \\phi^j\\epsilon_{t-j}\\right] = \\sum_{j=0}^{\\infty} \\phi^j\\mathbb{E}[\\epsilon_{t-j}]=0.\n\\]\n\\[\n\\begin{aligned}\n\\text{var}(y_t)\n&= \\text{var}\\left[\\sum_{j=0}^{\\infty} \\phi^j\\epsilon_{t-j}\\right]\n= \\sum_{j=0}^{\\infty} \\phi^j\\text{var}[\\epsilon_{t-j}] \\\\\n&= \\sigma^2 \\sum_{j=0}^{\\infty} \\phi^j\n=\\frac{\\sigma^2}{1-\\phi}.\n\\end{aligned}\n\\]\nFor the covariances,\n\\[\n\\begin{aligned}\n\\gamma_1 &= \\mathbb{E}(y_ty_{t-1})\n= \\mathbb{E}((\\phi y_{t-1} + \\epsilon_t)y_{t-1}) \\\\\n&= \\mathbb{E}(\\phi y_{t-1}^2 + \\epsilon_t y_{t-1}) \\\\\n&= \\phi\\mathbb{E}(y_{t-1}^2) + 0 \\\\\n&= \\frac{\\phi\\sigma^2}{1-\\phi};\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\gamma_2 &= \\mathbb{E}(y_ty_{t-2})\n= \\mathbb{E}((\\phi y_{t-1} + \\epsilon_t)y_{t-2}) \\\\\n&= \\mathbb{E}(\\phi y_{t-1}y_{t-2} + \\epsilon_t y_{t-2}) \\\\\n&= \\phi\\mathbb{E}(y_{t-1}y_{t-2}) \\\\\n&= \\phi\\gamma_1 = \\frac{\\phi^2\\sigma^2}{1-\\phi};\\\\\n&\\;\\vdots \\\\\n\\gamma_j &= \\frac{\\phi^j\\sigma^2}{1-\\phi}.\n\\end{aligned}\n\\]\nAll of them are independent of time \\(t\\). By Definition 4.2, the process is covariance stationary.\n\nSo the ACF decays gradually as \\(\\phi^j \\to 0\\). What about the PACF? Estimating the PACF is equivalent to regressing \\(y_t\\) on its lags. Since there is only one lag, the PACF should have non-zero value only for the first lag, and zeros for all other lags.\n\npar(mfrow=c(1,2), mar=c(2,4,1,1))\nacf(y); pacf(y)\n\n\n\n\nFigure 6.2: ACF and PACF for AR(1) process"
  },
  {
    "objectID": "ar.html#lag-operator",
    "href": "ar.html#lag-operator",
    "title": "6  AR Models",
    "section": "6.2 Lag Operator",
    "text": "6.2 Lag Operator\nTo facilitate easy manipulation of lags, we introduce the lag operator:\n\\[\nLy_t = y_{t-1}.\n\\]\nThe AR(1) process can be written with the lag operator:\n\\[\ny_t = \\phi Ly_t+ \\epsilon_t \\implies (1-\\phi L)y_t = \\epsilon_t.\n\\]\nThe lag operator \\(L\\) can be manipulated just as polynomials. It looks weird, but it actually works. Do a few exercises to convince yourself.\n\\[\nL^2 y_t = L(Ly_t) = Ly_{t-1} = y_{t-2}.\n\\]\n\\[\n\\begin{aligned}\n(1-L)^2y_t &= (1-L)(y_t - y_{t-1}) \\\\\n&= (y_t-y_{t-1})-(y_{t-1}-y_{t-2}) \\\\\n&= y_t - 2y_{t-1} + y_{t-2} \\\\\n&=(1-2L+L^2)y_t.\n\\end{aligned}\n\\]\nWe can even inverse a lag polynomial (provided \\(|\\phi|<1\\)),\n\\[\n\\begin{aligned}\n(1-\\phi L)y_t &= \\epsilon_t \\\\\n\\implies y_t &= (1-\\phi L)^{-1}\\epsilon_t =\\sum_{j=0}^{\\infty} \\phi^j L^j \\epsilon_t = \\sum_{j=0}^{\\infty} \\phi^j \\epsilon_{t-j}.\n\\end{aligned}\n\\]\nWe reach the same conclusion as Equation 6.3 with the lag operator."
  },
  {
    "objectID": "ar.html#arp-process",
    "href": "ar.html#arp-process",
    "title": "6  AR Models",
    "section": "6.3 AR(p) Process",
    "text": "6.3 AR(p) Process\nWe now generalize the conclusions above to AR(\\(p\\)) processes. With the help of the lag operator, an AR(\\(p\\)) process can be written as\n\\[\n(1-\\phi_1 L-\\phi_2 L^2-\\dots-\\phi_p L^p) y_t = \\epsilon_t,\n\\]\nor even more parsimoniously,\n\\[\n\\phi(L)y_t = \\epsilon_t.\n\\]\nNote that we ignore the constant term, which can always be removed by redefine \\(\\tilde{y}_t = y_t - \\frac{\\mu}{1-\\phi_1-\\phi_2-\\dots-\\phi_p}\\).\nTo derive the MA representation, we need to figure out \\(\\phi^{-1}(L)\\). By the Fundamental Theorem of Algebra, we know the polynomial \\(\\phi(z)\\) has \\(p\\) roots in the complex space. So the lag polynomial can be factored as\n\\[\n(1-\\lambda_1L)(1-\\lambda_2L)\\dots(1-\\lambda_pL) y_t = \\epsilon_t,\n\\]\nwhere \\(z=\\lambda_i^{-1}\\) is the \\(i\\)-th root of \\(\\phi(z)\\). If the roots are outside the unit circle, \\(|\\lambda_i|<1\\) means each of the left hand terms is inversible.\n\\[\n\\begin{aligned}\ny_t &= \\frac{1}{(1-\\lambda_1L)(1-\\lambda_2L)\\dots(1-\\lambda_pL)}\\epsilon_t \\\\\n&= \\left(\\frac{c_1}{1-\\lambda_1L} + \\frac{c_2}{1-\\lambda_2L} + \\dots + \\frac{c_p}{1-\\lambda_pL}\\right)\\epsilon_t \\\\\n&= \\sum_{j=0}^{\\infty}(c_1\\lambda_1^j+c_2\\lambda_2^j+\\dots+c_p\\lambda_p^j)L^j\\epsilon_t \\\\\n&= \\sum_{j=0}^{\\infty} \\theta_j\\epsilon_{t-j}, \\text{ where }\\theta_j=c_1\\lambda_1^j+\\dots+c_p\\lambda_p^j.\n\\end{aligned}\n\\]\nIt follows that this process has constant mean and variance. For the covariances, given\n\\[\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t,\n\\]\nMultiply both sides by \\(y_t\\) and take expectation,\n\\[\n\\mathbb{E}[y_t^2] = \\phi_1 \\mathbb{E}[y_ty_{t-1}] + \\phi_2 \\mathbb{E}[y_ty_{t-2}] + \\dots + \\phi_p \\mathbb{E}[y_ty_{t-p}],\n\\]\n\\[\n\\gamma_0 = \\phi_1\\gamma_{-1} + \\phi_2\\gamma_{-2} + \\dots + \\phi_p\\gamma_{-p}.\n\\]\nSimilarly, multiply both sides by \\(y_{t-1},\\dots,y_{t-j}\\), we have\n\\[\n\\begin{aligned}\n\\gamma_1 &= \\phi_1\\gamma_{0} + \\phi_2\\gamma_{-1} + \\dots + \\phi_p\\gamma_{-p+1},\\\\\n&\\;\\vdots\\\\\n\\gamma_j &= \\phi_1\\gamma_{j-1} + \\phi_2\\gamma_{j-2} + \\dots + \\phi_p\\gamma_{j-p}.\n\\end{aligned}\n\\]\nThis is called the Yule-Walker equation. The first \\(p\\) unknowns \\(\\gamma_0,\\dots,\\gamma_{p-1}\\) can be solved by the first \\(p\\) equations. The rest can then be solved iteratively.\nIt can be shown all of the covariances are invariant with time. Therefore, under the condition all \\(|\\lambda_i|<1\\), the AR(\\(p\\)) process is stationary.\nFor the PACF, a regression of \\(y_t\\) over its lags would recover \\(p\\) non-zero coefficients. Longer lags should have coefficients insignificantly different from zero.\n\ny = arima.sim(list(ar=c(2.4, -1.91, 0.5)), n=3000)\npar(mfrow=c(1,2), mar=c(2,4,1,1))\nacf(y); pacf(y)\n\n\n\n\nFigure 6.3: ACF and PACF for AR(p) process\n\n\n\n\n\nProposition 6.2 An AR(\\(p\\)) process is stationary if all the roots of \\(\\phi(z)\\) are outside the unit circle.\n\n\nProposition 6.3 An AR(\\(p\\)) process is characterized by (i) an ACF that is infinite in extend but tails of gradually; and (ii) a PACF that is (close to) zero for lags after \\(p\\)."
  },
  {
    "objectID": "ma.html#ma1-process",
    "href": "ma.html#ma1-process",
    "title": "7  MA Models",
    "section": "7.1 MA(1) Process",
    "text": "7.1 MA(1) Process\nAgain, let’s start with the simplest moving average model. A first-order moving average process, or MA(1), is defined as\n\\[\ny_t = \\mu + \\epsilon_t + \\theta\\epsilon_{t-1},\n\\tag{7.1}\\]\nwhere \\(\\{\\epsilon_t\\} \\sim \\text{WN}(0, \\sigma^2)\\) are uncorrelated innovations. The MA model says the current value \\(y_t\\) is a moving average of past innovations (in MA(1), the weight on \\(\\epsilon_{t-1}\\) is \\(\\theta\\)). MA models directly relate the observable variable to past innovations. If we know the past innvation \\(\\epsilon_{t-j}\\), we can easily figure out its contribution to the outcome variable (unlike AR models where the effect of a past innovation is transmitted through \\(y_{t-j},\\dots,y_{t-1}\\)). So MA models are the preferred analytic tool in many applications, despite it looks odd from the eyes of regression modelers. You may wonder how it is possible to estimate such a model. We will put off the estimation techniques to the next chapter.\nIt is clear that \\(y_t\\) has a constant mean, \\(\\mathbb{E}(y_t) = \\mu\\). We can omit the constant if we work with the demeaned series \\(\\tilde{y}_t = y_t - \\mu\\). Without loss of generality, we assume for the rest \\(\\{y_t\\}\\) has zero mean, so the model is simplified as\n\\[\ny_t = \\epsilon_t + \\theta\\epsilon_{t-1}.\n\\tag{7.2}\\]\nLet’s compute its variance and covariances:\n\\[\n\\begin{aligned}\n\\gamma_0 &= \\text{var} (\\epsilon_t + \\theta\\epsilon_{t-1}) = \\text{var}(\\epsilon_t) + \\theta^2\\text{var}(\\epsilon_{t-1}) = (1+\\theta^2)\\sigma^2;\\\\\n\\gamma_1 &= \\text{cov}(y_t, y_{t-1}) = \\text{cov}(\\epsilon_{t} + \\theta\\epsilon_{t-1}, \\epsilon_{t-1} + \\theta\\epsilon_{t-2}) = \\text{cov} (\\theta\\epsilon_{t-1}, \\epsilon_{t-1} + \\theta\\epsilon_{t-2}) = \\theta\\sigma^2; \\\\\n\\gamma_2 &= \\text{cov}(y_t, y_{t-2}) = \\text{cov}(\\epsilon_{t} + \\theta\\epsilon_{t-1}, \\epsilon_{t-2} + \\theta\\epsilon_{t-3}) = 0; \\\\\n&\\vdots\\\\\n\\gamma_j &= 0 \\text{ for } |j|\\geq 2.\n\\end{aligned}\n\\]\nIt is clear that the MA(1) process is stationary. And the ACF cuts off after the first lag. Because more distant lags \\(y_{t-k}\\) are constituted by even more distant innovations \\(\\epsilon_{t-k}, \\epsilon_{t-k-1}, ...\\) which has no relevance for \\(y_t\\) given the MA(1) structure.\nWe have seen AR processes are equivalent to MA(\\(\\infty\\)) processes. Similar results hold for MA models. Rewrite the MA(1) process with the lag operator, assuming \\(|\\theta| < 1\\),\n\\[\ny_t = (1 + \\theta L) \\epsilon_t \\Leftrightarrow\n(1+\\theta L)^{-1} y_t = \\epsilon_t \\Leftrightarrow\n\\sum_{j=0}^{\\infty} (-\\theta)^j y_{t-j} = \\epsilon_t.\n\\]\nThat means an MA(1) is equivalent to an AR(\\(\\infty\\)) process if \\((1+\\theta L)\\) is invertible. This shows AR and MA are really the same family of models. The model AR or MA is chosen by parsimonious principle. For example, an AR model with many lags can possibly be modeled by a parsimonious MA model.\nSince an MA(1) is equivalent to some AR(\\(\\infty\\)) process, the PACF of an MA(1) should tail off gradually.\n\ny = arima.sim(list(ma=0.8), n=2000)\npar(mfrow=c(1,2), mar=c(1,4,1,1))\nacf(y); pacf(y)\n\n\n\n\nFigure 7.1: ACF and PACF of MA(1) process\n\n\n\n\n\n\n\n\n\n\nInvertibility\n\n\n\nIf \\(|\\theta|>1\\), \\(\\theta(L)\\) is not invertible. Define another MA(1) process,\n\\[\ny_t = \\epsilon_t + \\theta^{-1}\\epsilon_{t-1},\\quad\\epsilon_t\\sim\\text{WN}(0,\\theta^2\\sigma^2).\n\\]\nWe can verify that its variance and covariances are exactly the same as Equation 7.2. For non-invertible MA process, as long as \\(\\theta(L)\\) avoids unit root, we can always find an invertible process that shares the same ACF. This means, for a stationary MA process, it makes no harm to just assume it is invertible."
  },
  {
    "objectID": "ma.html#maq-process",
    "href": "ma.html#maq-process",
    "title": "7  MA Models",
    "section": "7.2 MA(q) Process",
    "text": "7.2 MA(q) Process\nA \\(q\\)-th order moving average, or MA(\\(q\\)) process, is written as\n\\[\ny_t = \\mu + \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\dots + \\theta_q\\epsilon_{t-q},\n\\tag{7.3}\\]\nwhere \\(\\{\\epsilon_t\\} \\sim \\text{WN}(0, \\sigma^2)\\).\n\nProposition 7.1 An MA(\\(q\\)) process is stationary.\n\n\nProof. We will show that the mean, variance and covariances of MA(\\(q\\)) are all invariant with time.\n\\[\n\\mathbb{E}(y_t)= \\mu.\n\\]\nAssume for the rest, \\(\\{y_t\\}\\) is demeaned.\n\\[\n\\begin{aligned}\n\\gamma_0 &= \\mathbb{E}(y_t^2) = \\mathbb{E}[(\\epsilon_t + \\theta_1\\epsilon_{t-1} + \\dots + \\theta_q\\epsilon_{t-q})^2] \\\\\n&= \\mathbb{E}[\\epsilon^2] + \\theta_1^2\\mathbb{E}[\\epsilon_{t-1}^2] + \\dots + \\theta_q^2\\mathbb{E}[\\epsilon_{t-q}^2] \\\\\n&= (1+\\theta_1^2+\\dots+\\theta_q^2)\\sigma^2; \\\\\n\\gamma_1 &= \\mathbb{E}[y_ty_{t-1}] = \\mathbb{E}[(\\epsilon_t + \\theta_1\\epsilon_{t-1} + \\dots + \\theta_q\\epsilon_{t-q}) \\\\\n&\\hspace{10.2em} (\\epsilon_{t-1} + \\dots + \\theta_{q-1}\\epsilon_{t-q} + \\theta_q\\epsilon_{t-q-1})] \\\\\n&= \\theta_1\\mathbb{E}[\\epsilon_{t-1}^2] + \\theta_2\\theta_1\\mathbb{E}[\\epsilon_{t-2}^2] + \\dots + \\theta_q\\theta_{q-1}\\mathbb{E}[\\epsilon_{t-q}^2] \\\\\n&= (\\theta_1 + \\theta_2\\theta_1 + \\dots + \\theta_q\\theta_{q-1})\\sigma^2;\\\\\n&\\vdots\\\\\n\\gamma_j &= \\mathbb{E}[y_ty_{t-j}] = \\mathbb{E}[(\\epsilon_t + \\dots + \\theta_j\\epsilon_{t-j} + \\dots + \\theta_q\\epsilon_{t-q}) \\\\\n&\\hspace{12.5em} (\\epsilon_{t-j} + \\dots + \\theta_{q-j}\\epsilon_{t-q} + \\dots + \\theta_q\\epsilon_{t-q-j})] \\\\\n&= \\theta_j\\mathbb{E}[\\epsilon_{t-j}^2] + \\theta_{j+1}\\theta_1\\mathbb{E}[\\epsilon_{t-j-1}^2] + \\dots + \\theta_q\\theta_{q-j}\\mathbb{E}[\\epsilon_{t-q}^2] \\\\\n&= (\\theta_j + \\theta_{j+1}\\theta_1 + \\dots + \\theta_q\\theta_{q-j})\\sigma^2, \\text{ for } j\\leq q; \\\\\n\\gamma_j &= 0, \\text{ for } j > q.\n\\end{aligned}\n\\]\n\n\nProposition 7.2 An MA(\\(q\\)) process is invertible iff the roots of \\(\\theta(z)\\) are outside the unit circle.\n\n\nProposition 7.3 An MA(\\(q\\)) process is characterized by (i) an ACF that is (close to) zero after \\(q\\) lags; and (i) a PACF that is infinite in extend but tails of gradually."
  },
  {
    "objectID": "ma.html#ma-process",
    "href": "ma.html#ma-process",
    "title": "7  MA Models",
    "section": "7.3 MA(∞) Process",
    "text": "7.3 MA(∞) Process\nMA(\\(\\infty\\)) is a special case deserves attention. Partly because all ARMA processes can be reduced to MA(\\(\\infty\\)) processes. In addition to MA(\\(q\\)) processes, we need more conditions for MA(\\(\\infty\\)) to be stationary. Consider the variance of\n\\[\ny_t = \\sum_{j=0}^{\\infty}\\theta_j\\epsilon_{t-j},\n\\]\n\\[\n\\gamma_0 = \\mathbb{E}[y_t^2] = \\mathbb{E}\\left[\\left(\\sum_{j=0}^{\\infty}\\theta_j\\epsilon_{t-j}\\right)^2\\right] = \\left(\\sum_{j=0}^{\\infty}\\theta_j^2\\right)\\sigma^2.\n\\]\nIt only make sense if \\(\\sum_{j=0}^{\\infty}\\theta_j^2<\\infty\\). This property is called square summable.\n\nProposition 7.4 An MA(\\(\\infty\\)) process is stationary if the coefficients \\(\\{\\theta_j\\}\\) are square summable."
  },
  {
    "objectID": "arma.html#armapq",
    "href": "arma.html#armapq",
    "title": "8  ARMA Models",
    "section": "8.1 ARMA(p,q)",
    "text": "8.1 ARMA(p,q)\nARMA(\\(p\\), \\(q\\)) is a mixed autoregressive and moving average process.\n\\[\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} +\n\\epsilon_t + \\theta_1\\epsilon_{t-1} + \\dots + \\theta_q\\epsilon_{t-q},\n\\]\nor\n\\[\n\\phi(L) y_t = \\theta(L) \\epsilon_t,\n\\]\nwhere \\(\\{\\epsilon_{t}\\} \\sim \\text{WN}(0, \\sigma^2)\\).\nThe MA part is always stationary as shown in Proposition 7.1. The stationarity of an ARMA process solely depends on the AR part. The condition is the same as Proposition 6.2.\nAssume \\(\\phi^{-1}(L)\\) exist, then the ARMA(\\(p\\),\\(q\\)) process can be reduce to MA(\\(\\infty\\)) process:\n\\[\ny_t = \\phi^{-1}(L)\\theta(L)\\epsilon_t = \\psi(L) \\epsilon_t,\n\\]\nwhere \\(\\psi(L) = \\phi^{-1}(L)\\theta(L)\\).\n\n\n\n\n\n\nExercise\n\n\n\nCompute the MA equivalence for ARMA(1,1)."
  },
  {
    "objectID": "arma.html#arimapdq",
    "href": "arma.html#arimapdq",
    "title": "8  ARMA Models",
    "section": "8.2 ARIMA(p,d,q)",
    "text": "8.2 ARIMA(p,d,q)\nARMA(\\(p\\),\\(q\\)) is used to model stationary time series. If \\(y_t\\) is not stationary, we can transform it to stationary and model it with an ARMA model. If the first-order difference \\((1-L)y_t = y_t - y_{t-1}\\) is stationary, then we say \\(y_t\\) is integrated of order 1. If it requires \\(d\\)-th order difference to be stationary, \\((1-L)^dy_t\\), we say it is integrated of order \\(d\\). The ARMA model involves integrated time series is called ARIMA model:\n\\[\n\\phi(L)(1-L)^d y_t = \\theta(L)\\epsilon_t.\n\\]"
  },
  {
    "objectID": "wold.html#wold-decomposition",
    "href": "wold.html#wold-decomposition",
    "title": "9  Wold Theorem",
    "section": "9.1 Wold Decomposition",
    "text": "9.1 Wold Decomposition\nSo far we have spent a lot of effort with ARMA models, which are the indispensable components of any time series textbook. The following theorem justifies its importance. The Wold Decomposition Theorem basically says every covariance-stationary process has an ARMA representation. Therefore, with long enough lags, any covariance-stationary process can be approximated arbitrarily well by ARMA models. This is a very bold conclusion to make. It sets up the generality of ARMA models, which makes it one of the most important theorems in time series analysis.\n\nTheorem 9.1 (Wold Decomposition Theorem) Every covariance-stationary time series \\(y_t\\) can be written as the sum of two time series, one deterministic and one stochastic. Formally,\n\\[\ny_t = \\eta_t + \\sum_{j=0}^{\\infty} b_j\\epsilon_{t-j},\n\\]\nwhere \\(\\eta_t \\in I_{-\\infty}\\) is a deterministic time series (such as one represented by a sine wave); \\(\\epsilon_t\\) is an uncorrelated innovation sequence with \\(\\mathbb{E}[\\epsilon_t]=0\\), \\(\\mathbb{E}[\\epsilon_t\\epsilon_{t-j}]=0\\) for \\(j\\neq 0\\); and \\(\\{b_j\\}\\) are square summable, \\(\\sum_{j=0}^{\\infty}|b_j|^2<\\infty\\).\n\n\nProof. We will prove the theorem by constructing the innovation sequence \\(\\{e_t\\}\\) and showing it satisfies the conditions stated. Let \\(e_t = y_t - \\hat{\\mathbb{E}}(y_t|I_{t-1}) = y_t - a(L)y_{t-1}\\), where \\(\\hat{\\mathbb{E}}(y_t|I_{t-1})\\) is the best linear predictor (BLP) of \\(y_t\\) based on information set at \\(t-1\\). \\(a(L)\\) does not depend on \\(t\\) because \\(y_t\\) is covariance stationary. As the best linear predictor, \\(a(L)\\) solves\n\\[\\min_{\\{a_j\\}} \\mathbb{E} (y_t - \\sum_{j=1}^{\\infty}a_jy_{t-j})^2.\\] The first-order conditions with respect to \\(a_j\\) gives\n\\[\n\\begin{aligned}\n\\mathbb{E}[y_{t-j}(y_t-\\sum_{j=1}^{\\infty}a_jy_{t-j})] &= 0, \\\\\n\\implies \\mathbb{E}[y_{t-j}e_t] &=0.\n\\end{aligned}\n\\]\nWe now verify that \\(e_t\\) satisfies the white noise conditions. Without loss of generality, we may assume \\(\\mathbb{E}(y_t)=0\\), it follows that \\(\\mathbb{E}(e_t)=0\\). \\(\\text{var}(e_t)=\\mathbb{E}(y_t-a(L)y_t)^2\\) is a function of covariance of \\(y_t\\) and \\(a_j\\), none of which varies with time. So \\(\\text{var}(e_t)=\\sigma^2\\) is constant. Utilizing the first-order condition, \\(\\mathbb{E}[e_te_{t-j}] = \\mathbb{E}[e_t(y_{t-j}-a(L)y_{t-j})] = 0.\\)\nRepeatedly substituting for \\(y_{t-k}\\) gives\n\\[\n\\begin{aligned}\ny_t &= e_t + \\sum_{k=1}^{\\infty} a_ky_{t-k} \\\\\n&= e_t + a_1(e_{t-1} + \\sum_{k=1}^{\\infty} a_ky_{t-1-k}) + \\sum_{k=2}^{\\infty} a_ky_{t-k}\\\\\n&= e_t + a_1 e_{t-1} + \\sum_{k=1}^{\\infty} \\tilde{a}_ky_{t-k-1} \\\\\n&= e_t + a_1 e_{t-1} + \\eta_t^1 \\\\\n&\\quad\\vdots\\\\\n&= \\sum_{j=0}^{k} c_j e_{t-j} + \\eta_t^k,\n\\end{aligned}\n\\] where \\(\\eta_t^k \\in I_{t-k-1}\\). As \\(k\\to\\infty\\), we have \\(v_t = y_t - \\sum_{j=0}^{\\infty}c_je_{t-j} \\in I_{-\\infty}\\).\n\nLet’s appreciate this theorem for a while. The property of stationarity can be loosely understood as having stable patterns over time. The Wold Theorem states that any such patterns can be captured by ARMA models. In other words, ARMA models are effective in modelling stable patterns repeated over time, in so far as only 2nd-order moments are of concern. Even if the time series is not entirely stationary, if we model it with ARMA, it can be thought as extracting the stationary patterns. Figure 9.1 demonstrates the ARIMA modelling of monthly export.\n\nlibrary(zoo)\ndata = read.csv.zoo(\"data/md.csv\", FUN = as.yearmon, regular = TRUE)\ny = data$Export\nmod = arima(y, order = c(2,0,1))\nyhat = y - mod$residuals\nplot(cbind(y, yhat), plot.type = \"s\", col = 1:2, ann = F)\n\n\n\n\nFigure 9.1: Monthly export modelled with ARIMA(2,0,1)"
  },
  {
    "objectID": "wold.html#causality-and-invertibility",
    "href": "wold.html#causality-and-invertibility",
    "title": "9  Wold Theorem",
    "section": "9.2 Causality and Invertibility*",
    "text": "9.2 Causality and Invertibility*\nWe have seen that AR models can be rewritten as MA models and vice versa, suggesting the ARMA representation of a stochastic process is not unique. We have also seen that a non-invertible MA process can be equivalently represented by an invertible MA process. For example, the following MA(1) processes have the same ACF:\n\\[\n\\begin{aligned}\nx_t &= w_t + \\frac{1}{5} w_{t-1}, & w_t\\sim\\text{WN}(0,25);\\\\\ny_t &= v_t + 5 v_{t-1}, & v_t\\sim\\text{WN}(0, 1).\n\\end{aligned}\n\\]\nThe same property holds for AR processes. In Chapter 6, we state that an AR(1) process is explosive if \\(|\\phi|>1\\). This is not entirely rigorous. Consider an AR(1) process,\n\\[\ny_t = \\phi y_{t-1} + \\epsilon_t, \\text{ where } |\\phi| > 1.\n\\]\nMultiply both sides by \\(\\phi^{-1}\\),\n\\[\n\\phi^{-1} y_t = y_{t-1} + \\phi^{-1}\\epsilon_t,\n\\]\nRewrite it as an MA process,\n\\[\n\\begin{aligned}\ny_t &= \\phi^{-1} y_{t+1} - \\phi^{-1}\\epsilon_{t+1} \\\\\n&= \\phi^{-1} (\\phi^{-1} y_{t+2} - \\phi^{-1}\\epsilon_{t+2}) - \\phi^{-1}\\epsilon_{t+1} \\\\\n&\\;\\vdots \\\\\n&= \\sum_{j=1}^{\\infty} -\\phi^{-j}\\epsilon_{t+j}.\n\\end{aligned}\n\\]\nGiven \\(|\\phi^{-1}|<1\\), the process is stationary, expressed as discounted innovations in the future (despite this looks quite odd). In fact, for an non-causal AR process, we can find a causal AR process that generates the same ACF (remember the term causal means an AR process can be converted to an MA process with absolute summable coefficients).\nThe problem is given an ARMA equation, it is not enough to uniquely pin down a stochastic process. Both the explosive process and the stationary process can be a solution to \\(y_t = \\phi y_{t-1} + \\epsilon_t\\). But for a stationary process expressed as an AR model with \\(|\\phi|>1\\), we can always find an AR(1) process with \\(|\\tilde\\phi|<1\\) and a different white noise sequence \\(\\{\\tilde\\epsilon_t\\}\\) that generate the same ACF.\nThe following theorems state the conditions for the existence of stationary solutions, and the possibility of rewriting non-causal or non-invertible ARMA representations as causal and invertible ones. Since it is always possible to do so, it loses nothing to stick with causal and invertible ARMA processes when modelling stationary time series.\n\nTheorem 9.2 A unique stationary solution to the ARMA process \\(\\phi(L)y_t = \\theta(L)\\epsilon_t\\) exists iff \\(\\phi\\) and \\(\\theta\\) have no common factors and the roots of \\(\\phi(z)\\) avoid the unit circle:\n\\[\n|\\phi(z)|=1 \\implies \\phi(z) = 1-\\phi_1z-\\dots-\\phi_pz^p \\neq 0.\n\\]\n\n\nTheorem 9.3 Let \\(\\{y_t\\}\\) be a stationary ARMA process defined by \\(\\phi(L)y_t = \\theta(L)\\epsilon_t\\). If the roots of \\(\\theta(z)\\) avoid unit circle, then there are polynomials \\(\\tilde{\\phi}\\) and \\(\\tilde\\theta\\) and a white noise sequence \\(\\tilde\\epsilon\\) such that \\(\\{y_t\\}\\) satisfies \\(\\tilde\\phi(L)y_t = \\tilde\\theta(L)\\tilde\\epsilon_t\\), and this is a causal and invertible ARMA process."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "10  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hamilton, James D. 1994. Time Series Analysis. Princeton University Press.\n\n\nHyndman, Rob J, and George Athanasopoulos. 2018. Forecasting: Principles and Practice (2nd Edition). OTexts.com/fpp2.\n\n\nMikusheva, Anna, and Paul Schrimpf. 2007. 14.384 Time Series Analysis. MIT OpenCourseWare (http://ocw.mit.edu)."
  }
]