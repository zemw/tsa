[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis for Economists",
    "section": "",
    "text": "Preface\nEmpirical macroeconomics often involves the analysis of time series data, such as GDP, inflation, and interest rates, which are distinct from those utilized in cross-sectional studies. The goal of this book is to bridge the gap between introductory time series textbooks and theoretical econometrics. In modern applied research, a rudimentary comprehension of the subject often proves insufficient. Though computational tasks can be executed through simple computer commands, practitioners must go below the surface to understand the intricacies and limitations of the techniques involved. However, an exhaustive exploration of advanced econometric theories would be excessive for practical purposes. For instance, introductory textbooks would caution against running OLS on non-stationary time series, citing the risk of spurious regression. Students often accept this as a rule of thumb without a grasp of its underlying rationale. Yet, delving into intricate topics such as Itô calculus is unnecessary for empirical researchers.\nThis book seeks to acquaint readers with the time series topics essential for understanding and conducting empirical research, with a focus on macroeconomic applications. In addition to introducing basic concepts and applications, the book endeavors to elevate comprehension to a deeper level by elucidating the “why” alongside the “what” and “how.” However, the objective is not to provide an exhaustive treatment replete with formal proofs; rather, emphasis is placed on providing intuitive explanations. Consequently, readers may encounter instances of informal proofs where a more formal approach is deemed unnecessary for applied works. This book can be read as intermediary materials between undergraduate econometrics and more rigorous treatments of the subject, such as Hamilton’s Time Series Analysis.\nThe materials presented are drawn from or influenced by various sources, which are listed in the References at the end of the book without being cited individually in the context.\nRegarding notations, I use lowercase letters for random variables, such as \\(x_t\\) and \\(y_t\\). Realizations of random variables are expressed as \\(x_1\\), \\(x_2\\), and so on. The context will make it clear whether I am referring to a random variable or its realizations. Capital letters are reserved for matrices, such as \\(A\\) and \\(B\\). Vectors and matrices are sometimes written in bold for emphasizing, such as \\(\\boldsymbol X\\) and \\(\\boldsymbol y\\); but mostly, in plain format, \\(X\\) and \\(y\\), provided that they will not lead to confusion. Greek letters are preferred for parameters, such as \\(\\alpha\\) and \\(\\beta\\). Estimators are indicated with a hat, such as \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\).\nI use the statistical language R whenever programming is involved. I am aware that there are many time series solutions available in R. To avoid burdening readers with excessive packages, I stick to base R as much as possible with a little help from the zoo package.\nI would like to emphasize that my knowledge and understanding of the subject are limited, and I acknowledge that there may be mistakes or areas where I could have provided a more accurate explanation. I deeply appreciate any feedback or corrections from readers that could improve the accuracy and clarity of this book."
  },
  {
    "objectID": "01_base.html",
    "href": "01_base.html",
    "title": "1  Time Series Data",
    "section": "",
    "text": "Raw data\nWe are not so interested in the raw data without any transformation, as it is hard to read information from it. Take the GDP as an example (Figure 1.1, upper-left). There is an overall upward trend. But we are more interested in: how much does the economy grow this year? Is it better or worse than last year? The answers are not obvious from the raw data. Besides, there are obvious seasonal fluctuations. Usually the first quarter has the lowest value in a whole year, due to holidays that significantly reduce the working days in the first quarter. But this does not necessarily mean the economic condition of the first quarter is worse than others. The seasonality prohibits us from sensibly comparing the values of two consecutive quarters.\n\n\nGrowth rates\nThe headline GDP growth rates are usually reported by comparing the current quarter with the same quarter from last year: \\(g=\\frac{x_t - x_{t-4}}{x_{t-4}}\\times 100.\\) As mentioned above, due to seasonal patterns, comparing two consecutive quarters does not make sense. The year-on-year growth rate tells us how fast the economy grows. However, it loses the information about absolute levels. That comes with sereval drawbacks. For instance, it is hard to tell whether the economy recovers to the pre-pandemic level after the shock. Due to the unprecedented impact of the pandemic, the GDP for 2020 is exceptionally low, which renders the growth rate for 2021 artifically high. This is undesirable, because it does not mean the economy in 2021 is actually good. We would like a growth rate that shirks off the excessive influence of past observations.\nThat’s why we sometimes prefer (annualized) quarterly growth rate: \\(g=\\frac{x_t-x_{t-1}}{x_{t-1}}\\times 400.\\) Due to seasonally patterns, two consecutive quarters are not comparable directly. But, since this pattern is the same every year, it is possible to remove the seasonal fluctuations. This is called seasonally adjustment. We will not cover the seasonal adjustment method here. But this is something that can be done. After seasonally adjusting the data, we can calculate the growth rate based on two consecutive values (annualized by multiplying \\(4\\)). The bottom-right panel of Figure 1.1 shows the seasonally-adjusted quarterly growth. Note that it is no longer biased upward in 2021 as the year-on-year growth rate.\n\n\nSeasonally-adjusted data\nThis is usually the data format we would prefer in time series analysis. FRED reports both seasonally-adjusted and non-seasonally-adjusted series. The method for seasonal adjustment is a science in itself. Popular algorithms include X-13-ARIMA developed by the United States Census Bureau, TRAMO/SEATS developed by the Bank of Spain, and so on.\n\n\n\n\n\nFigure 1.1: Quarterly GDP Time Series (Unit: RMB Billion or %)\n\n\n\n\n\n\nLogarithms\nWe like to work with log values. Lots of economic time series exhibit exponential growth, such as GDP. Taking logs convert them to linear. Another amazing fact about logs is that the difference between two log values can be interpreted as percentage growth. To see this, by Taylor’s theorem, we have \\(\\ln(\\Delta x +1) \\approx \\Delta x\\) for small values of \\(\\Delta x\\). Therefore,\n\\[\n\\ln x_t - \\ln x_{t-1} = \\ln\\left(\\frac{x_t}{x_{t-1}}\\right) = \\ln\\left(\\frac{x_t-x_{t-1}}{x_{t-1}}+1\\right) \\approx \\frac{x_t-x_{t-1}}{x_{t-1}}.\n\\]\nSo it is very handy to just difference the log levels to get the growth rates. Log-difference can also be interpreted as the continuously compounded rate of change:\n\\[\n\\frac{x_t}{x_{t-1}}=e^g \\implies g = \\ln x_t - \\ln x_{t-1}.\n\\]\nLog-difference also has the property of summability: summing up a sequence of log-differences gives back the log level provided the initial level. It is not as handy if you want to recover the levels from percentage growth.\n\\[\n\\ln x_t = x_0 + \\sum_{j=1}^{t} (\\ln x_j - \\ln x_{j-1}).\n\\]"
  },
  {
    "objectID": "02_dec.html#time-series-components",
    "href": "02_dec.html#time-series-components",
    "title": "2  Decomposition",
    "section": "2.1 Time Series Components",
    "text": "2.1 Time Series Components\nIt is helpful to think about a time series as composed of different components: a trend component, a seasonal component, and a remainder.\n\\[x_t = T_t + S_t + R_t.\\]\nThe formula assumes the “additive” composition. This assumption is appropriate if the magnitude of the fluctuations does not vary with the absolute levels of the time series. If the magnitude of fluctuations is proportional to the absolute levels, a “multiplicative” decomposition is more appropriate:\n\\[\nx_t = T_t \\times S_t \\times R_t.\n\\]\nNote that a multiplicative decomposition of a time series is equivalent to an additive decomposition on its log levels:\n\\[\n\\ln x_t = \\ln T_t + \\ln S_t + \\ln R_t.\n\\]\nDecomposing a time series allows us to extract information that is not obvious from the original time series. It also allows us to manipulate the time series. For example, if the seasonal component can be estimated, we can remove it to obtain seasonally-adjusted series, \\(x_t^{SA} = x_t - S_t\\), or \\(x_t^{SA} = x_t/S_t\\). The question is how to estimate the components given a time series."
  },
  {
    "objectID": "02_dec.html#moving-averages",
    "href": "02_dec.html#moving-averages",
    "title": "2  Decomposition",
    "section": "2.2 Moving Averages",
    "text": "2.2 Moving Averages\nMoving averages turn out to be handy in estiming trend-cycles by averaging out noisy fluctuations. A moving average of order \\(m\\) (assuming \\(m\\) is an odd number) is defined as\n\\[\n\\text{MA}(x_t,m) = \\frac{1}{m}\\sum_{j=-k}^{k} x_{t+j},\n\\]\nwhere \\(m=2k + 1\\). For example, a moving average of order \\(3\\) is\n\\[\n\\text{MA}(x_t, 3) = \\frac{1}{3}(x_{t-1} + x_t + x_{t+1}).\n\\]\nNote that \\(x_t\\) is centered right in the middle and the average is symmetric. This also means, if we apply this formula to real data, the first and last observation will have to be discarded. If the order \\(m\\) is an even number, the formula will no longer be symmetric. To overcome this, we can estimate a moving average over another moving average. For example, we can estimate a moving average of order \\(4\\), followed by a moving average of order \\(2\\). This is denoted as \\(2 \\times 4\\)-MA. Mathematically,\n\\[\n\\begin{aligned}\n\\text{MA}(x_t, 2 \\times 4) &= \\frac{1}{2}[\\text{MA}(x_{t-1}, 4) + \\text{MA}(x_t, 4)] \\\\\n&= \\frac{1}{2}\\left[\\frac{1}{4}(x_{t-2} + x_{t-1} + x_t + x_{t+1}) + \\frac{1}{4}(x_{t-1} + x_t + x_{t+1} + x_{t+2})\\right] \\\\\n&= \\frac{1}{8}x_{t-2} + \\frac{1}{4}x_{t-1} + \\frac{1}{4}x_{t} + \\frac{1}{4}x_{t+1} + \\frac{1}{8}x_{t+2}.\n\\end{aligned}\n\\]\nNote that how the \\(2\\times4\\)-MA averages out the seasonality for time series with seasonal period \\(4\\), e.g. quarterly series. The formula puts equal weight on every quarter — the first and last terms refer the same quarter and their weights combined to \\(\\frac{1}{4}\\).\nIn general, we can use \\(m\\)-MA to estimate the trend if the seasonal period is an odd number, and use \\(2\\times m\\)-MA if the seasonal period is an even number.\n\ndata = readRDS(\"data/gdp.Rds\")  # a `zoo` object\ngdp2x4MA = ma(ma(data$GDP,4),2) # from `forecast` package\nts.plot(cbind(data$GDP, gdp2x4MA), col=1:2)\n\n\n\n\nFigure 2.1: Quarterly GDP with 2x4-MA estimate of the trend-cycle"
  },
  {
    "objectID": "02_dec.html#classical-decomposition",
    "href": "02_dec.html#classical-decomposition",
    "title": "2  Decomposition",
    "section": "2.3 Classical Decomposition",
    "text": "2.3 Classical Decomposition\nMoving averages give us everything we need to perform classical decomposition. Classical decomposition, invented 1920s, is the simplest method decompose a time series into trend, seasonality and remainder. It is outdated nowadays and has been replaced by more advanced algorithms. Nonetheless, it serves as a good example for introductory purpose on how time series decomposition could possibly be achieved.\nThe algorithm for additive decomposition is as follows.\n\nEstimate the trend component \\(T_t\\) by applying moving averages. If the seasonal period is an odd number, apply the \\(m\\)-th order MA. If the seasonal period is even, apply the \\(2\\times m\\) MA.\nCalculate the detrended series \\(x_t - T_t\\).\nCalculate the seasonal component \\(S_t\\) by averaging all the detrended values of the season. For example, for quarterly series, the value of \\(S_t\\) for Q1 would be the average of all values in Q1. This assumes the seasonal component is constant over time. \\(S_t\\) is then adjusted to ensure all values summed up to zero.\nSubtracting the seasonal component to get the remainder \\(R_t = x_t-T_t-S_t\\).\n\n\nlog(data$GDP) |> decompose() |> plot()\n\n\n\n\nFigure 2.2: Classical multiplicative decomposition of quarterly GDP\n\n\n\n\nThe example performs additive decomposition to the logged quarterly GDP series. Note how the constant seasonal component is removed, leaving the smooth and nice-looking up-growing trend. The remainder component tells us the irregular ups and downs of the economy around the trend-cycle. Isn’t it amazing that a simple decomposition of the time series tells us a lot about the economy?"
  },
  {
    "objectID": "02_dec.html#seasonal-adjustment",
    "href": "02_dec.html#seasonal-adjustment",
    "title": "2  Decomposition",
    "section": "2.4 Seasonal Adjustment",
    "text": "2.4 Seasonal Adjustment\nBy decomposing a time series into trend, seasonality and remainder, it readily gives us a method for seasonal adjustment. Simply subtracting the seasonal component from the original data, or equivalently, summing up the trend and the remainder components, would give us the seasonally-adjusted series.\nThe following example compares the seasonally-adjusted series using the classical decomposition with the state-of-the-art X-13ARIMA-SEATS algorithm. Despite the former is far more rudimentary than the latter, they look quite close if we simply eye-balling the plot. By taking first-order differences, we can see the series based on classical decomposition is more volatile, suggesting the classical decomposition is less robust to unusual values.\n\nlogdata = log(data) |> window(start=2000)\nseasadj = as.ts(logdata$GDP) - decompose(logdata$GDP)$seasonal\n\npar(mfrow=c(1,2), mar=rep(2,4))\nts.plot(cbind(seasadj, logdata$GDPSA), col=1:2)\nts.plot(diff(cbind(seasadj, logdata$GDPSA)), col=1:2)\n\n\n\n\nComparing classical decomposition and X-13"
  },
  {
    "objectID": "03_acf.html#autocorrelation",
    "href": "03_acf.html#autocorrelation",
    "title": "3  ACF and PACF",
    "section": "3.1 Autocorrelation",
    "text": "3.1 Autocorrelation\nThe temporal dependence is characterized by the correlation between \\(y_t\\) and its own lags \\(y_{t-k}\\).\n\nDefinition 3.1 The \\(k\\)-th order autocovariance of \\(y_t\\) is defined as\n\\[\\gamma_k = \\text{cov}(y_t, y_{t-k}).\\]\nThe \\(k\\)-th order autocorrelation is defined as\n\\[\\rho_k = \\frac{\\text{cov}(y_t, y_{t-k})}{\\text{var}(y_t)} = \\frac{\\gamma_k}{\\gamma_0}.\\]\n\nIf we plot the autocorrelation as a function of the lag length \\(k\\), we get the autocorrelation function (ACF). Here is an example of the ACF of China’s monthly export growth (log-difference). The lag on the horizontal axis is counted by seasonal period. Because it is monthly data, 1 period is 12 months. We can see the autocorrelation is the strongest for the first two lags. Longer lags are barely significant. There are spikes with 12-month and 24-month lags, indicating the seasonality is not fully removed from the series.\n\ndata = readRDS(\"data/md.Rds\")\nacf(data$Export, main='Autocorrelation')\n\n\n\n\nFigure 3.1: ACF for monthly export growth"
  },
  {
    "objectID": "03_acf.html#partial-autocorrelation",
    "href": "03_acf.html#partial-autocorrelation",
    "title": "3  ACF and PACF",
    "section": "3.2 Partial Autocorrelation",
    "text": "3.2 Partial Autocorrelation\nACF measures the correlation between \\(y_t\\) and \\(y_{t-k}\\) regardless of their relationships with the intermediate variables \\(y_{t-1},y_{t-2},\\dots,y_{t-k+1}\\). Even if \\(y_t\\) is only correlated with the first-order lag, it is automatically made correlated with the \\(k\\)-th order lag through intermediate variables. Sometime we are interested in the correlation between \\(y_t\\) and \\(y_{t-k}\\) partialling out the influence of intermediate variables.\n\nDefinition 3.2 The partial autocorrelation function (PACF) considers the correlation between the remaining parts in \\(y_t\\) and \\(y_{t-k}\\) after partialling out the intermediate effect of \\(y_{t-1},y_{t-2},\\dots,y_{t-k+1}\\).\n\\[\n\\phi_k = \\begin{cases}\n\\text{corr}(y_t, y_{t-1})=\\rho_{_1}, \\text{ if } k=1;\\\\\n\\text{corr}(r_{y_t|y_{t-1},\\dots,y_{t-k+1}}, r_{y_{t-k}|y_{t-1},\\dots,y_{t-k+1}}), \\text{ if } k\\geq 2;\n\\end{cases}\n\\]\nwhere \\(r_{y|x}\\) means the remainder in \\(y\\) after partialling out the intermediate effect of \\(x\\).\n\nIn practice, \\(\\phi_k\\) can be estimated by the regression\n\\[\ny_t = \\mu + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_k y_{t-k} + \\epsilon_t.\n\\]\nThe estimated coefficient \\(\\hat\\phi_k\\) is the partial autocorrelation after controlling the intermediate lags.\n\npacf(data$Export, main='Partial Autocorrelation')\n\n\n\n\nFigure 3.2: PACF for monthly export growth"
  },
  {
    "objectID": "04_st.html#stationary-process",
    "href": "04_st.html#stationary-process",
    "title": "4  Stationarity",
    "section": "4.1 Stationary Process",
    "text": "4.1 Stationary Process\n\nDefinition 4.1 A stochastic process is said to be strictly stationary if its properties are unaffected by a change of time origin. In other words, the joint distribution at any set of time is not affect by an arbitrary shift along the time axis.\n\n\nDefinition 4.2 A stochastic process is called covariance stationary (or weak stationary) if its means, variances, and covariances are independent of time. Formally, a process \\(\\{y_t\\}\\) is covariance stationary if for all \\(t\\) it holds that\n\n\\(\\mathbb{E}(y_t) = \\mu < \\infty\\);\n\\(\\text{var}(y_t) = \\gamma_{_0} < \\infty\\);\n\\(\\text{cov}(y_t,y_{t-k})=\\gamma_k\\), for \\(k=1,2,3,\\dots\\)\n\n\nStationarity is an important concept in time series analysis. It basically says the statistical properties of a time series are stable over time. Otherwise, if the statistical properties vary with time, statistics estimated from past values, such autocorrelations, would be much less meaningful. Strict stationarity requires the joint distribution being stable, that is moments of any order would be stable over time. In practice, mostly we only care about the first- and second-order moments, that is means and variances and covariances. Therefore, covariance stationary is sufficient.\nFigure 4.1 shows some examples of stationary and non-stationary time series. Only the first one is stationary (it is generated from \\(i.i.d\\) normal distribution). The second one is not stationary as its mean is not constant over time. The third one is not stationary as its variance is not constant. The last one is not stationary either, because its covariance is not constant.\n\n\n\n\n\nFigure 4.1: Stationary and non-stationary time series\n\n\n\n\nReal-life time series are rarely stationary. But they can be transformed to (quasi) stationary by differencing. Figure 4.2 shows some examples of the first-order (log) differences of real-life time series. They more or less exhibit some properties of stationarity, but not perfectly stationary. The series can be further “stationarized” by taking a second-order difference. But these examples are acceptable to be treated as stationary in our models. Even if they are not perfectly stationary, the model can be thought of being used to “extract” their stationary properties.\n\n\n\n\n\nFigure 4.2: Stationary and non-stationary time series (real life)\n\n\n\n\n\nProposition 4.1 For stationary series, it holds that \\(\\gamma_k = \\gamma_{-k}\\).\n\n\nProof. By definition,\n\\[\n\\gamma_k = \\mathbb{E}[(y_t-\\mu)(y_{t-k}-\\mu)],\n\\]\n\\[\n\\gamma_{-k} = \\mathbb{E}[(y_t-\\mu)(y_{t+k}-\\mu)].\n\\]\nSince \\(y_t\\) is stationary, \\(\\gamma_k\\) is invariant with time. Let \\(t'=t+k\\), we have\n\\[\n\\begin{aligned}\n\\gamma_{k} &= \\mathbb{E}[(y_{t'}-\\mu)(y_{t'-k}-\\mu)] \\\\\n&= \\mathbb{E}[(y_{t+k}-\\mu)(y_{t}-\\mu)] \\\\\n&= \\gamma_{-k}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "04_st.html#ergodicity",
    "href": "04_st.html#ergodicity",
    "title": "4  Stationarity",
    "section": "4.2 Ergodicity",
    "text": "4.2 Ergodicity\nTemporal dependence is an important feature of time series variables. This dependence is both a bless and a curve. Autocorrelation enables us to make predictions based on past experiences. However, as we will see in later chapters, it also invalidates theorems that usually require \\(iid\\) assumptions. Ideally, we would like the temporal dependence to be not too strong. This is the property of ergodicity.\n\nDefinition 4.3 A stationary process \\(\\{y_t\\}\\) is ergodic if\n\\[\n\\lim_{n\\to\\infty} |\\mathbb{E}[f(y_t...y_{t+k})g(y_{t+n}...y_{t+n+k})]|=|\\mathbb{E}[f(y_t...y_{t+k})]||\\mathbb{E}[g(y_{t+n}...y_{t+n+k})]|.\n\\]\n\nHeuristically, ergodicity means if two random variables are positioned far enough in the sequence, they become almost independent. In other words, ergodicity is a restriction on dependency. An ergodic process allows serial correlation, but the serial correlation disappears if the two observations are far apart. Ergodicity is important because as we will see in later chapters, the Law of Large Numbers or the Central Limit Theorem will not hold without it.\n\nTheorem 4.1 A stationary time series is ergodic if \\(\\sum_{k=0}^{\\infty} |\\gamma_k| < \\infty\\).\n\n\nProof. A rigorous proof is not necessary. It is enough to give an intuition why autocorrelation disappears for far apart variables. Note that \\(\\sum_{k=0}^{\\infty} |\\gamma_k|\\) is monotonic and increasing, it converges. Therefore, \\(\\gamma_k \\to 0\\) by Cauchy Criterion."
  },
  {
    "objectID": "04_st.html#white-noise",
    "href": "04_st.html#white-noise",
    "title": "4  Stationarity",
    "section": "4.3 White Noise",
    "text": "4.3 White Noise\nWhite noise is a special stationary process that is an important building block of many time series models.\n\nDefinition 4.4 A stochastic process \\(w_t\\) is called white noise if its has constant mean \\(0\\) and variance \\(\\sigma^2\\) and no serial correlation \\(\\text{cov}(w_t, w_{t-k})=0\\) for any \\(k \\neq 0\\). The white noise process is denoted as\n\\[\nw_t \\sim \\text{WN}(0, \\sigma^2).\n\\]\n\nThis is the weakest requirement for while noise. It only requires no serial correlation. We may impose further assumptions. If every \\(w_t\\) is independent, it becomes independent white noise \\(w_t \\sim \\perp\\text{WN}(0, \\sigma^2)\\). Independence does not imply identical distribution. If every \\(w_t\\) is independently and identically distributed, it is called \\(i.i.d\\) white noise, \\(w_t \\overset{iid}{\\sim} \\text{WN}(0, \\sigma^2)\\). If the distribution is normal, it becomes the most perfect white noise, that is \\(i.i.d\\) Gaussian white noise, \\(w_t \\overset{iid}{\\sim} N(0, \\sigma^2)\\). The first plot of Figure 4.1 is a demonstration of the \\(i.i.d\\) Gaussian white noise. In most cases, the weakest form of white noise is sufficient.\n\n\n\n\n\n\nExercise\n\n\n\nProve that a while noise process is stationary."
  },
  {
    "objectID": "05_spec.html#classification",
    "href": "05_spec.html#classification",
    "title": "5  Model vs Spec",
    "section": "5.1 Classification",
    "text": "5.1 Classification\nTime series models can be broadly sorted into four categories based on whether we are dealing with stationary or non-stationary time series, or whether the model involves only one variable or multiple variables.\n\nTime series model classification\n\n\n\nStationary\nNonstationary\n\n\n\n\nUnivariate\nARMA\nUnit root\n\n\nMultivariate\nVAR\nCointegration"
  },
  {
    "objectID": "05_spec.html#model-vs-spec",
    "href": "05_spec.html#model-vs-spec",
    "title": "5  Model vs Spec",
    "section": "5.2 Model vs Spec",
    "text": "5.2 Model vs Spec\nWe use the word “model” rather loosely in economics and econometrics. Anything that deals with the quantified relationships between variables can be called a model. A general equilibrium model is a model. A regression is also a model.\nTo make things less confusing, we would use the word “model” more restrictively in this chapter. We reserve the word model to those representing the data generating processes (DGPs). That is, when we write down a model in an equation, we literally mean it. If we say \\(y_t\\) follows an AR(1) model:\n\\[\n\\begin{aligned}\ny_t &= \\phi y_{t-1} + \\epsilon_t,\\\\\n\\epsilon_t &\\sim N(0,\\sigma^2).\n\\end{aligned}\n\\]\nWe literally mean \\(y_t\\) is determined by its previous value and an contemporary innovation drawn from a Gaussian distribution.\nA model is distinguished from a specification. Suppose \\(\\{y_t\\}\\) represent the GDP series, we can estimate a regression:\n\\[\ny_t = \\phi y_{t-1} + e_t\n\\]\nThis is a specification not a model. Because the DGP of GDP data is unknown, definitely not an AR(1). We can nontheless fit this spec with the data and get an estimated \\(\\hat\\phi\\). If \\(e_t\\) satisfies some nice properties, for example, uncorrelated with the regressor, then we know this \\(\\hat\\phi\\) is consistent.\nWhen we run regressions with real-life data, we are actually working with specifications. They are not the DGPs of the random variables. But they allow us to recover some useful information from the data when certain assumptions are met. Mostly we are interested in the relationships between variables. A specification describes this relationship, even though it does not describe the full DGP.\nThis chapter deals with models in the abstract sense. The next chapter will discuss how to fit a model or a spec with real data."
  },
  {
    "objectID": "06_ar.html#ar1-process",
    "href": "06_ar.html#ar1-process",
    "title": "6  AR Models",
    "section": "6.1 AR(1) Process",
    "text": "6.1 AR(1) Process\nWe start with the simplest time series model — autoregressive model, or AR model. The simplest from of AR model is AR(1), which involves only one lag,\n\\[\ny_t = \\mu + \\phi y_{t-1} + \\epsilon_t,\n\\tag{6.1}\\]\nwhere \\(\\epsilon_t \\sim \\text{WN}(0,\\sigma^2)\\). The model can be extended to include more lags. An AR(\\(p\\)) model is defined as\n\\[\ny_t = \\mu + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} +  \\epsilon_t.\n\\]\nWe focus on AR(1) first. The model states that the value of \\(y_t\\) is determined by a constant, its previous value, and a random innovation. We call the last term \\(\\epsilon_t\\) innovation, not an error term. It is not an error, it is a random contribution that is unknown until time \\(t\\). It should also not be confused with the so-called “structural shock”, which is attached with a structural meaning and will be discussed in later chapters.\nThe model is probabilistic, as oppose to deterministic, in the sense that some information is unknown or deliberately omitted, so that we do not know the deterministic outcome, but only a probability distribution.\n\n\n\n\n\n\nNote\n\n\n\nThink about tossing a coin: if every piece of information is incorporated in the model, including the initial speed and position, the air resistance, and so on; then we can figure out the exact outcome, whether the coin will land on its head or tail. But this is unrealistic. Omitting all these information, we can model the process as a Bernoulli distribution. The probability model will not give a deterministic outcome, but only a distribution with each possible value associated with a probability.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe assumption that a process is only determined by its past values and a white noise innovation seems very restrictive. But it is not. Think about the three assumptions for technical analysis of the stock market (there are still many investors believing this): (1) The market discounts everything, (2) prices move in trends and counter-trends, and (3) price action is repetitive, with certain patterns reoccurring. Effectively, it is saying we can predict the stock market by the past price patterns. If we were to write a model for the stock market based on these assumptions, AR(\\(p\\)) isn’t a bad choice at all.\n\n\nNote that the model can be rewritten as\n\\[\ny_t - \\frac{\\mu}{1-\\phi} = \\phi\\left(y_{t-1} - \\frac{\\mu}{1-\\phi}\\right) + \\epsilon_t,\n\\]\nassuming \\(\\phi\\neq 1\\). If we define \\(\\tilde{y_t}=y_t - \\frac{\\mu}{1-\\phi}\\), we can get rid of the constant term:\n\\[\n\\tilde{y}_t = \\phi\\tilde{y}_{t-1} + \\epsilon_t.\n\\tag{6.2}\\]\nIt can be easily shown, if \\(y_t\\) is stationary, \\(\\frac{\\mu}{1-\\phi}\\) is the stationary mean. Because this mechanical transformation can always be done to remove the constant. We can simply ignore the constant term without lost of generality.\n\n\n\n\n\n\nNote\n\n\n\nWorking with demeaned variables greatly simplify the notation. For example, assuming \\(\\mathbb{E}(y_t)=0\\), the variance is simply the second-order moment \\(\\mathbb{E}(y_t^2)\\); the covariance can be written as \\(\\mathbb{E}(y_ty_{t-k})\\).\n\n\nFor a constant-free AR(1) model, we can rewrite the model as follows:\n\\[\n\\begin{aligned}\ny_t &= \\phi y_{t-1} + \\epsilon_t \\\\\n&= \\phi( \\phi y_{t-2} + \\epsilon_{t-1}) + \\epsilon_t \\\\\n&= \\phi^2 y_{t-2} + \\phi\\epsilon_{t-1} + \\epsilon_t \\\\\n&= \\phi^2 (\\phi y_{t-3} + \\epsilon_{t-2}) + \\phi\\epsilon_{t-1} + \\epsilon_t \\\\\n&= \\phi^3 y_{t-3} + \\phi^2\\epsilon_{t-2} + \\phi\\epsilon_{t-1} + \\epsilon_t \\\\\n&\\;\\vdots \\\\\n&= \\phi^t y_0 + \\sum_{j=0}^{t-1} \\phi^j\\epsilon_{t-j} \\\\\n&= \\sum_{j=0}^{\\infty} \\phi^j\\epsilon_{t-j}.\n\\end{aligned}\n\\tag{6.3}\\]\nThe exercise shows an AR(1) process can be reduced to an MA process, which will be discussed in the next section. It says the value of \\(y_t\\) is determined by its initial value (if it has one) and the accumulated innovations in the past. It is our deeds in history that shapes our world today.\n\n\n\n\n\n\nNote\n\n\n\nThe property that an AR process can be rewritten as an infinite MA process with absolute summable coefficients \\(\\sum_{j=0}^{\\infty}|\\phi^j|<\\infty\\) is called causal. This must not be confused with the causal effect in econometrics (defines in the ceteris paribus sense). To avoid confusion, we avoid use this term as much as possible.\n\n\nNow we focus our attention on the critical parameter \\(\\phi\\). If \\(|\\phi|>1\\), the process is explosive. We are not interested in explosive processes. If a real-world time series grows exponentially, we take logarithm to transform it to linear. So in most of our discussions, we rule out the case of explosive behaviour.\nIf \\(|\\phi|<1\\), \\(\\phi^j\\to 0\\) as \\(j\\to\\infty\\). This means the influence of innovations far away in the past decays to zero. We will show that the series is stationary and ergodic.\nIf \\(|\\phi|=1\\), we have \\(y_t = \\sum_{j=0}^{\\infty} \\text{sgn}(\\phi)^j\\epsilon_{t-j} = \\sum_{j=0}^{\\infty}\\tilde{\\epsilon}_{t-j}\\). This means the influence of past innovations will not decay no matter how distant away they are. This is known as a unit root process, which will be covered in later chapters. But it is clear that the process is not stationary. Consider the variance of \\(y_t\\) conditioned on an initial value:\n\\[\n\\text{var}(y_t|y_0) = \\text{var}(\\sum_{j=0}^{t-1}\\epsilon_{t-j})=\\sum_{j=0}^{t-1}\\text{var}(\\epsilon_{t-j})=\\sum_{j=0}^{t-1}\\sigma^2=t\\sigma^2.\n\\]\nThe variance is increasing with time. It is not constant. Figure 6.1 simulates the AR(1) with \\(\\phi=0.5\\) and \\(\\phi=1\\) respectively.\n\ny = arima.sim(list(ar=0.5), n=1000)\nz = arima.sim(list(order=c(0,1,0)), n=1000)\nplot(cbind(y,z), plot.type=\"multiple\", nc=2, ann=F, \n     mar.multi=rep(2,4), oma.multi = rep(0,4))\n\n\n\n\nFigure 6.1: Simulation of AR(1) processes\n\n\n\n\n\nProposition 6.1 An AR(1) process with \\(|\\phi|<1\\) is covariance stationary.\n\n\nProof. Let’s compute the mean, variance and covariance for the AR(1) process.\n\\[\n\\mathbb{E}(y_t) = \\mathbb{E}\\left[\\sum_{j=0}^{\\infty} \\phi^j\\epsilon_{t-j}\\right] = \\sum_{j=0}^{\\infty} \\phi^j\\mathbb{E}[\\epsilon_{t-j}]=0.\n\\]\n\\[\n\\begin{aligned}\n\\text{var}(y_t)\n&= \\text{var}\\left[\\sum_{j=0}^{\\infty} \\phi^j\\epsilon_{t-j}\\right]\n= \\sum_{j=0}^{\\infty} \\phi^j\\text{var}[\\epsilon_{t-j}] \\\\\n&= \\sigma^2 \\sum_{j=0}^{\\infty} \\phi^j\n=\\frac{\\sigma^2}{1-\\phi}.\n\\end{aligned}\n\\]\nFor the covariances,\n\\[\n\\begin{aligned}\n\\gamma_1 &= \\mathbb{E}(y_ty_{t-1})\n= \\mathbb{E}((\\phi y_{t-1} + \\epsilon_t)y_{t-1}) \\\\\n&= \\mathbb{E}(\\phi y_{t-1}^2 + \\epsilon_t y_{t-1}) \\\\\n&= \\phi\\mathbb{E}(y_{t-1}^2) + 0 \\\\\n&= \\frac{\\phi\\sigma^2}{1-\\phi};\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\gamma_2 &= \\mathbb{E}(y_ty_{t-2})\n= \\mathbb{E}((\\phi y_{t-1} + \\epsilon_t)y_{t-2}) \\\\\n&= \\mathbb{E}(\\phi y_{t-1}y_{t-2} + \\epsilon_t y_{t-2}) \\\\\n&= \\phi\\mathbb{E}(y_{t-1}y_{t-2}) \\\\\n&= \\phi\\gamma_1 = \\frac{\\phi^2\\sigma^2}{1-\\phi};\\\\\n&\\;\\vdots \\\\\n\\gamma_j &= \\frac{\\phi^j\\sigma^2}{1-\\phi}.\n\\end{aligned}\n\\]\nAll of them are independent of time \\(t\\). By Definition 4.2, the process is covariance stationary.\n\nSo the ACF decays gradually as \\(\\phi^j \\to 0\\). What about the PACF? Estimating the PACF is equivalent to regressing \\(y_t\\) on its lags. Since there is only one lag, the PACF should have non-zero value only for the first lag, and zeros for all other lags.\n\npar(mfrow=c(1,2), mar=c(2,4,1,1))\nacf(y); pacf(y)\n\n\n\n\nFigure 6.2: ACF and PACF for AR(1) process"
  },
  {
    "objectID": "06_ar.html#lag-operator",
    "href": "06_ar.html#lag-operator",
    "title": "6  AR Models",
    "section": "6.2 Lag Operator",
    "text": "6.2 Lag Operator\nTo facilitate easy manipulation of lags, we introduce the lag operator:\n\\[\nLy_t = y_{t-1}.\n\\]\nThe AR(1) process can be written with the lag operator:\n\\[\ny_t = \\phi Ly_t+ \\epsilon_t \\implies (1-\\phi L)y_t = \\epsilon_t.\n\\]\nThe lag operator \\(L\\) can be manipulated just as polynomials. It looks weird, but it actually works. Do a few exercises to convince yourself.\n\\[\nL^2 y_t = L(Ly_t) = Ly_{t-1} = y_{t-2}.\n\\]\n\\[\n\\begin{aligned}\n(1-L)^2y_t &= (1-L)(y_t - y_{t-1}) \\\\\n&= (y_t-y_{t-1})-(y_{t-1}-y_{t-2}) \\\\\n&= y_t - 2y_{t-1} + y_{t-2} \\\\\n&=(1-2L+L^2)y_t.\n\\end{aligned}\n\\]\nWe can even inverse a lag polynomial (provided \\(|\\phi|<1\\)),\n\\[\n\\begin{aligned}\n(1-\\phi L)y_t &= \\epsilon_t \\\\\n\\implies y_t &= (1-\\phi L)^{-1}\\epsilon_t =\\sum_{j=0}^{\\infty} \\phi^j L^j \\epsilon_t = \\sum_{j=0}^{\\infty} \\phi^j \\epsilon_{t-j}.\n\\end{aligned}\n\\]\nWe reach the same conclusion as Equation 6.3 with the lag operator."
  },
  {
    "objectID": "06_ar.html#arp-process",
    "href": "06_ar.html#arp-process",
    "title": "6  AR Models",
    "section": "6.3 AR(p) Process",
    "text": "6.3 AR(p) Process\nWe now generalize the conclusions above to AR(\\(p\\)) processes. With the help of the lag operator, an AR(\\(p\\)) process can be written as\n\\[\n(1-\\phi_1 L-\\phi_2 L^2-\\dots-\\phi_p L^p) y_t = \\epsilon_t,\n\\]\nor even more parsimoniously,\n\\[\n\\phi(L)y_t = \\epsilon_t.\n\\]\nNote that we ignore the constant term, which can always be removed by redefine \\(\\tilde{y}_t = y_t - \\frac{\\mu}{1-\\phi_1-\\phi_2-\\dots-\\phi_p}\\).\nTo derive the MA representation, we need to figure out \\(\\phi^{-1}(L)\\). By the Fundamental Theorem of Algebra, we know the polynomial \\(\\phi(z)\\) has \\(p\\) roots in the complex space. So the lag polynomial can be factored as\n\\[\n(1-\\lambda_1L)(1-\\lambda_2L)\\dots(1-\\lambda_pL) y_t = \\epsilon_t,\n\\]\nwhere \\(z=\\lambda_i^{-1}\\) is the \\(i\\)-th root of \\(\\phi(z)\\). If the roots are outside the unit circle, \\(|\\lambda_i|<1\\) means each of the left hand terms is inversible.\n\\[\n\\begin{aligned}\ny_t &= \\frac{1}{(1-\\lambda_1L)(1-\\lambda_2L)\\dots(1-\\lambda_pL)}\\epsilon_t \\\\\n&= \\left(\\frac{c_1}{1-\\lambda_1L} + \\frac{c_2}{1-\\lambda_2L} + \\dots + \\frac{c_p}{1-\\lambda_pL}\\right)\\epsilon_t \\\\\n&= \\sum_{j=0}^{\\infty}(c_1\\lambda_1^j+c_2\\lambda_2^j+\\dots+c_p\\lambda_p^j)L^j\\epsilon_t \\\\\n&= \\sum_{j=0}^{\\infty} \\theta_j\\epsilon_{t-j}, \\text{ where }\\theta_j=c_1\\lambda_1^j+\\dots+c_p\\lambda_p^j.\n\\end{aligned}\n\\]\nIt follows that this process has constant mean and variance. For the covariances, given\n\\[\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t,\n\\]\nMultiply both sides by \\(y_t\\) and take expectation,\n\\[\n\\mathbb{E}[y_t^2] = \\phi_1 \\mathbb{E}[y_ty_{t-1}] + \\phi_2 \\mathbb{E}[y_ty_{t-2}] + \\dots + \\phi_p \\mathbb{E}[y_ty_{t-p}],\n\\]\n\\[\n\\gamma_0 = \\phi_1\\gamma_{-1} + \\phi_2\\gamma_{-2} + \\dots + \\phi_p\\gamma_{-p}.\n\\]\nSimilarly, multiply both sides by \\(y_{t-1},\\dots,y_{t-j}\\), we have\n\\[\n\\begin{aligned}\n\\gamma_1 &= \\phi_1\\gamma_{0} + \\phi_2\\gamma_{-1} + \\dots + \\phi_p\\gamma_{-p+1},\\\\\n&\\;\\vdots\\\\\n\\gamma_j &= \\phi_1\\gamma_{j-1} + \\phi_2\\gamma_{j-2} + \\dots + \\phi_p\\gamma_{j-p}.\n\\end{aligned}\n\\]\nThis is called the Yule-Walker equation. The first \\(p\\) unknowns \\(\\gamma_0,\\dots,\\gamma_{p-1}\\) can be solved by the first \\(p\\) equations. The rest can then be solved iteratively.\nIt can be shown all of the covariances are invariant with time. Therefore, under the condition all \\(|\\lambda_i|<1\\), the AR(\\(p\\)) process is stationary.\nFor the PACF, a regression of \\(y_t\\) over its lags would recover \\(p\\) non-zero coefficients. Longer lags should have coefficients insignificantly different from zero.\n\ny = arima.sim(list(ar=c(2.4, -1.91, 0.5)), n=3000)\npar(mfrow=c(1,2), mar=c(2,4,1,1))\nacf(y); pacf(y)\n\n\n\n\nFigure 6.3: ACF and PACF for AR(p) process\n\n\n\n\n\nProposition 6.2 An AR(\\(p\\)) process is stationary if all the roots of \\(\\phi(z)\\) are outside the unit circle.\n\n\nProposition 6.3 An AR(\\(p\\)) process is characterized by (i) an ACF that is infinite in extend but tails of gradually; and (ii) a PACF that is (close to) zero for lags after \\(p\\)."
  },
  {
    "objectID": "07_ma.html#ma1-process",
    "href": "07_ma.html#ma1-process",
    "title": "7  MA Models",
    "section": "7.1 MA(1) Process",
    "text": "7.1 MA(1) Process\nAgain, let’s start with the simplest moving average model. A first-order moving average process, or MA(1), is defined as\n\\[\ny_t = \\mu + \\epsilon_t + \\theta\\epsilon_{t-1},\n\\tag{7.1}\\]\nwhere \\(\\{\\epsilon_t\\} \\sim \\text{WN}(0, \\sigma^2)\\) are uncorrelated innovations. The MA model says the current value \\(y_t\\) is a moving average of past innovations (in MA(1), the weight on \\(\\epsilon_{t-1}\\) is \\(\\theta\\)). MA models directly relate the observable variable to past innovations. If we know the past innvation \\(\\epsilon_{t-j}\\), we can easily figure out its contribution to the outcome variable (unlike AR models where the effect of a past innovation is transmitted through \\(y_{t-j},\\dots,y_{t-1}\\)). So MA models are the preferred analytic tool in many applications, despite it looks odd from the eyes of regression modelers. You may wonder how it is possible to estimate such a model. We will put off the estimation techniques to the next chapter.\nIt is clear that \\(y_t\\) has a constant mean, \\(\\mathbb{E}(y_t) = \\mu\\). We can omit the constant if we work with the demeaned series \\(\\tilde{y}_t = y_t - \\mu\\). Without loss of generality, we assume for the rest \\(\\{y_t\\}\\) has zero mean, so the model is simplified as\n\\[\ny_t = \\epsilon_t + \\theta\\epsilon_{t-1}.\n\\tag{7.2}\\]\nLet’s compute its variance and covariances:\n\\[\n\\begin{aligned}\n\\gamma_0 &= \\text{var} (\\epsilon_t + \\theta\\epsilon_{t-1}) = \\text{var}(\\epsilon_t) + \\theta^2\\text{var}(\\epsilon_{t-1}) = (1+\\theta^2)\\sigma^2;\\\\\n\\gamma_1 &= \\text{cov}(y_t, y_{t-1}) = \\text{cov}(\\epsilon_{t} + \\theta\\epsilon_{t-1}, \\epsilon_{t-1} + \\theta\\epsilon_{t-2}) = \\text{cov} (\\theta\\epsilon_{t-1}, \\epsilon_{t-1} + \\theta\\epsilon_{t-2}) = \\theta\\sigma^2; \\\\\n\\gamma_2 &= \\text{cov}(y_t, y_{t-2}) = \\text{cov}(\\epsilon_{t} + \\theta\\epsilon_{t-1}, \\epsilon_{t-2} + \\theta\\epsilon_{t-3}) = 0; \\\\\n&\\vdots\\\\\n\\gamma_j &= 0 \\text{ for } |j|\\geq 2.\n\\end{aligned}\n\\]\nIt is clear that the MA(1) process is stationary. And the ACF cuts off after the first lag. Because more distant lags \\(y_{t-k}\\) are constituted by even more distant innovations \\(\\epsilon_{t-k}, \\epsilon_{t-k-1}, ...\\) which has no relevance for \\(y_t\\) given the MA(1) structure.\nWe have seen AR processes are equivalent to MA(\\(\\infty\\)) processes. Similar results hold for MA models. Rewrite the MA(1) process with the lag operator, assuming \\(|\\theta| < 1\\),\n\\[\ny_t = (1 + \\theta L) \\epsilon_t \\Leftrightarrow\n(1+\\theta L)^{-1} y_t = \\epsilon_t \\Leftrightarrow\n\\sum_{j=0}^{\\infty} (-\\theta)^j y_{t-j} = \\epsilon_t.\n\\]\nThat means an MA(1) is equivalent to an AR(\\(\\infty\\)) process if \\((1+\\theta L)\\) is invertible. This shows AR and MA are really the same family of models. The model AR or MA is chosen by parsimonious principle. For example, an AR model with many lags can possibly be modeled by a parsimonious MA model.\nSince an MA(1) is equivalent to some AR(\\(\\infty\\)) process, the PACF of an MA(1) should tail off gradually.\n\ny = arima.sim(list(ma=0.8), n=2000)\npar(mfrow=c(1,2), mar=c(1,4,1,1))\nacf(y); pacf(y)\n\n\n\n\nFigure 7.1: ACF and PACF of MA(1) process\n\n\n\n\n\n\n\n\n\n\nInvertibility\n\n\n\nIf \\(|\\theta|>1\\), \\(\\theta(L)\\) is not invertible. Define another MA(1) process,\n\\[\ny_t = \\epsilon_t + \\theta^{-1}\\epsilon_{t-1},\\quad\\epsilon_t\\sim\\text{WN}(0,\\theta^2\\sigma^2).\n\\]\nWe can verify that its variance and covariances are exactly the same as Equation 7.2. For non-invertible MA process, as long as \\(\\theta(L)\\) avoids unit root, we can always find an invertible process that shares the same ACF. This means, for a stationary MA process, it makes no harm to just assume it is invertible."
  },
  {
    "objectID": "07_ma.html#maq-process",
    "href": "07_ma.html#maq-process",
    "title": "7  MA Models",
    "section": "7.2 MA(q) Process",
    "text": "7.2 MA(q) Process\nA \\(q\\)-th order moving average, or MA(\\(q\\)) process, is written as\n\\[\ny_t = \\mu + \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\dots + \\theta_q\\epsilon_{t-q},\n\\tag{7.3}\\]\nwhere \\(\\{\\epsilon_t\\} \\sim \\text{WN}(0, \\sigma^2)\\).\n\nProposition 7.1 An MA(\\(q\\)) process is stationary.\n\n\nProof. We will show that the mean, variance and covariances of MA(\\(q\\)) are all invariant with time.\n\\[\n\\mathbb{E}(y_t)= \\mu.\n\\]\nAssume for the rest, \\(\\{y_t\\}\\) is demeaned.\n\\[\n\\begin{aligned}\n\\gamma_0 &= \\mathbb{E}(y_t^2) = \\mathbb{E}[(\\epsilon_t + \\theta_1\\epsilon_{t-1} + \\dots + \\theta_q\\epsilon_{t-q})^2] \\\\\n&= \\mathbb{E}[\\epsilon^2] + \\theta_1^2\\mathbb{E}[\\epsilon_{t-1}^2] + \\dots + \\theta_q^2\\mathbb{E}[\\epsilon_{t-q}^2] \\\\\n&= (1+\\theta_1^2+\\dots+\\theta_q^2)\\sigma^2; \\\\\n\\gamma_1 &= \\mathbb{E}[y_ty_{t-1}] = \\mathbb{E}[(\\epsilon_t + \\theta_1\\epsilon_{t-1} + \\dots + \\theta_q\\epsilon_{t-q}) \\\\\n&\\hspace{10.2em} (\\epsilon_{t-1} + \\dots + \\theta_{q-1}\\epsilon_{t-q} + \\theta_q\\epsilon_{t-q-1})] \\\\\n&= \\theta_1\\mathbb{E}[\\epsilon_{t-1}^2] + \\theta_2\\theta_1\\mathbb{E}[\\epsilon_{t-2}^2] + \\dots + \\theta_q\\theta_{q-1}\\mathbb{E}[\\epsilon_{t-q}^2] \\\\\n&= (\\theta_1 + \\theta_2\\theta_1 + \\dots + \\theta_q\\theta_{q-1})\\sigma^2;\\\\\n&\\vdots\\\\\n\\gamma_j &= \\mathbb{E}[y_ty_{t-j}] = \\mathbb{E}[(\\epsilon_t + \\dots + \\theta_j\\epsilon_{t-j} + \\dots + \\theta_q\\epsilon_{t-q}) \\\\\n&\\hspace{12.5em} (\\epsilon_{t-j} + \\dots + \\theta_{q-j}\\epsilon_{t-q} + \\dots + \\theta_q\\epsilon_{t-q-j})] \\\\\n&= \\theta_j\\mathbb{E}[\\epsilon_{t-j}^2] + \\theta_{j+1}\\theta_1\\mathbb{E}[\\epsilon_{t-j-1}^2] + \\dots + \\theta_q\\theta_{q-j}\\mathbb{E}[\\epsilon_{t-q}^2] \\\\\n&= (\\theta_j + \\theta_{j+1}\\theta_1 + \\dots + \\theta_q\\theta_{q-j})\\sigma^2, \\text{ for } j\\leq q; \\\\\n\\gamma_j &= 0, \\text{ for } j > q.\n\\end{aligned}\n\\]\n\n\nProposition 7.2 An MA(\\(q\\)) process is invertible iff the roots of \\(\\theta(z)\\) are outside the unit circle.\n\n\nProposition 7.3 An MA(\\(q\\)) process is characterized by (i) an ACF that is (close to) zero after \\(q\\) lags; and (i) a PACF that is infinite in extend but tails of gradually."
  },
  {
    "objectID": "07_ma.html#mainfty-process",
    "href": "07_ma.html#mainfty-process",
    "title": "7  MA Models",
    "section": "7.3 MA(\\(\\infty\\)) Process",
    "text": "7.3 MA(\\(\\infty\\)) Process\nMA(\\(\\infty\\)) is a special case deserves attention. Partly because all ARMA processes can be reduced to MA(\\(\\infty\\)) processes. In addition to MA(\\(q\\)) processes, we need more conditions for MA(\\(\\infty\\)) to be stationary. Consider the variance of\n\\[\ny_t = \\sum_{j=0}^{\\infty}\\theta_j\\epsilon_{t-j},\n\\]\n\\[\n\\gamma_0 = \\mathbb{E}[y_t^2] = \\mathbb{E}\\left[\\left(\\sum_{j=0}^{\\infty}\\theta_j\\epsilon_{t-j}\\right)^2\\right] = \\left(\\sum_{j=0}^{\\infty}\\theta_j^2\\right)\\sigma^2.\n\\]\nIt only make sense if \\(\\sum_{j=0}^{\\infty}\\theta_j^2<\\infty\\). This property is called square summable.\n\nProposition 7.4 An MA(\\(\\infty\\)) process is stationary if the coefficients \\(\\{\\theta_j\\}\\) are square summable."
  },
  {
    "objectID": "08_arma.html#armapq",
    "href": "08_arma.html#armapq",
    "title": "8  ARMA Models",
    "section": "8.1 ARMA(p,q)",
    "text": "8.1 ARMA(p,q)\nARMA(\\(p\\), \\(q\\)) is a mixed autoregressive and moving average process.\n\\[\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} +\n\\epsilon_t + \\theta_1\\epsilon_{t-1} + \\dots + \\theta_q\\epsilon_{t-q},\n\\]\nor\n\\[\n\\phi(L) y_t = \\theta(L) \\epsilon_t,\n\\]\nwhere \\(\\{\\epsilon_{t}\\} \\sim \\text{WN}(0, \\sigma^2)\\).\nThe MA part is always stationary as shown in Proposition 7.1. The stationarity of an ARMA process solely depends on the AR part. The condition is the same as Proposition 6.2.\nAssume \\(\\phi^{-1}(L)\\) exist, then the ARMA(\\(p\\),\\(q\\)) process can be reduce to MA(\\(\\infty\\)) process:\n\\[\ny_t = \\phi^{-1}(L)\\theta(L)\\epsilon_t = \\psi(L) \\epsilon_t,\n\\]\nwhere \\(\\psi(L) = \\phi^{-1}(L)\\theta(L)\\).\n\n\n\n\n\n\nExercise\n\n\n\nCompute the MA equivalence for ARMA(1,1)."
  },
  {
    "objectID": "08_arma.html#arimapdq",
    "href": "08_arma.html#arimapdq",
    "title": "8  ARMA Models",
    "section": "8.2 ARIMA(p,d,q)",
    "text": "8.2 ARIMA(p,d,q)\nARMA(\\(p\\),\\(q\\)) is used to model stationary time series. If \\(y_t\\) is not stationary, we can transform it to stationary and model it with an ARMA model. If the first-order difference \\((1-L)y_t = y_t - y_{t-1}\\) is stationary, then we say \\(y_t\\) is integrated of order 1. If it requires \\(d\\)-th order difference to be stationary, \\((1-L)^dy_t\\), we say it is integrated of order \\(d\\). The ARMA model involves integrated time series is called ARIMA model:\n\\[\n\\phi(L)(1-L)^d y_t = \\theta(L)\\epsilon_t.\n\\]"
  },
  {
    "objectID": "09_wold.html#wold-decomposition",
    "href": "09_wold.html#wold-decomposition",
    "title": "9  Wold Theorem",
    "section": "9.1 Wold Decomposition",
    "text": "9.1 Wold Decomposition\nSo far we have spent a lot of effort with ARMA models, which are the indispensable components of any time series textbook. The following theorem justifies its importance. The Wold Decomposition Theorem basically says every covariance-stationary process has an ARMA representation. Therefore, with long enough lags, any covariance-stationary process can be approximated arbitrarily well by ARMA models. This is a very bold conclusion to make. It sets up the generality of ARMA models, which makes it one of the most important theorems in time series analysis.\n\nTheorem 9.1 (Wold Decomposition Theorem) Every covariance-stationary time series \\(y_t\\) can be written as the sum of two time series, one deterministic and one stochastic. Formally,\n\\[\ny_t = \\eta_t + \\sum_{j=0}^{\\infty} b_j\\epsilon_{t-j},\n\\]\nwhere \\(\\eta_t \\in I_{-\\infty}\\) is a deterministic time series (such as one represented by a sine wave); \\(\\epsilon_t\\) is an uncorrelated innovation sequence with \\(\\mathbb{E}[\\epsilon_t]=0\\), \\(\\mathbb{E}[\\epsilon_t\\epsilon_{t-j}]=0\\) for \\(j\\neq 0\\); and \\(\\{b_j\\}\\) are square summable, \\(\\sum_{j=0}^{\\infty}|b_j|^2<\\infty\\).\n\n\nProof. We will prove the theorem by constructing the innovation sequence \\(\\{e_t\\}\\) and showing it satisfies the conditions stated. Let \\(e_t = y_t - \\hat{\\mathbb{E}}(y_t|I_{t-1}) = y_t - a(L)y_{t-1}\\), where \\(\\hat{\\mathbb{E}}(y_t|I_{t-1})\\) is the best linear predictor (BLP) of \\(y_t\\) based on information set at \\(t-1\\). \\(a(L)\\) does not depend on \\(t\\) because \\(y_t\\) is covariance stationary. As the best linear predictor, \\(a(L)\\) solves\n\\[\\min_{\\{a_j\\}} \\mathbb{E} (y_t - \\sum_{j=1}^{\\infty}a_jy_{t-j})^2.\\] The first-order conditions with respect to \\(a_j\\) gives\n\\[\n\\begin{aligned}\n\\mathbb{E}[y_{t-j}(y_t-\\sum_{j=1}^{\\infty}a_jy_{t-j})] &= 0, \\\\\n\\implies \\mathbb{E}[y_{t-j}e_t] &=0.\n\\end{aligned}\n\\]\nWe now verify that \\(e_t\\) satisfies the white noise conditions. Without loss of generality, we may assume \\(\\mathbb{E}(y_t)=0\\), it follows that \\(\\mathbb{E}(e_t)=0\\). \\(\\text{var}(e_t)=\\mathbb{E}(y_t-a(L)y_t)^2\\) is a function of covariance of \\(y_t\\) and \\(a_j\\), none of which varies with time. So \\(\\text{var}(e_t)=\\sigma^2\\) is constant. Utilizing the first-order condition, \\(\\mathbb{E}[e_te_{t-j}] = \\mathbb{E}[e_t(y_{t-j}-a(L)y_{t-j})] = 0.\\)\nRepeatedly substituting for \\(y_{t-k}\\) gives\n\\[\n\\begin{aligned}\ny_t &= e_t + \\sum_{k=1}^{\\infty} a_ky_{t-k} \\\\\n&= e_t + a_1(e_{t-1} + \\sum_{k=1}^{\\infty} a_ky_{t-1-k}) + \\sum_{k=2}^{\\infty} a_ky_{t-k}\\\\\n&= e_t + a_1 e_{t-1} + \\sum_{k=1}^{\\infty} \\tilde{a}_ky_{t-k-1} \\\\\n&= e_t + a_1 e_{t-1} + \\eta_t^1 \\\\\n&\\quad\\vdots\\\\\n&= \\sum_{j=0}^{k} c_j e_{t-j} + \\eta_t^k,\n\\end{aligned}\n\\] where \\(\\eta_t^k \\in I_{t-k-1}\\). As \\(k\\to\\infty\\), we have \\(v_t = y_t - \\sum_{j=0}^{\\infty}c_je_{t-j} \\in I_{-\\infty}\\).\n\nLet’s appreciate this theorem for a while. The property of stationarity can be loosely understood as having stable patterns over time. The Wold Theorem states that any such patterns can be captured by ARMA models. In other words, ARMA models are effective in modelling stable patterns repeated over time, in so far as only 2nd-order moments are of concern. Even if the time series is not entirely stationary, if we model it with ARMA, it can be thought as extracting the stationary patterns. Figure 9.1 demonstrates the ARIMA modelling of monthly export.\n\nlibrary(zoo)\ndata = read.csv.zoo(\"data/md.csv\", FUN = as.yearmon, regular = TRUE)\ny = data$Export\nmod = arima(y, order = c(2,0,1))\nyhat = y - mod$residuals\nplot(cbind(y, yhat), plot.type = \"s\", col = 1:2, ann = F)\n\n\n\n\nFigure 9.1: Monthly export modelled with ARIMA(2,0,1)"
  },
  {
    "objectID": "09_wold.html#causality-and-invertibility",
    "href": "09_wold.html#causality-and-invertibility",
    "title": "9  Wold Theorem",
    "section": "9.2 Causality and Invertibility*",
    "text": "9.2 Causality and Invertibility*\nWe have seen that AR models can be rewritten as MA models and vice versa, suggesting the ARMA representation of a stochastic process is not unique. We have also seen that a non-invertible MA process can be equivalently represented by an invertible MA process. For example, the following MA(1) processes have the same ACF:\n\\[\n\\begin{aligned}\nx_t &= w_t + \\frac{1}{5} w_{t-1}, & w_t\\sim\\text{WN}(0,25);\\\\\ny_t &= v_t + 5 v_{t-1}, & v_t\\sim\\text{WN}(0, 1).\n\\end{aligned}\n\\]\nThe same property holds for AR processes. In Chapter 6, we state that an AR(1) process is explosive if \\(|\\phi|>1\\). This is not entirely rigorous. Consider an AR(1) process,\n\\[\ny_t = \\phi y_{t-1} + \\epsilon_t, \\text{ where } |\\phi| > 1.\n\\]\nMultiply both sides by \\(\\phi^{-1}\\),\n\\[\n\\phi^{-1} y_t = y_{t-1} + \\phi^{-1}\\epsilon_t,\n\\]\nRewrite it as an MA process,\n\\[\n\\begin{aligned}\ny_t &= \\phi^{-1} y_{t+1} - \\phi^{-1}\\epsilon_{t+1} \\\\\n&= \\phi^{-1} (\\phi^{-1} y_{t+2} - \\phi^{-1}\\epsilon_{t+2}) - \\phi^{-1}\\epsilon_{t+1} \\\\\n&\\;\\vdots \\\\\n&= \\sum_{j=1}^{\\infty} -\\phi^{-j}\\epsilon_{t+j}.\n\\end{aligned}\n\\]\nGiven \\(|\\phi^{-1}|<1\\), the process is stationary, expressed as discounted innovations in the future (despite this looks quite odd). In fact, for an non-causal AR process, we can find a causal AR process that generates the same ACF (remember the term causal means an AR process can be converted to an MA process with absolute summable coefficients).\nThe problem is given an ARMA equation, it is not enough to uniquely pin down a stochastic process. Both the explosive process and the stationary process can be a solution to \\(y_t = \\phi y_{t-1} + \\epsilon_t\\). But for a stationary process expressed as an AR model with \\(|\\phi|>1\\), we can always find an AR(1) process with \\(|\\tilde\\phi|<1\\) and a different white noise sequence \\(\\{\\tilde\\epsilon_t\\}\\) that generate the same ACF.\nThe following theorems state the conditions for the existence of stationary solutions, and the possibility of rewriting non-causal or non-invertible ARMA representations as causal and invertible ones. Since it is always possible to do so, it loses nothing to stick with causal and invertible ARMA processes when modelling stationary time series.\n\nTheorem 9.2 A unique stationary solution to the ARMA process \\(\\phi(L)y_t = \\theta(L)\\epsilon_t\\) exists iff \\(\\phi\\) and \\(\\theta\\) have no common factors and the roots of \\(\\phi(z)\\) avoid the unit circle:\n\\[\n|\\phi(z)|=1 \\implies \\phi(z) = 1-\\phi_1z-\\dots-\\phi_pz^p \\neq 0.\n\\]\n\n\nTheorem 9.3 Let \\(\\{y_t\\}\\) be a stationary ARMA process defined by \\(\\phi(L)y_t = \\theta(L)\\epsilon_t\\). If the roots of \\(\\theta(z)\\) avoid unit circle, then there are polynomials \\(\\tilde{\\phi}\\) and \\(\\tilde\\theta\\) and a white noise sequence \\(\\tilde\\epsilon\\) such that \\(\\{y_t\\}\\) satisfies \\(\\tilde\\phi(L)y_t = \\tilde\\theta(L)\\tilde\\epsilon_t\\), and this is a causal and invertible ARMA process."
  },
  {
    "objectID": "10_ols.html#chapter-overview",
    "href": "10_ols.html#chapter-overview",
    "title": "10  Preliminaries",
    "section": "10.1 Chapter Overview",
    "text": "10.1 Chapter Overview\nThis chapter serves two purposes. One is to introduce the techniques for estimating time series models. The other is to explain the concept of dynamic causal effect. We join the two topics in one chapter because both of them can be done via a regression framework. Maximum likelihood estimation plays a pivotal role in estimating time series models. Nonetheless, starting with OLS always make things easier. We start with a quick review of the basic OLS concepts that are familiar to any students in econometrics, that is the regressions applied to cross-sectional \\(iid\\) observations. We then extend it to time series data. We will see it is not as straightforward as one might expect, as intertemporal dependencies between observation need additional treatment. In the second half of the chapter, we will explain the concept of dynamic causal effect, that is the causal effect of an intervention on outcome variables. Similar to cross-sectional studies, we need to define the causal effect relative to counterfactuals. With time series data, the counterfactuals have to be defined across time rather across individuals."
  },
  {
    "objectID": "10_ols.html#asymptotic-theorems-for-i.i.d-random-variables",
    "href": "10_ols.html#asymptotic-theorems-for-i.i.d-random-variables",
    "title": "10  Preliminaries",
    "section": "10.2 Asymptotic Theorems for i.i.d Random Variables",
    "text": "10.2 Asymptotic Theorems for i.i.d Random Variables\n\nTheorem 10.1 (Law of Large Numbers) Let \\(\\{x_i\\}\\) be \\(iid\\) random variables with \\(\\mathbb{E}(x_i)=\\mu\\) and \\(\\text{Var}(x_i)=\\sigma^2<\\infty\\). Define \\(\\bar{x}_n = \\frac{1}{n}\\sum_{i=1}^n x_i\\). Then \\(\\bar{x}_n \\overset{p}{\\to} \\mu\\) as \\(n \\to \\infty\\).\n\n\nProof. We will give an non-rigorous proof, but nonetheless shows the tenets. It is easy to see \\(\\mathbb{E}(\\bar{x}_n) = \\mu\\). Consider the variance,\n\\[\n\\text{Var}(\\bar x_n) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n x_i\\right) \\overset{iid}= \\frac{1}{n^2}\\sum_{i=1}^n\\text{Var}(x_i) = \\frac{\\sigma^2}{n}\\to 0.\n\\]\nThat is \\(\\bar x_n\\) converges to \\(\\mu\\) with probability 1 as \\(n\\to\\infty\\). Note that we can move the variance inside the summation operator because \\(x_i\\) are \\(iid\\), in which all the covariance terms are 0.\n\n\nTheorem 10.2 (Central Limit Theorem) Let \\(\\{x_i\\}\\) be \\(iid\\) random variables with \\(\\mathbb{E}(x_i)=\\mu\\) and \\(\\text{Var}(x_i)=\\sigma^2<\\infty\\). Define \\(\\bar{x}_n = \\frac{1}{n}\\sum_{i=1}^n x_i\\). Then\n\\[\n\\frac{\\bar x_n - \\mu}{\\sigma/\\sqrt{n}} \\overset{d}\\to N(0,1).\n\\]\n\n\nProof. Without loss of generality, assume \\(x_i\\) is demeaned and standardized to have standard deviation 1. It remains to show \\(\\sqrt{n} \\bar x_n \\to N(0,1)\\). Define the moment generating function (MGF) for \\(\\sqrt{n}\\bar x_n\\):\n\\[M_{\\sqrt{n}\\bar x_n}(t) =\\mathbb{E}[e^{(\\sqrt{n}^{-1}\\sum_{i=1}^n x_i)t}] \\overset{iid}= \\{\\mathbb{E}[e^{(n^{-1/2}x_i)t}]\\}^n.\\]\nEvaluate the MGF for each \\(x_i\\):\n\\[\n\\mathbb{E}[e^{(n^{-1/2}x_i)t}] = 1 + \\mathbb{E}(n^{-1/2}x_i)t + \\mathbb{E}(n^{-1}x_i^2)t^2 + \\cdots =1 + \\frac{t^2}{2n} + o(n^{-1}).\n\\]\nSubstituting back,\n\\[\nM_{\\sqrt{n}\\bar x_n}(t) = \\left[1 + \\frac{t^2}{2n} + o(n^{-1})\\right]^n = \\left[\\left(1 + \\frac{t^2}{2n} \\right)^{\\frac{2n}{t^2}}\\right]^{\\frac{t^2}{2}}\\to e^{\\frac{t^2}{2}}.\n\\]\nNote that we drop the \\(o(n^{-1})\\) because it converges faster than \\(\\frac{1}{n}\\). \\(e^{\\frac{t^2}{2}}\\) is the MGF for standard normal distribution. Hence, the theorem is proved."
  },
  {
    "objectID": "10_ols.html#ols-for-i.i.d-random-variables",
    "href": "10_ols.html#ols-for-i.i.d-random-variables",
    "title": "10  Preliminaries",
    "section": "10.3 OLS for i.i.d Random Variables",
    "text": "10.3 OLS for i.i.d Random Variables\nWe now give a very quick review of OLS with \\(iid\\) random variables. These materials are assumed familiar to the readers. We do not intend to introduce them in any detail. This section is a quick snapshot of some key concepts, so that we could contrast them with the time series regression introduced in the next section.\nA linear regression model postulates the joint distribution of \\((y_i, x_i)\\) follows a linear relationship,\n\\[\ny_i = x_i'\\beta + \\epsilon_i.\n\\]\nExpressed in terms of data matrix,\n\\[\n\\begin{bmatrix}\ny_1\\\\y_2\\\\\\vdots\\\\y_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nx_{11}, x_{12}, \\dots, x_{1p}\\\\\nx_{21}, x_{22}, \\dots, x_{2p}\\\\\n\\ddots \\\\\nx_{n1}, x_{n2}, \\dots, x_{np}\\\\\n\\end{bmatrix}'\n\\begin{bmatrix}\n\\beta_1\\\\\\beta_2\\\\\\vdots\\\\\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_1\\\\\\epsilon_2\\\\\\vdots\\\\\\epsilon_n\n\\end{bmatrix}.\n\\]\nFrom the perspective of dataset, the matrix matrix is fixed in the sense that they are just numbers in the dataset. But for statistical analysis, we view each entry in the matrix as random, that is as a realization of a random process.\nTo estimate the parameter \\(\\beta\\) from sample data, OLS seeks to minimize the squared residuals\n\\[\n\\min_{\\beta} \\sum_{i=1}^{n}(y_i - x_i'\\beta)^2.\n\\]\nThe first-order condition implies,\n\\[\n\\begin{aligned}\n&\\sum_i x_i(y_i - x_i'\\beta) =0, \\\\\n&\\sum_i x_iy_i - \\sum_i x_ix_i'\\beta=0, \\\\\n&\\hat\\beta = \\left(\\sum_i x_ix_i'\\right)^{-1} \\left(\\sum_i x_iy_i\\right) \\\\\n&= \\beta + \\left(\\sum_i x_ix_i'\\right)^{-1} \\left(\\sum_i x_i\\epsilon_i\\right).\n\\end{aligned}\n\\]\nUnder the Gauss-Markov assumptions, particularly \\(\\mathbb{E}(\\epsilon_i|x_j)=0\\) and \\(\\text{var}(\\epsilon|X)=\\sigma^2 I\\) (homoskedasticity and nonautocorrelation), the OLS estimator is BLUE (Best Linear Unbiased Estimator).\nUnder the assumption of \\(iid\\) random variables and homoskedasticity, we invoke the LLN and CLT to derive the asymptotic distribution for the OLS estimator,\n\\[\n\\begin{aligned}\n\\sqrt{n}(\\hat\\beta-\\beta) &= \\left(\\frac{1}{n}\\sum_i x_ix_i'\\right)^{-1} \\left(\\sqrt{n}\\frac{1}{n}\\sum_i x_i\\epsilon_i\\right) \\\\[1em]\n&\\to [\\mathbb{E}(x_ix_i')]^{-1} \\text{N}(0, \\mathbb{E}(x_i\\epsilon_i\\epsilon_i'x_i')) \\\\[1em]\n&\\to \\text{N}(0, \\sigma^2[\\mathbb{E}(x_ix_i')]^{-1}).\n\\end{aligned}\n\\]\nNote how the \\(iid\\) assumption is required throughout the process. The following section will show how to extend the OLS to non-\\(iid\\) random variables and how it leads to modification of the results."
  },
  {
    "objectID": "11_ols2.html#asymptotic-theorems-for-dependent-random-variables",
    "href": "11_ols2.html#asymptotic-theorems-for-dependent-random-variables",
    "title": "11  OLS for Time Series",
    "section": "11.1 Asymptotic Theorems for Dependent Random Variables",
    "text": "11.1 Asymptotic Theorems for Dependent Random Variables\nThe asymptotic theorems and regressions that work for \\(iid\\) random variable do not immediately apply to time series. Consider the proof for Theorem 10.1, without the \\(iid\\) assumption we have\n\\[\n\\begin{aligned}\n\\text{var}\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_i\\right)\n&= \\frac{1}{n^2}\\sum_{i=1}^n\\sum_{j=1}^n \\text{cov}(x_i, x_j) \\\\\n&= \\frac{1}{n^2}[\\text{cov}(x_1, x_1) + \\text{cov}(x_1, x_2) + \\cdots + \\text{cov}(x_1, x_n)+ \\\\\n&\\hspace{3em} \\text{cov}(x_2, x_1) + \\text{cov}(x_2, x_2) + \\cdots + \\text{cov}(x_2, x_n)+ \\\\\n&\\hspace{3em}\\vdots\\\\\n&\\hspace{3em} \\text{cov}(x_n, x_1) + \\text{cov}(x_n, x_2) + \\cdots + \\text{cov}(x_n, x_n)] \\\\\n&= \\frac{1}{n^2} [n\\gamma_0 + 2(n-1)\\gamma_1 + 2(n-2)\\gamma_1 + 2(n-2)\\gamma_2 + \\dots] \\\\\n&= \\frac{1}{n} \\left[2\\sum_{k=1}^n \\gamma_k\\left(1-\\frac{k}{n}\\right) + \\gamma_0 \\right].\n\\end{aligned}\n\\]\nThe argument for the \\(iid\\) does not work with the presence of serial correlations. If we assume absolute summability, \\(\\sum_{j=-\\infty}^{\\infty} |\\gamma_j| <\\infty\\), then\n\\[\n\\lim_{n\\to\\infty} \\frac{1}{n} \\left[2\\sum_{k=1}^n \\gamma_k\\left(1-\\frac{k}{n}\\right) + \\gamma_0 \\right] =0.\n\\]\nIn this case, we still have the LLN holds. Otherwise, as the variance may not converge. Remember Theorem 4.1, absolute summability implies the series is ergodic.\n\nProposition 11.1 If \\(x_t\\) is a covariance stationary time series with absolutely summable auto-covariances, then a Law of Large Numbers holds.\n\nFrom the new proof of LLN one can guess that the variance in a Central Limit Theorem should also change. The serially correlated \\(x_t\\), the liming variance is given by\n\\[\n\\begin{aligned}\n\\text{var}\\left(\\frac{1}{\\sqrt n}\\sum_{i=1}^{n}x_i\\right)\n&= 2\\sum_{k=1}^n \\gamma_k\\left(1-\\frac{k}{n}\\right) + \\gamma_0 \\\\\n&\\to 2\\sum_{k=1}^{\\infty} \\gamma_k + \\gamma_0 = \\sum_{k=-\\infty}^{\\infty}\\gamma_k = S.\n\\end{aligned}\n\\]\nWe call \\(S\\) the long-run variance. There are many CLTs for serially correlated observations. We give the two mostly commonly cited versions: one applies to MA(\\(\\infty\\)) processes, the other one is more general.\n\nTheorem 11.1 Let \\(y_t\\) be an MA process: \\(y_t = \\mu + \\sum_{j=0}^\\infty c_j\\epsilon_{t-j}\\) where \\(\\epsilon_t\\) is independent white noise and \\(\\sum_{j=0}^{\\infty} |c_j|<\\infty\\) (this implies ergodic), then\n\\[\n\\sqrt{T} \\bar y_t \\overset{d}\\to N(0, S),\n\\]\nwhere \\(S = \\sum_{k=-\\infty}^{\\infty}\\gamma_k\\) is the long-run variance.\n\n\nTheorem 11.2 (Gordin’s CLT) Assume we have a strictly stationary and ergodic series \\(\\{y_t\\}\\) with \\(\\mathbb{E}(y_t^2) <\\infty\\) satisfying: \\(\\sum_j\\{\\mathbb{E}[\\mathbb{E}[y_t|I_{t-j}]-\\mathbb{E}[y_t|I_{t-j-1}]]^2\\}^{1/2} <\\infty\\) and \\(\\mathbb{E}[y_t|I_{t-j}]\\to 0\\) as \\(j\\to\\infty\\), then\n\\[\n\\sqrt{T} \\bar y_t \\overset{d}\\to N(0, S),\n\\]\nwhere \\(S = \\sum_{k=-\\infty}^{\\infty}\\gamma_k\\) is the long-run variance.\n\nThe Gordin’s conditions are intended to make the dependence between distant observations to decrease to 0. ARMA process is a special case of Gordin series. The essence of these theorems is that we need some restrictions on dependencies for LLN and CLT to hold. We allow serial correlations as long as they are not too strong. If the observations become almost independent as they are far away in time, the can still apply the asymptotic theorems."
  },
  {
    "objectID": "11_ols2.html#ols-for-time-series",
    "href": "11_ols2.html#ols-for-time-series",
    "title": "11  OLS for Time Series",
    "section": "11.2 OLS for Time Series",
    "text": "11.2 OLS for Time Series\n\nDefinition 11.1 Given a time series regression model\n\\[y_t = x_t'\\beta + \\epsilon_t, \\]\n\\(x_t\\) is weakly exogenous if\n\\[ \\mathbb{E}(\\epsilon_t | x_t, x_{t-1}, ...) = 0;\\]\n\\(x_t\\) is strictly exogenous if\n\\[ \\mathbb{E}(\\epsilon_t | \\{x_t\\}_{t=-\\infty}^{\\infty}) = 0. \\]\n\nStrictly exogeneity requires innovations being exogenous from all past and future regressors; while weakly exogeneity only requires being exogenous from past regressors. In practice, strict exogeneity is too strong as an assumption. The weak exogenous is more practical and it is enough to ensure the consistency of the OLS estimator.\nThe OLS estimator is as usual:\n\\[\\hat\\beta = \\beta + \\left(\\frac{1}{n}\\sum_t x_t x_t'\\right)^{-1} \\left(\\frac{1}{n}\\sum_t x_t\\epsilon_t\\right).\\]\nAssuming LLN holds and \\(x_t\\) is weakly exogenous, we have\n\\[\n\\begin{aligned}\n\\frac{1}{n}\\sum_t x_t x_t' &\\to \\mathbb{E}(x_t x_t') = Q,\\\\\n\\frac{1}{n}\\sum_t x_t\\epsilon_t &\\to \\mathbb{E}(x_t\\epsilon_t) = \\mathbb{E}[x_t\\mathbb{E}[\\epsilon_t|x_t]] = 0.\n\\end{aligned}\n\\]\nTherefore, \\(\\hat\\beta \\to \\beta\\). The OLS estimator is consistent.\nAssuming the Gordin’s conditions hold for \\(z_t=x_t\\epsilon_t\\), the CLT gives\n\\[\n\\frac{1}{\\sqrt n}\\sum_t x_t\\epsilon_t \\to N(0,S),\n\\]\nwhere \\(S = \\sum_{-\\infty}^{\\infty}\\gamma_j\\) is the long-run variance for \\(z_t\\). Thus, we have the asymptotic normality for the OLS estimator\n\\[\n\\sqrt{T}(\\hat\\beta - \\beta) \\to N(0,Q^{-1}SQ^{-1}).\n\\]\nNote how the covariance matrix \\(S\\) is different from the one in the \\(iid\\) case where \\(S=\\sigma^2\\mathbb{E}(x_ix_i')\\). The long-run variance \\(S\\) takes into account the auto-dependencies between observations. The auto-dependencies usually arise from the serially correlated error terms. It may also arise from \\(x_t\\) being autocorrelated and from conditional heteroskedasticity of the error terms. Because of the auto-covariance structure, \\(S\\) cannot be estimated in the same way as in the \\(iid\\) case. The estimator for \\(S\\) is called HAC (heteroskedasticity autocorrelation consistent) standard errors."
  },
  {
    "objectID": "11_ols2.html#hac-standard-errors",
    "href": "11_ols2.html#hac-standard-errors",
    "title": "11  OLS for Time Series",
    "section": "11.3 HAC Standard Errors",
    "text": "11.3 HAC Standard Errors\n\\(S\\) can be estimated with truncated autocovariances,\n\\[\n\\hat S = \\sum_{j=-h(T)}^{h(T)} \\hat\\gamma_j.\n\\]\n\\(h(T)\\) is a function of \\(T\\) and \\(h(T)\\to\\infty\\) as \\(T\\to\\infty\\), but more slowly. Because we don’t want to include too many imprecisely estimated covariances. Another problem is the estimated \\(\\hat S\\) might be negative. The solution is weight the covariances in a way to ensure positiveness:\n\\[\n\\hat S = \\sum_{j=-h(T)}^{h(T)} k_T(j) \\hat\\gamma_j.\n\\]\n\\(k_T(\\cdot)\\) is called a kernel. The weights are chosen to guarantee positive-definiteness by weighting down high lag covariances. Also we need \\(k_T(\\cdot)\\to 1\\) for consistency.\nA popular HAC estimator is the Newey-West variance estimator, in which \\(h(T) = 0.75 T^{1/3}\\) and \\(k_T(j) = \\frac{h-j}{h}\\), so that\n\\[\n\\hat S = \\sum_{j=-h}^{h} \\left(\\frac{h-j}{h}\\right)\\hat\\gamma_j.\n\\]"
  },
  {
    "objectID": "11_ols2.html#example",
    "href": "11_ols2.html#example",
    "title": "11  OLS for Time Series",
    "section": "11.4 Example",
    "text": "11.4 Example\nNote that all of our discussions in this chapter apply only to stationary time series. Without stationarity, even the autocovariance \\(\\gamma_j\\) might not be well-defined. In the following example, we generate artificial data from an AR(2) process, and recover the parameters by regression \\(y_t\\) on its lags.\n\nlibrary(lmtest)\ny = arima.sim(list(ar = c(0.5, 0.3)), n = 1000)\nmod = lm(y ~ ., data = cbind(y, lag(y,-1), lag(y,-2)))\ncoeftest(mod, vcov. = sandwich::NeweyWest(mod))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(>|t|)    \n(Intercept)  -0.014405   0.031012 -0.4645    0.6424    \n`lag(y, -1)`  0.550817   0.031510 17.4808 < 2.2e-16 ***\n`lag(y, -2)`  0.234545   0.032602  7.1942 1.235e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "12_mle.html",
    "href": "12_mle.html",
    "title": "12  MLE for ARMA Models",
    "section": "",
    "text": "OLS can only be used to estimate AR models, but not MA models. MA models or ARMA models in general can be estimated using maximum likelihood approach. Maximum likelihood estimation (MLE) starts with an assumed distribution of the random variables. The parameters are chosen to maximize the likelihood of observing the data under the distribution.\nConsider an ARMA(\\(p\\), \\(q\\)) model\n\\[\ny_t = \\phi_1 y_{t-1} +\\dots + \\phi_p y_{t-p} + u_t + \\theta_1 u_{t-1} + \\dots + \\theta_q u_{t-q}\n\\]\nWrite in the form of data matrix:\n\\[\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\y_T\n\\end{bmatrix}=\n\\underbrace{\n\\begin{bmatrix}\ny_0 & y_{-1} & \\dots & y_{1-p} \\\\\ny_1 & y_{0} & \\dots & y_{2-p} \\\\\ny_2 & y_{1} & \\dots & y_{3-p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_T & y_{T-1} & \\dots & y_{T-p} \\\\\n\\end{bmatrix}\n}_{\\boldsymbol X}\n\\begin{bmatrix}\n\\phi_1 \\\\ \\phi_2 \\\\ \\phi_3 \\\\ \\vdots \\\\ \\phi_p\n\\end{bmatrix} +\n\\underbrace{\n\\begin{bmatrix}\n1 & 0 & 0 & \\dots & 0 \\\\\n\\theta_1 & 1 & 0 & \\dots & 0 \\\\\n\\theta_2 & \\theta_1 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & & \\ddots & \\vdots \\\\\n0  & \\dots & \\theta_2 & \\theta_1 & 1\n\\end{bmatrix}\n}_{\\boldsymbol\\Gamma}\n\\begin{bmatrix}\nu_1 \\\\ u_2 \\\\ u_3 \\\\ \\vdots \\\\ u_T\n\\end{bmatrix}\n\\]\nOr compactly,\n\\[\n\\boldsymbol{y = X\\phi + \\Gamma u}.\n\\]\nWe assume the innovations are jointly normal \\(\\boldsymbol u \\sim N(0,\\sigma^2\\boldsymbol I)\\). We also assume the first \\(p\\) observations are known initial values \\(y_0, y_{-1},\\dots,y_{1-p}\\) and \\(u_0 = u_{-1} = \\dots = u_{1-q} = 0\\). Therefore, the observed data are jointly normal given the initial condition,\n\\[\n\\boldsymbol{y|y_0} \\sim N(\\boldsymbol{X\\phi}, \\sigma^2\\boldsymbol{\\Gamma\\Gamma'}).\n\\]\nThe probability density function for multivariate normal is\n\\[\nf(\\boldsymbol{y} | \\boldsymbol{y_0}, \\boldsymbol{\\phi}, \\boldsymbol{\\Gamma},\\sigma^2) = (2\\pi)^{-T/2}|\\sigma^2\\boldsymbol{\\Gamma\\Gamma'}|^{-1/2}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{y-X\\phi})' (\\sigma^2\\boldsymbol{\\Gamma\\Gamma'})^{-1} (\\boldsymbol{y-X\\phi}) \\right)\n\\]\nTo simplify computation, take logarithm to get the log-likelihood function\n\\[\n\\ell(\\boldsymbol{\\phi},\\boldsymbol{\\Gamma},\\sigma^2|\\boldsymbol{y},\\boldsymbol{y_0}) = -\\frac{T}{2}\\ln(2\\pi) -\\frac{1}{2}\\ln|\\sigma^2\\boldsymbol{\\Gamma\\Gamma'}|-\\frac{1}{2\\sigma^2}(\\boldsymbol{y-X\\phi})' (\\boldsymbol{\\Gamma\\Gamma'})^{-1} (\\boldsymbol{y-X\\phi}).\n\\]\nThe parameters are then chosen to maximize this log-likelihood function, i.e. the probability of observing the data under the assumed distribution. This can be done by conducting a grid search over the parameter space using a computer. To reduce the seach dimensions, we may concentrate the log-likelihood by computing the first-order conditions:\n\\[\n\\frac{\\partial\\ell}{\\partial\\boldsymbol\\phi}=0 \\implies \\boldsymbol{\\hat\\phi} = (\\boldsymbol X'(\\boldsymbol{\\hat\\Gamma\\hat\\Gamma'})^{-1}\\boldsymbol X)^{-1}\\boldsymbol X'(\\boldsymbol{\\hat\\Gamma\\hat\\Gamma'})^{-1}\\boldsymbol y\n\\]\n\\[\n\\frac{\\partial\\ell}{\\partial\\sigma^2}=0 \\implies \\hat\\sigma^2 =\\frac{1}{T} (\\boldsymbol{y-X\\hat\\phi})' (\\boldsymbol{\\hat\\Gamma\\hat\\Gamma'})^{-1} (\\boldsymbol{y-X\\hat\\phi})\n\\]\nThia allows us to focus our search only on \\(\\boldsymbol\\phi\\)."
  },
  {
    "objectID": "13_fc.html#intuitive-approach",
    "href": "13_fc.html#intuitive-approach",
    "title": "13  Forecasting",
    "section": "13.1 Intuitive Approach",
    "text": "13.1 Intuitive Approach\nSuppose we have an AR(1) process,\n\\[\ny_t = \\phi y_{t-1} + \\epsilon_t, \\quad\\epsilon_t\\sim\\text{WN}(0,\\sigma^2).\n\\]\nWhat would be the reasonable forecast for \\(y_{T+1}\\) given \\(y_1,...,y_T\\)? It seems sensible to simply drop the white noise, as it is something completely unpredictable and it has mean zero. Thus,\n\\[\n\\hat y_{T+1|T} = \\phi y_t.\n\\]\nThis is 1-period ahead forecast. But how do we forecast \\(k\\)-period ahead? Heuristically, we can simply iterate over to the future:\n\\[\n\\begin{aligned}\n\\hat y_{T+2|T} &= \\phi\\hat y_{T+1|T} = \\phi^2 y_T, \\\\\n\\hat y_{T+h|T} &= \\phi\\hat y_{T+h-1} = \\cdots = \\phi^h y_T.\n\\end{aligned}\n\\]\nWe will leave the heuristic solutions here and justify them later. If we accept this heuristic approach, we can easily generalize it to AR(\\(p\\)) processes:\n\\[\n\\begin{aligned}\ny_t &= \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t. \\\\\n\\hat y_{T+1|T} &= \\phi_1 y_{T} + \\phi_2 y_{T-1} + \\dots + \\phi_p y_{T-p+1}, \\\\\n\\hat y_{T+2|T} &= \\phi_1\\hat y_{T+1|T} + \\phi_2 y_{T} + \\dots + \\phi_p y_{T-p+2}, \\\\\n&\\vdots \\\\\n\\hat y_{T+h|T} &= \\phi_1\\hat y_{T+h-1|T} + \\phi_2\\hat y_{T+h-2|T} + \\dots + \\phi_p y_{T-p+h}. \\\\\n\\end{aligned}\n\\]\nFor MA(\\(q\\)) processes\n\\[\ny_t = \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2} + \\cdots + \\theta_q\\epsilon_{t-q}  \n\\] Suppose we know the past innovations until \\(T\\): \\(\\epsilon_{T}, \\epsilon_{T-1}, ...\\) The best way to forecast \\(\\hat y_{T+h|T}\\) looks to simply discard \\(\\epsilon_{T+1},...,\\epsilon_{T+h}\\). Since we have no knowledge about future innovations given the information at time \\(T\\). Therefore,\n\\[\n\\begin{array}{llll}\n\\hat y_{T+1|T} &= \\theta_1\\epsilon_T + &\\theta_2\\epsilon_{T-1} + &\\theta_3\\epsilon_{T-2} + \\cdots \\\\\n\\hat y_{T+2|T} &=  &\\theta_2\\epsilon_T + &\\theta_3\\epsilon_{T-1} + \\cdots \\\\\n&\\vdots & & \\\\\n\\hat y_{T+h|T} &=  & & \\theta_h\\epsilon_T + \\theta_{h+1}\\epsilon_{T-1} + \\cdots\n\\end{array}\n\\]"
  },
  {
    "objectID": "13_fc.html#best-linear-predictor",
    "href": "13_fc.html#best-linear-predictor",
    "title": "13  Forecasting",
    "section": "13.2 Best Linear Predictor",
    "text": "13.2 Best Linear Predictor\nWe now justify our heuristic solutions by the theory of best linear predictor. Suppose we want to forecast \\(y\\) give the information set \\(X\\).\n\nDefinition 13.1 The best linear predictor (BLP) is defined as\n\\[\n\\mathcal{F}(y|X)=x'\\beta^*\n\\] which is a linear function of \\(X=(x_1,x_2,...,x_p)\\) such that\n\\[\n\\beta^* =\\text{argmin}\\ \\mathbb{E}(y-x'\\beta)^2.\n\\]\n\nTaking first-order condition with respect to \\(\\beta\\) gives\n\\[\n\\beta^* = [\\mathbb{E}(xx')]^{-1} \\mathbb{E}(xy).\n\\]\nTherefore, the BLP is given by\n\\[\n\\hat y = \\mathcal{F}(y|X)=x'\\beta^* = x'[\\mathbb{E}(xx')]^{-1} \\mathbb{E}(xy).\n\\]\nThe prediction error is\n\\[\nr_{y|X} = y - \\hat y = y - x'[\\mathbb{E}(xx')]^{-1} \\mathbb{E}(xy).\n\\]\nThe BLP is the linear projection of \\(y\\) onto \\(X\\). Because \\(\\mathbb{E}[x(y-x'\\beta)]=0\\). The forecast error is orthogonal to \\(X\\).\n\nProposition 13.1 BLP has the following properties:\n\n\\(\\mathcal{F}[ax + by| z_1...z_k] = a\\mathcal{F}[x|z_1...z_k] + b\\mathcal{F}[y|z_1...z_k]\\);\nIf \\(x = a_1z_1 + \\cdots + a_kz_k\\) is already a linear combination of \\(z_1...z_k\\), then \\(\\mathcal{F}[x|z_1...z_k]=x\\);\nIf for all \\(1\\leq j\\leq k\\), \\(\\text{cov}(x,z_j)=\\mathbb{E}(xz_j)=0\\), then \\(\\mathcal{F}[x|z_1...z_k]=0\\)."
  },
  {
    "objectID": "13_fc.html#forecasting-with-arma-models",
    "href": "13_fc.html#forecasting-with-arma-models",
    "title": "13  Forecasting",
    "section": "13.3 Forecasting with ARMA Models",
    "text": "13.3 Forecasting with ARMA Models\nARMA model is a basic yet powerful tool for forecasting. Given all stationary time series can be approximated by ARMA processes, it makes sense to model a stationary time series with ARMA, and then make forecast based on that model. We will see our heuristic solutions in the first part can be easily justified with the theory of BLP.\n\n13.3.1 Forecasting with AR(p)\nWe have said that, for an AR(\\(p\\)) process\n\\[\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t,\n\\]\nThe one-step-ahead forecast is simply\n\\[\n\\hat y_{T+1|T} = \\phi_1 y_{T} + \\phi_2 y_{T-1} + \\dots + \\phi_p y_{T-p+1}.\n\\]\nThis is the BLP immediately from Property 2 of Proposition 13.1. We can also justify the iterated \\(h\\)-step-ahead forecast by Property 1 (assuming \\(h<p\\)):\n\\[\n\\begin{aligned}\n\\hat y_{T+h|T} &= \\phi_1\\hat y_{T+h-1|T} + \\phi_2\\hat y_{T+h-2|T} + \\cdots + \\phi_h y_{T} + \\dots + \\phi_p y_{T+h-p} \\\\\n&= \\phi_1\\mathcal{F}[y_{T+h-1}|y_T, y_{T-1}...] + \\cdots + \\phi_p\\mathcal{F}[y_{T+h-p}|y_T, y_{T-1}...] \\\\\n&= \\mathcal{F}[\\phi_1 y_{T+h-1} + \\cdots + \\phi_p y_{T+h-p} | y_T,y_{T-1},...] \\\\\n&= \\mathcal{F}[y_{T+h}|y_T,y_{T-1},...]\n\\end{aligned}\n\\]\nThis is assuming all the forecast before \\(h\\) are BLPs, which can be justified recursively. Also note that for the values readily observed: \\(y_T, y_{T-1},...\\) , the BLP is the value itself.\n\n\n13.3.2 Forecasting with MA(q)\nFor the MA(\\(q\\)) process\n\\[\ny_t = \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2} + \\cdots + \\theta_q\\epsilon_{t-q}  \n\\]\nThe BLP for \\(h\\)-step-ahead forecast is (assuming \\(h<q\\))\n\\[\n\\begin{aligned}\n\\hat y_{T+h|T} &= \\mathcal{F}(y_{T+h}|\\epsilon_T,\\epsilon_{T-1}...) \\\\\n&= \\mathcal{F}(\\epsilon_{T+h}|\\epsilon_T,\\epsilon_{T-1}...) + \\theta_1\\mathcal{F}(\\epsilon_{T+h-1}|\\epsilon_T,\\epsilon_{T-1}...) + \\cdots + \\theta_q\\mathcal{F}(\\epsilon_{T+h-q}|\\epsilon_T,\\epsilon_{T-1}...) \\\\\n&= 0 + \\cdots + 0 + \\theta_h\\epsilon_T + \\cdots + \\theta_q\\epsilon_{T+h-q}\n\\end{aligned}\n\\]\nWe make use of Property 3 of Proposition 13.1 with the knowledge that \\(\\text{cov}(\\epsilon_i,\\epsilon_j)=0\\) for \\(i \\neq j\\). This result is also consistent with our intuition, because we have no knowledge of future innovations, the best thing we can do is assuming they are zeros. If \\(h>q\\), then all \\(\\mathcal{F}(\\epsilon_{T+h-q}|\\epsilon_T,\\epsilon_{T-1}...)\\) are zero, which yields \\(\\hat y_{T+h|T} = 0\\).\nIn practice, we do not observe \\(\\{\\epsilon_{t}\\}\\). If we have an estimated MA model and we want to make forecast based on the model, we need to back out \\(\\{\\epsilon_{t}\\}\\) from \\(\\{y_t\\}\\) by inverting the MA process: \\(\\epsilon_t = \\theta^{-1}(L)y_t\\).\nWith the MA specification, we can easily compute the Mean Squared Forecast Error (MSFE) as follows\n\\[\nQ_{T+h|T} = \\mathbb{E}(y_{T+h} - \\hat y_{T+h|T})^2 = \\mathbb{E}\\left(\\sum_{j=0}^{h-1}\\theta_j\\epsilon_{T+h-j}\\right)^2 = \\sigma^2\\sum_{j=0}^{h-1}\\theta_j^2.\n\\]\n\n\n13.3.3 Forecasting with ARMA(p,q)\nConsider the ARMA(\\(p\\), \\(q\\)) process\n\\[\n(1-\\phi_1L-\\cdots-\\phi_pL^p)y_t = (1+\\theta_1L+\\cdots+\\theta_qL^q)\\epsilon_t\n\\]\nWe assume the process is causal and invertible. We can transform it to an AR(\\(\\infty\\)) process or MA(\\(\\infty\\)) process.\nCausal form:\n\\[\ny_t = \\phi^{-1}(L)\\theta(L)\\epsilon_t = \\sum_{j=0}^{\\infty} \\psi_j\\epsilon_{t-j}\n\\]\nInvertible form:\n\\[\n\\epsilon_t = \\theta^{-1}(L)\\phi(L)y_t = \\sum_{j=0}^{\\infty} \\pi_j y_{t-j}\n\\]\nor\n\\[\ny_t = -\\sum_{j=1}^{\\infty} \\pi_j y_{t-j} + \\epsilon_t\n\\]\nAs we have seen so far, it is relatively easier to compute the mean forecast with AR models, and the MSFE with MA models. So we make forecast with the AR representation:\n\\[\n\\hat y_{T+h|T} = -\\sum_{j=1}^{h-1}\\pi_j\\hat y_{T+h-j} - \\sum_{j=h}^{\\infty}\\pi_jy_{T+h-j}\n\\]\nHowever, we do not observe infinite past values in real world. We can only use the truncated values, discarding past values that we do not observe \\(y_0,y_{-1}, y_{-1},...\\)\n\\[\n\\hat y_{T+h|T} = -\\sum_{j=1}^{h-1}\\pi_j\\hat y_{T+h-j} - \\sum_{j=h}^{T+h-1}\\pi_jy_{T+h-j}\n\\]\nWe compute the MSFE with the MA representation:\n\\[\nQ_{T+h|T} = \\mathbb{E}(y_{T+h} - \\hat y_{T+h})^2 =  \\sigma^2\\sum_{j=0}^{h-1}\\psi_j^2\n\\]\nIf we can compute the prediction interval if we assume some probability distributions for the innovations. If we assume \\(\\epsilon_t\\overset{iid}\\sim N(0, \\sigma^2)\\), then \\((y_1,...,y_{T+h})'\\) is jointly normal. Therefore,\n\\[\ny_{T+h} - \\hat y_{T+h|T} \\sim N(0, Q_{T+h|T})\n\\]\nThe prediction interval is thus given by \\(\\hat y_{T+h|T} \\pm z_{\\alpha/2}\\sqrt{Q_{T+h|T}}\\)."
  },
  {
    "objectID": "13_fc.html#applications",
    "href": "13_fc.html#applications",
    "title": "13  Forecasting",
    "section": "13.4 Applications",
    "text": "13.4 Applications\nThe following examples use ARMA models to forecast inflation rate and stock market index. The parameters of the ARMA models are chosen automatically. We can see for the inflation rate, the model produces some patterns in the forecast. But for the stock market index, the forecast is an uninformative flat line, indicating there is no useful patterns in the past data can be extrapolated by the ARMA model.\n\nlibrary(forecast)\ndata = readRDS(\"data/md.Rds\")\ndata$CPI |>\n  auto.arima() |> \n  forecast(h=20) |>\n  autoplot()\n\n\n\n\nFigure 13.1: ARMA forecast for monthly inflation\n\n\n\n\n\ndata$SHSE |>\n  auto.arima() |> \n  forecast(h=20) |>\n  autoplot()\n\n\n\n\nFigure 13.2: ARMA forecast for stock market index"
  },
  {
    "objectID": "14_dce.html",
    "href": "14_dce.html",
    "title": "14  Dynamic Causal Effect",
    "section": "",
    "text": "As in all fields of science, we are perpetually interested in understanding the causal effect of one thing on another. In economics, we want to understand how monetary policy affects output and inflation, how exchange rate affects import and export, and so on. However, causality is something much easier said than done. In reality, there are multiple forces at work simultaneously that leads to the consequences we observed. It is challenging both conceptually and statistically to isolate the causality of a variable of particular interest.\nIn cross-sectional analysis, causality is defined counterfactually. That is, the causal effect of a treatment is defined as the difference between the treated outcome and the untreated outcome assuming that they would be otherwise the same without the treatment. In practice, that involves working with a large number of \\(iid\\) observations that are similar on average only differentiated by the status of the treatment. This approach, however, does not work well with many macroeconomic studies. For example, suppose we want to figure out the causal effect of monetary policy on inflation rate. The cross-sectional approach would entail finding a large number of almost identical countries, each with independent monetary policy. And a random subset of them tighten their monetary policies while others do not. Then we work out the different economic outcomes between these two groups. This is clearly infeasible. The question we posed concerns only one country with inflation and interest rates observed through time. We would need a definition of causal effect that encompasses observations over time not across individuals.\nSuppose \\(\\epsilon_t\\) denote a random treatment happened at time \\(t\\). Then the causal effect on an outcome variable \\(y_{t+h}\\), \\(h\\) periods ahead, of a unit shock in \\(\\epsilon\\) is defined as\n\\[\n\\mathbb{E}[y_{t+h}|\\epsilon_t=1]-\\mathbb{E}[y_{t+h}|\\epsilon_t=0].\n\\tag{14.1}\\]\nWe require the randomness of the treatment \\(\\epsilon_t\\) in a sense that it is uncorrelated with any other variables that could possible have an impact on the outcome. Therefore, \\(\\epsilon_t\\) happens or not does not affect other forces that shape the outcome. The difference in the outcomes is solely attributable to \\(\\epsilon_t\\). It is this randomness that guarantees a causal interpretation.\nOur example of monetary policy above clearly does not meet this requirement. The monetary authority does not set the interest rate randomly, but based on the economic conditions of the time, which makes it correlated with other economic variables that could also have an impact on inflation. A qualified random shock may be a change in weather conditions. Weather has huge impact on agricultural production, but it is determined independent of any human activity. If \\(\\epsilon_t\\) denotes a rainy day at time \\(t\\), and \\(y_{t+h}\\) be the agricultural production, Equation 14.1 could be a plausible causal effect. However, most variables of interest in economics are endogenously determined. How to estimate the causal effect in such cases is an art in itself. We will come back to this point later.\nThe conceptual definition of Equation 14.1 can not be computed directly as the counterfactual is not observed. What we have is a sample of experiments over time, in which the treatment happens randomly at some points but not others, \\(\\{\\epsilon_1=0, \\epsilon_2=1, \\epsilon_3=0,\\dots\\}\\). We could envision that if we have long enough observations, by comparing the outcomes when the shock happens and when it does not, it gives us an reasonable estimation of the causal effect because all other factors that contributing to the outcome, despite they are changing over time, would be averaged out provided the randomness of the treatment.\nAssuming linearity and stationarity, the causal effect of Equation 14.1 can be effectively captured by a regression framework,\n\\[\ny_{t+h} = \\theta_h\\epsilon_t + u_{t+h},\n\\]\nwhere \\(u_{t+h}\\) represents all other factors contributing to the outcome variable. Since \\(\\epsilon_t\\) is random, it holds that \\(\\mathbb{E}(u_{t+h}|\\epsilon_t) = 0\\). Therefore,\n\\[\n\\theta_h = \\mathbb{E}(y_{t+h}|\\epsilon_t=1)-\\mathbb{E}(y_{t+h}|\\epsilon_t=0).\n\\]\nThus, \\(\\theta_h\\) captures the causal effect of one unit shock of \\(\\epsilon_t\\) on \\(y_{t+h}\\). The path of the causal effects mapped out by \\(\\{\\theta_0, \\theta_1, \\theta_2, \\dots\\}\\) is called the dynamic causal effect, in a sense that it is the causal effects through time."
  },
  {
    "objectID": "15_ssf.html",
    "href": "15_ssf.html",
    "title": "15  The Structural Shock Framework",
    "section": "",
    "text": "The counterfactual framework introduced in the last section defines the dynamic causal effect of any variable on another. As economists, we are more interested in understanding the causal relationships between important forces that drive the economy. We now introduce the structural shock framework, or the Slutzky-Frisch paradigm. This paradigm is explicitly or implicitly embedded in virtually every mainstream macroeconomic models or econometric models. It is not an essential component of time series analysis. But, as we would like to approach the topic from an economist’s perspective, it is good to have this framework in mind for many of our applications.\nThe structural shock framework envisions our economy as a complex system driven by a set of fundamental structural forces and coordinated by numerous price signals that automatically balance the demand and supply of all goods and services. The structural forces could be technology progress, climate change, policy changes and so on. These structural shocks are the primitive forces underlying our economy. When a structural shock happens, it triggers a reallocation of economic resources guided by market forces. In theoretical works, we are interested in modelling the system as a whole, particularly how resources are allocated optimally by market forces. In empirical works, we are interested how to recover the underlying structural shocks and estimate their causal effect on other economic variables.\nIn the language of time series analysis, we can envision our economy as an MA process, in which the observable variables (output, employment, inflation, etc) are the outcomes of accumulated past and current structural shocks:\n\\[\n\\boldsymbol y_t = \\boldsymbol\\Theta(L) \\boldsymbol\\epsilon_t,\n\\]\nor\n\\[\n\\begin{bmatrix}\ny_{1t}\\\\ y_{2t}\\\\ \\vdots\\\\ y_{nt}\n\\end{bmatrix} =\n\\sum_{j=0}^{\\infty}\n\\begin{bmatrix}\n\\theta_{j,11} & \\theta_{j,12} & \\cdots & \\theta_{j,1m}\\\\\n\\theta_{j,21} & \\theta_{j,22} & \\cdots & \\theta_{j,2m}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\theta_{j,n1} & \\theta_{j,n2} & \\cdots & \\theta_{j,nm}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\epsilon_{1,t-j}\\\\ \\epsilon_{2,t-j}\\\\ \\vdots\\\\ \\epsilon_{m,t-j}\n\\end{bmatrix}.\n\\]\n\\(\\boldsymbol y_t\\) represents the vector of economic variables of concern. The space of \\(\\boldsymbol y_t\\) are spanned by \\(m\\) structural shocks (current and past): \\(\\{\\boldsymbol \\epsilon_{t-j}\\}_{j=0}^{\\infty}\\).\nStructural shocks are conceptual constructions that are primitive, unforeseeable, and uncorrelated underlying forces. Whether structural shocks do exist or not is an open question. But they are useful constructions that enable econometricians to disentangle different driving forces of the outcome variable.\nIn reality, almost every economic variable is endogenous. For example, monetary policy (interest rate) is set by the monetary authority based on their assessment of the economic conditions. But we can also imagine, there is a “genuine” component of the monetary policy, which may come from the personality of the policymaker and his mental conditions when he make the decision, that is not predictable from other variables. This genuine component is what we deem as the “monetary policy shock”. It is a shock in a sense that it is not predictable. It speaks for its own sake and contribute to the economic outcomes independently.\nWe do not observe the structural shocks directly. The observable variables are linear combinations of the structural shocks. For example, we may think of the observed interest rate as a linear combination of the monetary policy shock together with supply-side shocks and others.\n\\[\ni_t = \\theta_{1}(L)\\epsilon_{t}^{\\text{MP}} + \\theta_{2}(L)\\epsilon_{t}^{\\text{SS}} + \\cdots\n\\]\nTherefore, regressing inflation or output on interest rate will not give the causal effect of the monetary policy. Because interest rate does not represent the “genuine” monetary policy shock. It is determined by other economic variables and there are multiple structural forces at work. There are numerous literature that works on methods to isolate the “monetary policy shock” from the observed interest rates. Such way of constructing the structural shocks is not only a conceptual idea, but also a prerequisite for meaningful interpretation of the coefficients of econometric models."
  },
  {
    "objectID": "16_adl.html#distributed-lags",
    "href": "16_adl.html#distributed-lags",
    "title": "16  Estimating Dynamic Multipliers",
    "section": "16.1 Distributed Lags",
    "text": "16.1 Distributed Lags\nThe easiest approach to estimate dynamic causal effect is to include lags in the specification:\n\\[\ny_t = \\beta_0\\epsilon_t + \\beta_1\\epsilon_{t-1} + \\cdots + \\beta_p\\epsilon_{t-p} + u_t,\n\\]\nwhere \\(\\epsilon_t\\) is the structural shock, \\(u_t\\) is everything that otherwise influences \\(y_t\\). Since \\(\\epsilon_t\\) happens randomly, we have \\(\\mathbb E(u_t |\\epsilon_{t-j}) = 0\\). Thus, the \\(\\beta\\)s, which capture the dynamic causal effect, would be consistently estimated by OLS.\nNote that we call it a specification, in a sense that the joint distribution of the random variables is unknown, which distinguishes itself from the DGP model in Chapter 6. But it does not stop us from uncovering the causal effect, as long as the exogenous condition holds.\nThe effect of a unit change in \\(\\epsilon\\) on \\(y\\) after \\(h\\) periods, which is \\(\\beta_{h}\\), is also called the \\(h\\)-period dynamic multiplier. Sometimes, we are interested in the accumulated effect over time, \\(\\beta_0+\\beta_1+\\cdots+\\beta_h\\), which is called cumulative dynamic multiplier.\nBecause \\(u_t\\) is the linear combination of all other current and past shocks, it is likely serially correlated. So HAC standard errors are required for robust inferences.\n\nProposition 16.1 Assumptions for a consistent estimation of dynamic causal effects with distributed lag models:\n\n\\(\\epsilon\\) is an exogenous shock, \\(\\mathbb E(u_t|\\epsilon_t,\\epsilon_{t-1},...)=0\\);\nAll variables are stationary;\nRegular conditions for OLS to work.\n\n\nTo reduce the serial correlations \\(\\{u_t\\}\\), and also allow for slow adjustment of \\(y_t\\), we can also include lagged dependent variables in the specification, which becomes an autoregressive distributed lag (ADL) specification:\n\\[\ny_t = \\phi_1 y_{t-1} + \\cdots + \\phi_p y_{t-p} + \\beta_0\\epsilon_t + \\beta_1\\epsilon_{t-1} + \\cdots + \\beta_p\\epsilon_{t-p} + u_t,\n\\]\nor\n\\[\n\\phi(L) y_t = \\beta(L) \\epsilon_t + u_t.\n\\]\nWhen lags of the dependent variable are included as regressors, strict exogeneity fails for sure, because \\(X=\\{y_{t-1},\\dots,\\epsilon_t, \\epsilon_{t-1},\\dots\\}\\) is correlated with past errors \\(u_{t-1}\\), despite it is uncorrelated with the contemporary error \\(u_t\\). The OLS is consistent so long as \\(\\{u_t\\}\\) are not serially correlated. Otherwise, \\(u_t\\) would be correlated with \\(X\\) through \\(u_{t-1}\\). The serial correlation can be tested with Durbin-Watson test or Breusch-Godfrey test.\nThe dynamic causal effect is more convoluted with the ADL specification though,\n\\[\n\\hat\\theta(L) = \\hat\\phi^{-1}(L) \\hat\\beta(L).\n\\]\nADL also require truncated lags. \\(p\\) and \\(q\\) are chosen as an increasing function of the sample size. In general, choosing \\(p\\) and \\(q\\) to be of order \\(T^{1/3}\\) would be sufficient for consistency."
  },
  {
    "objectID": "16_adl.html#local-projections",
    "href": "16_adl.html#local-projections",
    "title": "16  Estimating Dynamic Multipliers",
    "section": "16.2 Local Projections",
    "text": "16.2 Local Projections\nDynamic causal effect can also be estimated by projecting future outcomes directly on the shock. Jordà (2005) named it local projections (LP).\n\\[\ny_{t+h} = \\theta_h \\epsilon_t + u_{t+h}.\n\\]\nBy assumption, \\(\\mathbb E(u_{t+h}|\\epsilon_t)=0\\). So \\(\\hat\\theta_h\\) is a consistent estimate of the \\(h\\)-period dynamic multiplier. HAC standard errors are also required in local projections, as \\(u_{t+h}\\) in are usually serially correlated.\nReaders may wonder, since ADL and LP both give consistent estimates of the dynamic multipliers, what is the difference between them. There are two obvious differences:\n\nLagged shocks do not appear in LP specifications as they do in distributed lag specifications.\nThe LP method requires running separate regressions for each \\(h\\). The dynamic response \\(\\{\\theta_0,\\theta_1,\\theta_2, \\dots\\}\\) are estimated through multiple regressions rather than one.\n\nThe error structure is also different. To see this, suppose the DGP is an MA(\\(\\infty\\)) process\n\\[\ny_t = \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2} +\\cdots\n\\]\nIf we estimate it with a DL specification with two lags,\n\\[\ny_t = \\beta_0\\epsilon_t + \\beta_1\\epsilon_{t-1} + u_t,\n\\]\nwhere \\(u_t = \\sum_{j=2}^{\\infty}\\theta_j\\epsilon_{t-j}\\). Exogeneity would ensure \\(\\hat\\beta_1 \\to \\theta_1\\).\nWe can also estimate it with a local projection (suppose we are interested in the one-step-ahead dynamic multiplier):\n\\[\ny_{t+1} = \\psi_1\\epsilon_t + u_{t+1}.\n\\]\nAgain, we have consistency \\(\\hat\\psi_1\\to\\theta_1\\). But the error structure is different \\(u_{t+1} = \\epsilon_{t+1} + \\sum_{j=2}^{\\infty}\\theta_j\\epsilon_{t-j}\\).\nBoth the DL and LP specifications may include additional control variables, which can reduce the variance of the residuals and improve the efficiency of the estimators."
  },
  {
    "objectID": "16_adl.html#example-of-observable-exogenous-shocks",
    "href": "16_adl.html#example-of-observable-exogenous-shocks",
    "title": "16  Estimating Dynamic Multipliers",
    "section": "16.3 Example of Observable Exogenous Shocks",
    "text": "16.3 Example of Observable Exogenous Shocks\nDirectly observable exogenous shocks are rare. Here we use an example from Stock and Watson’s textbook, which explores the dynamic causal effect of cold weather on orange juice prices. Cold weather is bad for orange production. Orange trees cannot withstand freezing temperatures that last for more than a few hours. Florida accounts for more than 98 percent of U.S. production of frozen concentrated orange juice. Therefore, the frozen weather in Florida would reduce the supply and orange juice and raise the price. The dataset includes the number of freezing degree days in Florida and the average producer price for orange juice. Cold weather is plausibly exogenous, which allows us the utilize the regression framework above to estimate the dynamic causal effect.\n\nlibrary(AER)\nlibrary(dynlm)\nlibrary(lmtest)\n\ndata(\"FrozenJuice\") # load data\n\n# compute percentage change on price\npchg = 100*diff(log(FrozenJuice[, 'price']))\nsample = ts.union(fdd = FrozenJuice[,'fdd'], pchg)\n\n# distributed lag model\nmod = dynlm(pchg ~ L(fdd, 0:6), data = sample)\n# confidence interval\nci = coefci(mod, vcov. = NeweyWest)\n\n# plot dynamic multiplier\n{\n  plot(mod$coefficients[-1], # remove intercept\n       type = \"l\", \n       col = 2, \n       ylim = c(-0.4,1), \n       xlab = \"Lag\", \n       ylab = \"Dynamic Multiplier\")\n  abline(h = 0, lty = 2)\n  lines(ci[-1,1], col = 4)\n  lines(ci[-1,2], col = 4)\n}\n\n\n\n\nFigure 16.1: Dynamic Effect of Freezing Days on Orange Juice Price\n\n\n\n\nWe can also use local projections. Note that local projections require estimating multiple regressions. The coefficients from each of the regressions constitute the dynamic multiplier.\n\n# apply local projection for horizons 0-6\nlps = sapply(0:6, function(h) {\n  lp = dynlm(L(pchg, -h) ~ fdd, data = sample)\n  ci = coefci(lp, vcov. = NeweyWest)\n  c(lp$coefficients[-1], ci[-1,]) # remove intercept\n}) |> t() # transpose it\n\n# plot the LP coefficients\n{\n  plot(lps[,'fdd'], \n       type = \"l\", \n       col = 2, \n       ylim = c(-0.4,1), \n       xlab = \"Horizon\", \n       ylab = \"LP Coefficient\")\n  abline(h = 0, lty = 2)\n  lines(lps[,'2.5 %'], col = 4)\n  lines(lps[,'97.5 %'], col = 4)\n}\n\n\n\n\nFigure 16.2: Local Projections of Freezing Days on Orange Juice Price"
  },
  {
    "objectID": "16_adl.html#example-of-constructed-structural-shocks",
    "href": "16_adl.html#example-of-constructed-structural-shocks",
    "title": "16  Estimating Dynamic Multipliers",
    "section": "16.4 Example of Constructed Structural Shocks",
    "text": "16.4 Example of Constructed Structural Shocks\nMost structural shocks in economics are not directly observed, such as monetary policy shocks, or fiscal policy shocks, yet they are of profound interest of researchers. As we have explained before, regressing output or inflation on interest rate does not give a plausible estimation of the causal effect of monetary policy, due to the endogeneity problem. Thus, we need to isolate the exogenous part of the monetary policy from observed variables. The method to achieve this is an active research field in itself. We here demonstrate the monetary policy shock for China constructed by Das and Song (2023).\nThe authors utilize the high-frequency price changes of interest rate swap around the window of monetary policy announcement to approximate the monetary policy shock. The rationale of this construction is that, the price of the financial instrument reflects the expected interest rate by market participants based on the economic conditions. Therefore, the sudden change of the price in the tiny window of monetary policy announcement captures the unexpected part of the monetary policy.\n\nlibrary(zoo)\n\nmp = readRDS(\"data/mpshocks.Rds\") # monetary policy shock\nmd = readRDS(\"data/md.Rds\") # monthly data\n\nplot(na.exclude(cbind(mp$shock_1y, md$LoanRate1Y)), \n     main=\"\", xlab=\"\", ylab=c(\"Loan Rate\", \"MP Shock\"))\n\n\n\n\nFigure 16.3: Monetary Policy Shock and Lending Rate\n\n\n\n\nWe estimate the dynamic causal effect of monetary policy shock on inflation using the constructed MP shocks. It shows that a tightening of monetary policy implies a gradual cooling down of inflation. The price level starts to decline roughly half a year after the initial tightening shock. However, the confidence interval is wide, suggesting an insignificant estimation of the policy effect. The result does not provide very strong evidence underlining the effectiveness of monetary policy to control inflation.\n\nsample = na.exclude(cbind(cpi=md$CPI, mp))\n\n# distributed lag model\nmod = dynlm(cpi ~ L(shock_1y, 0:12), sample)\n# confidence interval\nci = coefci(mod, vcov. = NeweyWest)\n\n# plot dynamic multiplier\n{\n  plot(mod$coefficients[-1], # remove intercept\n       type = \"l\", \n       col = 2, \n       ylim = c(-1,1.5), \n       xlab = \"Lag\", \n       ylab = \"Dynamic Multiplier\")\n  abline(h = 0, lty = 2)\n  lines(ci[-1,1], col = 4)\n  lines(ci[-1,2], col = 4)\n}\n\n\n\n\nFigure 16.4: Dynamic response of inflation on monetary policy shocks"
  },
  {
    "objectID": "17_iv.html",
    "href": "17_iv.html",
    "title": "17  Instrument Variables",
    "section": "",
    "text": "If a structural shock is not directly observable, neither can it be constructed through observable variables, we can identify it using an instrument variable approach if an instrument is available.\nSuppose our observable space \\(\\boldsymbol y=(y_1,y_2,\\dots)'\\) is spanned by multiple structural shocks \\(\\epsilon = (\\epsilon_1,\\epsilon_2,\\dots)'\\). We want to identify the causal effect of structural shock \\(\\epsilon_1\\). An instrument variable \\(z\\) satisfies the following conditions:\n\n\\(\\mathbb E(\\epsilon_{1t}z_t) =\\alpha\\neq 0\\) (relevance);\n\\(\\mathbb E(\\epsilon_{2:n}z_t) = 0\\) (contemporaneous exogeneity);\n\\(\\mathbb E(\\boldsymbol\\epsilon_{t+j}z_t) = 0\\) for \\(j\\neq 0\\) (lead-lag exogeneity).\n\n\\(\\epsilon_{2:n}\\) denotes all other structural shocks except \\(\\epsilon_1\\). The lead-lag exogeneity is unique to time series. To understand this, consider an local projection: \\(y_{t+h} = \\theta_h\\epsilon_t +u_{t+h}\\). As illustrated in the last section, \\(u_{t+h}\\) is a linear combination of the entire history of structural shocks. If \\(z_t\\) is to identify the causal effect of shock \\(\\epsilon_{1t}\\) alone, it must be uncorrelated with all leads and lags. The requirement that \\(z_t\\) be uncorrelated with future \\(\\epsilon\\)’s is generally not restrictive — by definition, future shocks are unanticipated. To the contrary, the requirement that \\(z_t\\) be uncorrelated with past \\(\\epsilon\\)’s is more restrictive and hard to meet.\nSuppose we want to estimate the causal effect of \\(\\epsilon_{1,t}\\) on \\(y_{2,t+h}\\), where \\(\\epsilon_{1,t}\\) is only observable through \\(y_{1,t}\\). Suppose we have an instrument variable \\(z_t\\) that satisfies the above conditions. The local projection\n\\[\ny_{2,t+h} = \\theta_{h,21} y_{1,t} + u_{t+h}\n\\]\ncannot be consistently estimated because \\(y_{1,t}\\) and \\(u_{t+h}\\) are correlated. However, with the help with \\(z_t\\) as an instrument, we can consistently estimate the dynamic multiplier \\(\\theta_{h,21}\\):\n\\[\n\\begin{aligned}\n\\beta_{\\text{LP-IV}} &= \\frac{\\mathbb E(y_{2,t+h}z_t)}{\\mathbb E(y_{1,t}z_t)}\\\\\n&= \\frac{\\mathbb E[(\\theta_{h,21}y_{1,t} + u_{t+h})z_t]}{\\mathbb E(y_{1,t}z_t)}\\\\\n&= \\frac{\\theta_{h,21}\\alpha}{\\alpha} = \\theta_{h,21}.\n\\end{aligned}\n\\]\nLead-lag exogeneity implies \\(z_t\\) being unforecastable in a regression of \\(z_t\\) on lags of \\(y_t\\). If the exogeneity fails, LP-IV is not consistent. This problem can be partially addressed by including control variables in the regression:\n\\[\ny_{2,t+h} = \\theta_{h,21} y_{1,t} + \\boldsymbol{\\gamma_h'w_t} + u_{t+h}^{\\perp}.\n\\]\nWe could also include lagged values of \\(y_t\\) or other lagged variables. The IV estimator is consistent if \\(\\boldsymbol w_t\\) absorbs all past shocks that could potentially correlated with \\(z_t\\). In a broad sense, the validity of the instrument variable with additional controls requires that the controls span the space of all structural shocks."
  },
  {
    "objectID": "18_sp.html",
    "href": "18_sp.html",
    "title": "18  Spurious Regression",
    "section": "",
    "text": "It is said, all stationary series are alike, but each non-stationary series is non-stationary in its own way (remember Leo Tolstoy’s famous quote: all happy families are alike; each unhappy family is unhappy in its own way.)\nIn all previous chapters, we have been working on stationary processes. We have shown that similar regression techniques and asymptotic results hold for stationary processes as for \\(iid\\) observations, albeit not exactly the same. If a time series is not stationary, we transform it to stationary by taking differences.\nThis chapter is devoted to study non-stationary time series. Special attention is given to unit root processes. We will see the theories involving non-stationary processes are entirely different from those applied to stationary processes. This makes unit root analysis an rather independent topic. The obsession with unit root in academia have faded away in recent decades (I do not know if this assessment is accurate). Despite the topic posses immense theoretical interest, it does not seem to provide proportionate value for applied studies. Nonetheless, the topic is indispensable for a comprehensive understanding of time series analysis.\nWe will focus on two types of non-stationary processes: trend-stationary processes and unit root processes, which are the most common types of non-stationary series we would encounter in economic and finance. Non-stationary series with exponential growth can be transformed into linear trend, hence is not of particular interest. We will start with the relatively easy tread-stationary processes, and spend most of the paragraphs on unit root processes.\nWe start by pointing out that, it is very dangerous to blindly include non-stationary variables in a regression. To illustrate this, we simulate two random walks:\n\\[\n\\begin{aligned}\nx_{t} &= x_{t-1} + \\epsilon_{t},\\quad\\epsilon_{t}\\overset{iid}\\sim N(0,\\sigma_X^2)\\\\\ny_{t} &= y_{t-1} + \\eta_{t},\\quad\\eta_{t}\\overset{iid}\\sim N(0,\\sigma_Y^2)\n\\end{aligned}\n\\]\n\\(\\epsilon_t\\) and \\(\\eta_t\\) are independent to each other.\n\nset.seed(2024)\nx = cumsum(rnorm(200))\ny = cumsum(rnorm(200))\nts.plot(cbind(x,y), col=1:2)\n\n\n\n\nWe would expect the two series completely uncorrelated, as they are two independent random processes. However, if we regress \\(y_t\\) on \\(x_t\\), we would likely find a very strong correlation. This is called a spurious regression.\n\\[\ny_t = \\alpha + \\beta x_t + u_t\n\\]\n\ncoeftest(lm(y ~ x))\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(>|t|)    \n(Intercept)  5.895196   0.510099  11.557 < 2.2e-16 ***\nx           -0.267537   0.075726  -3.533 0.0005113 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that if we difference the two series to stationary, the spurious correlation disappears.\n\ncoeftest(lm(diff(y) ~ diff(x)))\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.068500   0.066740  1.0264   0.3060\ndiff(x)     -0.074427   0.065236 -1.1409   0.2553\n\n\nIf we approximate the distribution of the \\(t\\)-value for \\(\\hat\\beta\\) with run Monte Carlo simulations, we would find the distribution is not Gaussian and has much heavier tails. That means we would much more likely to find significant results with spurious regression.\n\n# Monte Carlo simulation\ntvalue = sapply(1:1000, function(i) {\n  x = cumsum(rnorm(200))\n  y = cumsum(rnorm(200))\n  # extract t-value\n  summary(lm(y~x))$coef['x','t value']\n})\n# plot the density with Gaussian curve\n{\n  hist(tvalue, prob = TRUE, breaks = 40, xlim=c(-20,20), ylim = c(0,.4))\n  range = seq(-20, 20, by = .2)\n  lines(range, dnorm(range), col = 2)\n}\n\n\n\n\nTherefore, the conventional statistical inference against non-stationary series is totally misleading. The rest of the chapter will demystify the nature of spurious regression and discuss how we can properly deal with non-stationary time series."
  },
  {
    "objectID": "19_ts.html",
    "href": "19_ts.html",
    "title": "19  Trend Stationary",
    "section": "",
    "text": "Trend-stationary process is a stationary process round a deterministic trend:\n\\[\ny_t = \\alpha + \\delta t + \\psi(L)\\epsilon_t,\n\\]\nwhere \\(\\delta t\\) is a deterministic linear time trend, \\(\\psi(L)\\epsilon_t\\) is a stationary process. After de-trending \\(-(\\alpha+\\delta t)\\) , the result is a stationary process.\n\n\n\n\n\n\nTrend stationary vs stochastic trend\n\n\n\nTrend-stationary processes must be distinguished from stochastic trend process (unit root with a drift):\n\\[\ny_t = \\delta + y_{t-1} +\\epsilon_t = y_0+\\delta t+\\sum_{j=1}^{t}\\epsilon_j.\n\\]\nBoth of them have a time trend component. But in the latter model, the stochastic component is not stationary. In other words, a innovation in a trend-stationary model does not have long-lasting effect, whereas the effect is persistent in a stochastic trend model.\nThe difference becomes clearer by comparing the variances. The variance of the trend-stationary process\n\\[\n\\text{var}(y_t) = \\psi^2(L)\\sigma^2\n\\]\nis constant which does not depend on time. However, the variance of the stochastic trend process\n\\[\n\\text{var}(y_t) = \\text{var}(\\sum_{j=1}^{t} \\epsilon_j) = \\sigma^2 t\n\\]\nis increasing over time. Therefore, stochastic trend process fluctuates more widely as time goes by.\n\n\nUnlike unit root processes, trend-stationary processes can be safely estimated by OLS. The usual \\(t\\) and \\(F\\) statistics have the same asymptotic distribution as they are for stationary processes. But they converge at a different speed, due to the presence of the trend. To see this, rewrite the regression in vector form\n\\[\n\\begin{aligned}\ny_t = \\alpha + \\delta t + \\epsilon_t\n= \\begin{bmatrix}1 & t\\end{bmatrix}\\begin{bmatrix}\\alpha\\\\\\delta\\end{bmatrix} + \\epsilon_t = \\boldsymbol{x_t'\\beta} + \\epsilon_t;\n\\end{aligned}\n\\]\nFor simplicity, assume \\(\\epsilon_t \\sim IID(0,\\sigma^2)\\) for the following computation. The result can be generalized to \\(\\epsilon_t\\) being stationary. The OLS estimator is given by\n\\[\n\\begin{aligned}\n\\hat\\beta = \\begin{bmatrix}\\hat\\alpha\\\\ \\hat\\delta\\end{bmatrix} &= \\left(\\sum_t x_tx_t'\\right)^{-1} \\left(\\sum_t x_ty_t\\right), \\\\\n\\sqrt{T}(\\hat\\beta-\\beta) &= \\left(\\frac{1}{T}\\sum_t x_tx_t'\\right)^{-1} \\left(\\frac{1}{\\sqrt T}\\sum_t x_t\\epsilon_t\\right).\n\\end{aligned}\n\\]\nThe usual asymptotic results are\n\\[\n\\begin{aligned}\n\\frac{1}{T}\\sum_t x_t x_t' &\\to Q \\\\\n\\frac{1}{\\sqrt T}\\sum_t x_t \\epsilon_t &\\to N(0,\\sigma^2 Q) \\\\\n\\sqrt{T}(\\hat\\beta -\\beta) &\\to N(0, \\sigma^2 Q^{-1})\n\\end{aligned}\n\\]\nBut this is not the case with deterministic trend if we do the computation:\n\\[\n\\frac{1}{T}\\sum_t x_t x_t' =\n\\begin{bmatrix}\n1 & \\frac{1}{T}\\sum t \\\\\n\\frac{1}{T}\\sum t & \\frac{1}{T}\\sum t^2\n\\end{bmatrix}\n\\]\ndoes not converge. Because \\(\\sum_{t=1}^{T} t = \\frac{T(T+1)}{2}\\), and \\(\\sum_{t=1}^{T} t^2 = \\frac{T(T+1)(2T+1)}{6}\\). It requires stronger divider to make them converge, \\(T^{-2}\\sum_{t=1}^{T}t\\to\\frac{1}{2}\\), \\(T^{-3}\\sum_{t=1}^{T}t^2\\to\\frac{1}{3}\\). In general,\n\\[\n\\frac{1}{T^{v+1}} \\sum_{t=1}^{T} t^v \\to \\frac{1}{v+1}.\n\\]\nDividing by \\(T^3\\) will make the convergence\n\\[\n\\frac{1}{T^3}\\sum_t x_t x_t' \\to \\begin{bmatrix}0 & 0\\\\0 & \\frac{1}{3}\\end{bmatrix}\n\\]\nHowever, this matrix is not invertible. We need different rates of convergence for \\(\\hat\\alpha\\) and \\(\\hat\\delta\\).\nDefine\n\\[\n\\boldsymbol\\gamma_T = \\begin{bmatrix}\\sqrt{T} & 0 \\\\ 0 & T^{3/2}\\end{bmatrix}\n\\]\nMultiple this matrix with the coefficient vector would apply different convergence speed to different coefficients:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\\sqrt{T}(\\hat\\alpha-\\alpha) \\\\ T^{3/2}(\\hat\\delta - \\delta)\\end{bmatrix}\n&= \\gamma_T\\left(\\sum_t x_tx_t'\\right)^{-1} \\left(\\sum_t x_t\\epsilon_t\\right) \\\\\n&= \\left[\\gamma_T^{-1}\\left(\\sum_t x_tx_t'\\right)\\gamma_T^{-1}\\right]^{-1} \\left[\\gamma_T^{-1}\\left(\\sum_t x_t\\epsilon_t\\right)\\right]\n\\end{aligned}\n\\]\nin which\n\\[\n\\begin{aligned}\n\\gamma_T^{-1}\\left(\\sum_t x_tx_t'\\right)\\gamma_T^{-1} &=\n\\begin{bmatrix}T^{-1/2} & \\\\ & T^{-3/2}\\end{bmatrix}\n\\begin{bmatrix}\\sum 1 & \\sum t \\\\ \\sum t & \\sum t^2 \\end{bmatrix}\n\\begin{bmatrix}T^{-1/2} & \\\\ & T^{-3/2}\\end{bmatrix} \\\\ &=\n\\begin{bmatrix}T^{-1}\\sum 1 & T^{-2}\\sum t \\\\ T^{-2}\\sum t & T^{-3}\\sum t^2 \\end{bmatrix} \\to\n\\begin{bmatrix}1 & \\frac{1}{2}\\\\ \\frac{1}{2} & \\frac{1}{3}\\end{bmatrix}=Q.\n\\end{aligned}\n\\]\nTurning to the second term:\n\\[\n\\gamma_T^{-1}\\left(\\sum_t x_t\\epsilon_t\\right) =\n\\begin{bmatrix}T^{-1/2} & \\\\ & T^{-3/2}\\end{bmatrix}\n\\begin{bmatrix}\\sum\\epsilon_t \\\\ \\sum t\\epsilon_t\\end{bmatrix} =\n\\begin{bmatrix}T^{-1/2}\\sum\\epsilon_t \\\\ T^{-1/2}\\sum\\frac{t}{T}\\epsilon_t\\end{bmatrix}\n\\]\n\\(T^{-1/2}\\sum\\epsilon_t\\to N(0,\\sigma^2)\\) by standard CLT. Observe that \\(\\zeta_t = \\frac{t}{T}\\epsilon_t\\) is not serially correlated,\n\\[\n\\mathbb E(\\zeta_t\\zeta_{t-j})=\\frac{t(t-j)}{T^2}\\mathbb{E}(\\epsilon_t\\epsilon_{t-j})=0\n\\]\nwith stabilized variance\n\\[\n\\text{var}(T^{-1/2}\\sum\\zeta_t)=\\frac{1}{T}\\sum\\text{var}\\left(\\frac{t}{T}\\epsilon_t\\right) = \\frac{\\sigma^2}{T^3}\\sum t^2 \\to \\frac{\\sigma^2}{3}\n\\]\nTherefore, \\(T^{-1/2}\\sum\\frac{t}{T}\\epsilon_t\\to N(0,\\frac{\\sigma^2}{3})\\). We also need to consider the covariance,\n\\[\n\\text{cov}(T^{-1/2}\\sum\\epsilon_t, T^{-1/2}\\sum \\frac{t}{T}\\epsilon_t) =\n\\frac{1}{T}\\mathbb{E}\\left(\\sum\\epsilon_t\\sum\\frac{t}{T}\\epsilon_t\\right) = \\frac{\\sigma^2}{T^2}\\sum t \\to \\frac{\\sigma^2}{2}\n\\]\nTherefore, we have\n\\[\n\\begin{bmatrix}T^{-1/2}\\sum\\epsilon_t \\\\ T^{-1/2}\\sum\\frac{t}{T}\\epsilon_t\\end{bmatrix} \\to\n\\begin{bmatrix}\n\\sigma^2 & \\frac{\\sigma^2}{2} \\\\\n\\frac{\\sigma^2}{2} & \\frac{\\sigma^2}{3}\n\\end{bmatrix} = \\sigma^2 Q\n\\]\nFinally, putting everything together,\n\\[\n\\gamma_T(\\hat\\beta-\\beta)\\to N(0, \\sigma^2 Q^{-1}).\n\\]\nThis means the usual OLS \\(t\\)-test and \\(F\\)-test are asymptotically valid, despite at different convergence rates. After all, trend-stationary process is stationary after de-trending. But unit root process is a totally different species.\n\n\n\n\n\n\nKey Point Summary\n\n\n\n\nTrend-stationary process vs stochastic-trend process;\nApplying different convergence rates to OLS estimator;\nUsual \\(t\\)-test and \\(F\\)-test are still valid."
  },
  {
    "objectID": "20_uni.html",
    "href": "20_uni.html",
    "title": "20  Unit Root Process",
    "section": "",
    "text": "A unit root process is characterized by the presence of unit roots in the character equation of its ARMA representation. The simplest unit root process is an AR(1) process with \\(\\phi=1\\):\n\\[\ny_t = \\phi y_{t-1} + \\epsilon_t\n\\]\nWhen \\(\\phi=1\\), it makes each innovation persistent. The effect of past innovations do not fade away no matter how distant they are.\n\\[\ny_t = \\sum_{j=0}^{\\infty} \\epsilon_{t-j}\n\\]\nThe persistence makes the behavior of unit root processes drastically different from stationary processes. Unit root processes hold particular significance among non-stationary processes due to the prevalence of similar behavior in economic or financial time series. For example, stock prices behave a lot like unit root processes (the Random Walk Hypothesis).\n\n\n\n\n\nFigure 20.1: Stock market index (black) vs Simulation of a unit root process (red)\n\n\n\n\nThe particularity of unit root process makes it a unique class in itself in terms of analytic techniques. The usual OLS estimator and asymptotic normality does not work with unit root processes. If we regress a unit root process on its lags, the OLS estimator is given by\n\\[\n\\hat\\phi = \\frac{\\sum_t y_{t-1}y_t}{\\sum_t y_{t-1}^2}\n\\]\nWe would expect \\(\\sqrt{T}(\\hat\\phi -1) \\to N(0,\\omega^2)\\). However, this is not the case. To see this, consider\n\\[\nT(\\hat\\phi -1) =\n\\frac{T^{-1}\\sum_t y_{t-1}\\epsilon_t}{T^{-2}\\sum_t y_{t-1}^2}\n\\]\nAssuming Gaussian innovation \\(u_t \\sim N(0,\\sigma^2)\\), we have\n\\[\ny_t = \\epsilon_t + \\epsilon_{t-1} + \\cdots + \\epsilon_1 \\sim N(0, \\sigma^2t)\n\\]\nTherefore, \\(z_t = \\frac{y_t}{\\sigma\\sqrt{t}} \\sim N(0,1)\\). Consider the numerator,\n\\[\n\\begin{aligned}\n\\frac{1}{T}\\sum_{t=1}^{T}y_{t-1}\\epsilon_t\n&= \\frac{1}{T}\\sum_{i=1}^{T} (\\epsilon_1+\\cdots+\\epsilon_{t-1})\\epsilon_t = \\frac{1}{T}\\sum_{s<t}^{T}\\epsilon_s\\epsilon_t \\\\\n&= \\frac{1}{2T}\\left[(\\epsilon_1+\\cdots+\\epsilon_T)^2-\\sum_{t=1}^{T}\\epsilon_t^2\\right] \\\\\n&= \\frac{1}{2T}y_T^2 - \\frac{1}{2T}\\sum_{t=1}^{T}\\epsilon_t^2 \\\\\n&= \\frac{\\sigma^2}{2}\\left(\\frac{y_T}{\\sigma\\sqrt{T}}\\right)^2 - \\frac{1}{2T}\\sum_{t=1}^{T}\\epsilon_t^2 \\\\\n&= \\frac{\\sigma^2}{2}z_T^2 -\\frac{1}{2T}\\sum_{t=1}^{T}\\epsilon_t^2 \\\\\n\\end{aligned}\n\\]\nSince \\(z_T \\sim N(0,1)\\), \\(z_T^2 \\sim \\chi^2(1)\\). By the LLN, \\(\\frac{1}{T}\\sum_{t=1}^{T}\\epsilon_t^2 \\to \\mathbb{E}(\\epsilon_t^2)=\\sigma^2\\). Thus,\n\\[\n\\frac{1}{T}\\sum_{t=1}^{T}y_{t-1}\\epsilon_t \\to \\frac{\\sigma^2}{2}(\\chi^2(1)-1).\n\\]\nSo the asymptotic distribution of \\(\\hat\\phi\\) is non-Gaussian. The conventional statistical inference no longer make sense. If we simulate the distribution of \\(\\hat\\phi\\) by Monte Carlo, we see it is left-skewed. The negative values are almost twice as likely as positive values, meaning two thirds of the time, the estimated \\(\\hat\\phi\\) will be less than the true value \\(1\\). Therefore, the OLS estimate of a unit root process is biased.\n\n# Monte Carlo simulation\nbeta = sapply(1:1000, function(i) {\n  y = cumsum(rnorm(200))\n  x = dplyr::lag(y)\n  coef(lm (y ~ x))[2]\n})\nhist(beta, freq = FALSE)\n\n\n\n\nTo derive the asymptotic distribution for the unit root process, we need further knowledge of Brownian motions. The idea is derive a continuous version of the unit root process, where each innovation is infinitesimally small. This is the topic of the next section."
  },
  {
    "objectID": "21_ito.html#continuous-random-walk",
    "href": "21_ito.html#continuous-random-walk",
    "title": "21  Brownian Motion",
    "section": "21.1 Continuous random walk",
    "text": "21.1 Continuous random walk\nBrownian motion, or Wiener process, is the continuous-time extension of a discrete-time random walk. To define the continuous version of a random walk, we cannot simply sum up infinite number of white noises, which will certainly explode. Instead, we chop up a finite interval into infinitely many small intervals, each one corresponding to a tiny Gaussian white noise. The following table shows how a discrete random walk extends to a continuous function.\n\n\n\nRandom Walk and Brownian Motion\n\n\n\n\n\n\n\n\nRandom Walk\nBrownian Motion\n\n\n\n\nInnovation\n\\(\\epsilon_i\\sim\\text{WN}(0,1)\\)\n\n\n\nStochastic process\n\\(y_t = \\epsilon_1 + \\cdots + \\epsilon_t\\)\n\\(W(t)_{t\\in[0,1]} = \\lim_{n\\to\\infty}\\sum_{i=1}^{nt}\\epsilon_i\\)\n\n\nExpectation\n\\(\\mathbb E[y_t]=0\\)\n\\(\\mathbb E[W(t)] = 0\\)\n\n\nVariance\n\\(\\text{Var}[y_t]=t\\text{Var}[\\epsilon_i]=t\\)\n\\(\\text{Var}[W(t)]=nt\\text{Var}[\\epsilon_i]=t\\)\n\n\nQuadratic variation\n\\(\\mathbb E\\sum_{i=1}^{t}(y_i-y_{i-1})^2=t\\)\n\\(\\int_0^t(dW)^2 = t\\)\n\n\n\nBrownian motion \\(W(t)\\) is a stochastic function. Its realized path is different each time we take a draw from it. But every piece of it follows a tiny Gaussian process. The function is continuous, but nowhere differentiable. It is hard to imagine such a function at first glance. But as we proceed, we will appreciate its amazing properties. Despite its path is random, the area under the curve integrates to a well-defined probability distribution. The squared changes (quadratic variation) even sum up to a deterministic constant.\n\n\n\n\n\nFigure 21.1: Multiple Realizations of Brownian Motion"
  },
  {
    "objectID": "21_ito.html#some-properties",
    "href": "21_ito.html#some-properties",
    "title": "21  Brownian Motion",
    "section": "21.2 Some properties",
    "text": "21.2 Some properties\nThe quadratic variation is one of the most important properties of Brownian motions. Intuitively, it says the squared tiny changes sum up to a constant with probability \\(1\\) no matter which realized path it takes. Smooth functions will not have such property, because \\((dX)^2\\) diminishes much faster than \\(dX\\), surely we get \\(\\int_0^t(dX)^2 =0\\). Because Brownian motion fluctuations too much, the squared changes do not diminish away. To see this, imagine summing up the squares of infinitely many small increments up to \\(t\\):\n\\[\n\\lim_{n\\to\\infty}\\sum_{i=1}^{nt}\\left[\n  X\\left(\\frac{i}{n}\\right) - X\\left(\\frac{i-1}{n}\\right)\n  \\right]^2 = \\lim_{n\\to\\infty}\\sum_{i=1}^{nt}\\epsilon_i^2,\n\\]\nwith \\(\\epsilon_i\\sim N\\left(0,\\frac{1}{n}\\right)\\). Therefore, \\(\\mathbb E(\\epsilon_i^2)=\\frac{1}{n}\\). Let \\(z_i = \\epsilon_i^2\\). The sum above can be rewritten as\n\\[\n\\sum_{i=1}^{nt} z_i = nt\\left(\\frac{1}{nt}\\sum_{i=1}^{nt} z_i\\right)\n\\overset{\\text{LLN}}\\to nt\\mathbb{E}(z_i) = t.\n\\]\nThe integral version is the quadratic variation\n\\[\n\\int_0^t(dW)^2 = t,\n\\tag{21.1}\\]\nor written in differential form\n\\[\n(dW)^2 = dt.\n\\tag{21.2}\\]\nIt should be stressed, \\(W\\) is not differentiable. We use the notation \\(dW\\), but it is not the same as conventional differentials. The calculus for Brownian motions, the Itô calculus, which will be introduced below, is a different class of calculus specifically designed for stochastic functions. Before we get to that, let’s first have a look at some additional properties of Brownian motions.\nBy Central Limit Theorem, it holds that\n\\[\n\\frac{1}{\\sqrt{nt}} W(t) = \\sqrt{nt}\\frac{1}{nt}\\sum_{i=1}^{nt}\\epsilon_i\n\\to N\\left(0,\\frac{1}{n}\\right)\n\\]\nTherefore,\n\\[\nW(t) \\sim \\sqrt{nt} N\\left(0,\\frac{1}{n}\\right) \\sim N(0,t);\n\\tag{21.3}\\]\nIt follows that, for any \\(r<s\\),\n\\[\nW(s) - W(t) \\sim N(0, s-t);\n\\tag{21.4}\\]\nAs \\(s\\) and \\(t\\) become arbitrarily close, we have\n\\[\ndW \\sim N(0, dt).\n\\tag{21.5}\\]\nIn essence, Brownian motion is the accumulation of tiny independent Gaussian innovations. We give the formal definition of Brownian motions below.\n\n\n\n\n\n\nBrownian motion\n\n\n\nA Brownian motion (Wiener process) is a stochastic function \\(W(t)\\) such that\n\n\\(W(0)=0\\);\nFor \\(0 \\leq t \\leq s \\leq 1\\), \\(W(s)-W(t) \\sim N(0, s-t)\\);\nFor any realization, \\(W(t)\\) is continuous in \\(t\\).\n\n\n\nBrownian motions are frequently used to model stock returns. For a fixed horizon \\(T\\), the returns are normally distributed, with volatility scaled by \\(\\sqrt T\\). And the returns over different periods are independent, which means no predictability from past returns to future returns."
  },
  {
    "objectID": "21_ito.html#itô-calculus",
    "href": "21_ito.html#itô-calculus",
    "title": "21  Brownian Motion",
    "section": "21.3 Itô calculus",
    "text": "21.3 Itô calculus\n\nLemma 21.1 Let \\(F(W)\\) be a “smooth” function of a Brownian motion \\(W(t)\\). Then\n\\[\ndF = F'dW + \\frac{1}{2}F''dt.\n\\]\n\n\nProof. For an informal proof, it immediately follows from the Taylor expansion\n\\[\nF(W(t+h))-F(W(t))=F'(W(t+h)-W(t))+\\frac{1}{2}F''(W(t+h)-W(t))^2+\\cdots\n\\]\nAs \\(h\\to 0\\),\n\\[\ndF = F'dW + \\frac{1}{2}F''(dW)^2.\n\\]\nBy Equation 21.2, \\((dW)^2= dt\\). Therefore,\n\\[\ndF = F'dW + \\frac{1}{2}F''dt.\n\\]\n\nThis formula is known as the Itô’s lemma, which is the key equation of Itô calculus. Note that how this differs from the differential formula for normal functions: \\(dF = F'dW\\). The second-order term does not disappear precisely because the quadratic variation does not go to zero.\n\nExample 21.1 \\(F(W) = W^2 \\implies dF = 2WdW + dt\\)\n\n\nExample 21.2 Let’s do a more involved example to familiar ourselves with the Itô’s lemma, especially how the second-order differentiation of the Brownian motions plays out in computation.\nWe like to model the continuous-time stock price with Brownian motions. Let \\(S_t\\) be the stock price at time \\(t\\). Assume the behavior of \\(S_t\\) follows a stochastic differential equation:\n\\[\n\\frac{dS_t}{S_t}=\\mu dt + \\sigma dW\n\\]\nThat is, the percentage of \\(S_t\\) is a continuous time random walk with drift \\(\\mu\\) and volatility \\(\\sigma\\). Let’s compute the log-return of the stock over the horizon \\(T\\):\n\\[\nR_T = \\ln (S_T) - \\ln (S_0) = f(S_T)\n\\]\nApply second-order Taylor expansion:\n\\[\n\\begin{aligned}\ndR_T &= f' dS_T + \\frac{1}{2}f'' (dS_T)^2 \\\\\n&= \\frac{1}{S_T} dS_T - \\frac{1}{2}\\frac{1}{S_T^2} (dS_T)^2 \\\\\n&= \\frac{dS_T}{S_T} - \\frac{1}{2}\\left(\\frac{dS_T}{S_T}\\right)^2 \\\\\n&= (\\mu dt + \\sigma dW) - \\frac{1}{2}(\\mu dt + \\sigma dW)^2 \\\\\n&= (\\mu dt + \\sigma dW) - \\frac{1}{2}(\\mu^2 dt^2 + 2\\mu\\sigma dtdW + \\sigma^2 dW^2) \\\\\n&\\to (\\mu dt + \\sigma dW) - \\frac{1}{2} \\sigma^2 dt \\\\\n&= \\left(\\mu - \\frac{1}{2} \\sigma^2\\right) dt + \\sigma dW\n\\end{aligned}\n\\]\nThe second last step holds because, as \\(dt \\to 0\\), the terms \\(dt^2\\) and \\(dt dW\\) tend to zero faster than \\(dW^2\\). The only term left is \\(dW^2 = dt\\). If we define integral as the inverse of differentiation, we have\n\\[\n\\begin{aligned}\nR_T = \\int_0^T dR_T &= \\left(\\mu - \\frac{1}{2} \\sigma^2\\right) \\int_0^T dt + \\sigma \\int_0^T dW \\\\\n&= \\left(\\mu - \\frac{1}{2} \\sigma^2\\right)T + \\sigma\\sqrt{T}\\epsilon_T\n\\end{aligned}\n\\]\nwhere \\(\\epsilon_T\\) is a standard normal variable. The model tells us that the log-return of a stock over a fixed horizon of \\(T\\) is normally distributed with mean \\((\\mu - \\sigma^2/2)T\\) and standard deviation of \\(\\sigma\\sqrt{T}\\). Everything looks familiar, except the Ito’s term, \\(\\sigma^2/2\\), which comes from the non-zero second-order differential \\(dW^2\\). The famous Black-Scholes formula for option pricing is derived from this model.\n\n\nDefinition 21.1 Stochastic integrals as the reverse operation of the stochastic differentiation. Note that we change \\(W(t)\\) to \\(W(s)\\) when it enters as the integrand.\n\n\\(\\displaystyle \\int_{0}^{t} dW = W(t)\\);\n\\(\\displaystyle F(W(t))=\\int_0^t f(W(s))dW\\) , if \\(dF = f\\ dW\\);\n\\(\\displaystyle F(t, W(t)) = \\int_0^t f(s, W(s))dW + \\int_0^t g(s, W(s))ds\\) , if \\(dF = f\\ dW + g\\ dt\\).\n\n\n\nExample 21.3 Given \\(dW^2 = 2W dW + dt\\), take integral on both sides:\n\\[\nW^2(t) = 2\\int_0^t W(s) dW + \\int_0^t ds\n\\]\n\\[\n\\implies \\int_0^t W dW = \\frac{1}{2}[W^2(t) - t].\n\\]\nSetting \\(t=1\\), it follows that\n\\[\n\\int_0^1 W dW = \\frac{1}{2}[W^2(1)-1].\n\\]\nNote that \\(W(1) \\sim N(0,1)\\). So \\(W^2(1) \\sim \\chi^2(1)\\) with expectation 1. Thus \\(\\int_0^1 W dW\\) is centered at 0 but skewed.\n\n\nExample 21.4 Given \\(d(tW) = Wdt + t\\ dW\\) (verify this with Ito’s lemma), we have\n\\[\ntW(t) = \\int_0^t W(s)ds + \\int_0^t s\\ dW\n\\]\n\\[\n\\begin{aligned}\n\\implies \\int_0^t W(s)ds &= tW(t) - \\int_0^t s\\ dW \\\\\n&=t\\int_0^t dW - \\int_0^t s\\ dW \\\\\n&=\\int_0^t (t-s) dW.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nMaking Sense of Itô Calculus\n\n\n\nLet \\(F(t)\\) be a continuous-time trading strategy that holds an amount of \\(F(t)\\) of a stock at time \\(t\\). The stock price is a Brownian motion \\(W(t)\\). \\(dW\\) represents the movement of stock price. Consider the Itô’s integral\n\\[\nY_t = \\int_0^t F\\ dW\n\\]\nThe stochastic integral represents the payoff of the trading strategy up to time \\(t\\). Note that the integral always evaluates at the left. So the strategy can only make decision based on available information.\n\n\n\nProposition 21.1 Let \\(W(t)\\) be a Brownian motion. Let \\(f(t)\\) be a nonrandom function of time. Then\n\n\\(\\mathbb{E}\\left[\\int_0^t f(s)dW\\right]=0\\);\n\\(\\mathbb{E}\\left[\\left(\\int_0^t f(s)dW\\right)^2\\right]=\\mathbb{E}\\left[\\int_0^t f^2 ds\\right]\\) (Itô isometry);\n\\(\\int_0^t f(s) dW \\sim N\\left(0, \\int_0^t f^2 ds\\right)\\).\n\n\nIf \\(f(t)\\) represents a trading strategy and the stock price follows a Brownian motion, the theorem tells us the expected payoff of this strategy is zero; more precisely, the payoff follows a Gaussian distribution.\n\nExample 21.5 Following the last example,\n\\[\n\\int_0^t W ds = \\int_0^t (t-s) dW\n\\]\n\\(f(s) = t-s\\) is a nonrandom function, apply the theorem above\n\\[\n\\text{var}\\left[\\int_0^t W ds\\right]=\\int_0^t (t-s)^2ds = \\frac{1}{3}t^3\n\\]\nTherefore,\n\\[\n\\int_0^t W(s) ds \\sim N\\left(0, \\frac{1}{3}t^3\\right).\n\\]\nThus, the integral, the area under the curve of a Brownian motion, follows a Gaussian distribution."
  },
  {
    "objectID": "21_ito.html#unit-root-process",
    "href": "21_ito.html#unit-root-process",
    "title": "21  Brownian Motion",
    "section": "21.4 Unit root process",
    "text": "21.4 Unit root process\nConsider a unit root process,\n\\[\ny_t = y_{t-1} + \\epsilon_t = \\sum_{j=1}^{t}\\epsilon_j,\n\\]\nwhere \\(\\epsilon_t\\sim\\text{WN}(0,1)\\), \\(t=1,2,\\dots,T\\). It is not surprising to see the unit root process converges to Brownian motion if stabilizing it by \\(T^{-1/2}\\) :\n\\[\n\\begin{aligned}\n\\frac{1}{\\sqrt{T}}y_t &= \\frac{1}{\\sqrt{T}}\\sum_{j=1}^{t}\\epsilon_j \\\\\n&= \\frac{1}{\\sqrt{T}}\\sum_{j=1}^{Tr}\\epsilon_j \\quad(r=t/T) \\\\\n&= \\sqrt{r}\\left(\\frac{1}{\\sqrt{Tr}}\\sum_{j=1}^{Tr}\\epsilon_j\\right) \\\\[1em]\n&\\to \\sqrt{r} N(0, 1) \\sim N(0,r) \\quad(\\text{by CLT}) \\\\[1em]\n&\\to W(r) \\quad(\\text{by definition}).\n\\end{aligned}\n\\]\nIf \\(\\epsilon_t\\) has variance \\(\\sigma^2\\), we would have \\(T^{-1/2}y_t \\to \\sigma W(t/T)\\). Note if \\(y_t\\) is stationary, \\(y_t\\) will not deviate too far from \\(\\mathbb E(y_t)\\), we would have \\(T^{-1/2} y_t \\to 0\\).\nNow let’s consider the behaviour of the mean: \\(\\bar y=\\frac{1}{T}\\sum_{t=1}^T y_t\\). Define \\(\\xi_T(r)=T^{-1/2}y_t\\) , where \\(r=t/T\\). We have\n\\[\n\\begin{aligned}\n\\frac{1}{\\sqrt T} \\bar{y} &= \\frac{1}{\\sqrt T}\\left(\\frac{1}{T}\\sum_{t=1}^T y_t\\right) \\\\\n&= \\frac{1}{T}\\sum_{t=1}^T \\left(\\frac{1}{\\sqrt T}y_t\\right)=\\frac{1}{T}\\sum_{t=1}^T \\xi\\left(\\frac{t}{T}\\right) \\\\\n&= \\Delta r \\sum_{r=0}^{1} \\xi(r) \\quad (r=t/T, \\Delta r = 1/T) \\\\\n&\\to \\int_0^1 \\sigma W(r) dr \\quad (\\Delta r \\to 0)\n\\end{aligned}\n\\]Remember for stationary process, we would have \\(\\bar y \\to \\mathbb E(y_t)\\) . With unit root process, the mean no longer converges to a constant, but to a distribution \\(\\int_0^1 W(r)dr\\sim N(0,\\frac{1}{3})\\).\nFor higher orders of \\(y_t\\), we have\n\\[\n\\begin{aligned}\n\\frac{1}{T^2}\\sum_{t=1}^{T}y_t^2 &= \\frac{1}{T}\\sum_{t=1}^{T}\\left(\\frac{1}{\\sqrt T}y_t\\right)^2 \\\\\n&= \\frac{1}{T}\\sum_{t=1}^{T}\\left[\\xi\\left(\\frac{t}{T}\\right)\\right]^2 \\\\\n&= \\sum_{r=0}^{1} [\\xi(r)]^2 \\Delta r \\\\\n&\\to \\int_0^1 \\sigma^2 W^2(r) dr\n\\end{aligned}\n\\]\nBy continuous mapping theorem, it can be shown, in general\n\\[\n\\frac{1}{T^{1+k/2}}\\sum_{i=1}^T y_t^k \\to \\sigma^k\\int_0^1 W^k(r)dr.\n\\]\nConsider the numerator of the OLS estimator of the unit root process,\n\\[\n\\begin{aligned}\n\\sum_{t=1}^{T}y_{t-1}\\epsilon_t &= \\sum_{i=1}^{T} (\\epsilon_1+\\cdots+\\epsilon_{t-1})\\epsilon_t = \\sum_{s<t}^{T}\\epsilon_s\\epsilon_t \\\\\n&= \\frac{1}{2}\\left[(\\epsilon_1+\\cdots+\\epsilon_T)^2-\\sum_{t=1}^{T}\\epsilon_t^2\\right] \\\\\n&= \\frac{1}{2}y_T^2 - \\frac{1}{2}\\sum_{t=1}^{T}\\epsilon_t^2\n\\end{aligned}\n\\]\nDivide it by \\(T\\), we have\n\\[\n\\begin{aligned}\n\\frac{1}{T}\\sum_{t=1}^{T}y_{t-1}\\epsilon_t\n&= \\frac{1}{2}\\left(\\frac{1}{\\sqrt T}y_T\\right)^2 - \\frac{1}{2T}\\sum_{t=1}^{T}\\epsilon_t^2 \\\\\n&= \\frac{1}{2}[\\xi^2(1)-\\hat\\sigma^2] \\\\\n&\\to \\frac{1}{2}[\\sigma^2 W^2(1) - \\sigma^2] \\\\\n&= \\frac{1}{2}\\sigma^2 [ W^2(1) - 1] \\\\\n&= \\sigma^2 \\int_0^1 W dW.\n\\end{aligned}\n\\]\nWe summarize the important results below.\n\n\n\n\n\n\nKey Rules Summary\n\n\n\n\n\\(\\int_0^T W dt = N(0, \\frac{T^3}{3})\\)\n\\(\\int_0^T W dW = \\frac{1}{2}[ W^2(T) - T]\\)\n\\(\\frac{1}{T^{3/2}}\\sum_{t=1}^T y_t \\to \\sigma \\int_0^1 W(r)dr\\)\n\\(\\frac{1}{T^{1+k/2}}\\sum_{i=1}^T y_t^k \\to \\sigma^k\\int_0^1 W^k(r)dr\\)\n\\(\\frac{1}{T}\\sum_{t=1}^{T}y_{t-1}\\epsilon_t \\to \\sigma^2\\int_0^1 WdW\\)\n\\(\\frac{1}{T^2}\\sum_{t=1}^{T}y_{1t}y_{2t} \\to \\sigma_1\\sigma_2\\int_0^1 W_1(r)W_2(r)dr\\)\n\n\n\nThe last rule was given without proof, as we will need it in the following chapters."
  },
  {
    "objectID": "22_uni2.html#univariate-case",
    "href": "22_uni2.html#univariate-case",
    "title": "22  Unit Root Process (contd)",
    "section": "22.1 Univariate case",
    "text": "22.1 Univariate case\nWe now have all the ingredient to further analyse the unit root process\n\\[\ny_t = \\phi y_{t-1} + \\epsilon_t,\n\\]\nwhere \\(\\phi=1\\), and its OLS estimator\n\\[\nT(\\hat\\phi-1) = \\frac{T^{-1}\\sum_t y_{t-1}\\epsilon_t}{T^{-2}\\sum_t y_{t-1}^2}.\n\\]\nWe have shown that\n\\[\n\\begin{aligned}\nT^{-1}\\sum_t y_{t-1}\\epsilon_t &\\to \\sigma^2\\int_0^1 W dW =\\frac{\\sigma^2}{2}(W^2(1)-1)\\\\\nT^{-2}\\sum_t y_{t-1}^2 &\\to \\sigma^2\\int_0^1 W^2 ds\n\\end{aligned}\n\\]\nTherefore,\n\\[\nT(\\hat\\phi -1) \\to \\frac{\\int_0^1 W dW}{\\int_0^1 W^2 ds}.\n\\]\n\\(\\int W dW\\) is centered around \\(0\\), meaning \\(\\hat\\phi\\) is consistent for large samples. But it is biased in small smaples. Moreover, the distribution is not Gaussian, rending all conventional \\(t\\)-test or \\(F\\)-test meaningless. We contrast the properties of stationary processes and unit root processes below.\n\nStationary AR(1) process vs unit root process\n\n\n\n\n\n\n\n\nStationary\nUnit Root\n\n\n\n\nModel\n\\(y_t =\\phi y_{t-1} +\\epsilon_t\\)\n\\(y_t = y_{t-1}+\\epsilon_t\\)\n\n\nAsymptotic distribution of \\(\\hat\\phi\\)\n\\(\\sqrt{T}(\\hat\\phi-\\phi)\\to N(0,1-\\phi^2)\\)\n\\(\\sqrt{T}(\\hat\\phi-1)\\to\\frac{\\int WdW}{\\int W^2 dt}\\)\n\n\nAsymptotic distribution of \\(t\\)-statistics\n\\(t \\to N(0,1)\\)\n\\(t \\to \\frac{\\int WdW}{\\sqrt{\\int W^2dt}}\\)"
  },
  {
    "objectID": "22_uni2.html#spurious-regression",
    "href": "22_uni2.html#spurious-regression",
    "title": "22  Unit Root Process (contd)",
    "section": "22.2 Spurious regression",
    "text": "22.2 Spurious regression\nWe now dive deeper into the nature of spurious regression problem presented at the beginning of the chapter. We formulate the problem as below. Suppose\n\\[\ny_{t} = \\alpha + \\beta x_{t} + u_t,\n\\]\nwhere \\(y_{t}\\) and \\(x_{t}\\) are unit root processes and there does not exist \\((\\alpha, \\beta)\\) such that the residual \\(u_t\\) is stationary. In this case, OLS is likely to produce spurious result: even if \\(y_{t}\\) is completely unrelated to \\(x_t\\), the estimated value of \\(\\hat\\beta\\) is likely to appear to be statistically significantly different from zero.\nSpurious regression happens when\n\nDependent/independent variables are non-stationary;\nThe residual is non-stationary for all possible values of the coefficient vector.\n\nTo understand why this happens, consider the OLS estimator:\n\\[\n\\hat b=\\begin{bmatrix}\\hat\\alpha \\\\ \\hat\\beta\\end{bmatrix} =\n\\begin{bmatrix}T & \\sum x_t \\\\ \\sum x_t & \\sum x_t^2 \\end{bmatrix}^{-1}\n\\begin{bmatrix}\\sum y_t \\\\ \\sum x_ty_t\\end{bmatrix}\n\\]\nTo account for different convergent speed, similar to the trend-stationary case, we multiply the estimators by a matrix,\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\\sqrt{T}^{-1} & \\\\ & 1\\end{bmatrix}\n\\begin{bmatrix}\\hat\\alpha \\\\ \\hat\\beta\\end{bmatrix} &=\n\\begin{bmatrix}\\sqrt{T}^{-1} & \\\\ & 1\\end{bmatrix}\n\\begin{bmatrix}T & \\sum x_t \\\\ \\sum x_t & \\sum x_t^2 \\end{bmatrix}^{-1}\n\\begin{bmatrix}\\sqrt{T}^{-1} & \\\\ & 1\\end{bmatrix}^{-1}\n\\begin{bmatrix}\\sqrt{T}^{-1} & \\\\ & 1\\end{bmatrix}\n\\begin{bmatrix}\\sum y_t \\\\ \\sum x_ty_t\\end{bmatrix} \\\\ &=\n\\begin{bmatrix}1 & T^{-3/2}\\sum x_t \\\\ T^{-3/2}\\sum x_t & T^{-2}\\sum x_t^2 \\end{bmatrix}^{-1}\n\\begin{bmatrix}\\sum T^{-3/2}y_t \\\\ T^{-2}\\sum x_ty_t\\end{bmatrix} \\\\ &\\to\n\\begin{bmatrix}1 & \\int W_X dt \\\\ \\int W_X dt & \\int W_X^2 dt\\end{bmatrix}^{-1}\n\\begin{bmatrix}\\int W_Y dt \\\\ \\int W_X W_Y dt\\end{bmatrix}\n\\end{aligned}\n\\]\nThis means, \\(\\hat\\alpha\\) actually diverges. Because it needs to be divided by \\(\\sqrt T\\) to be able to converge to a stable distribution, rather than being multiplied by a stabilizing factor. \\(\\hat\\beta\\) converges, but it is not consistent. If there is no \\(\\alpha\\), we would have\n\\[\\hat\\beta = \\frac{\\sum x_ty_t}{\\sum x_t^2}\\to\\frac{\\int W_XW_Y dt}{\\int W_X^2 dt},\\]\nwhich is inconsistent. So we won’t get zero even in very large samples.\nThe OLS estimate of the variance of \\(u_t\\) also diverges. It needs to be divided by \\(T\\) to converge:\n\\[\n\\begin{aligned}\n\\frac{1}{T}\\hat\\sigma^2 &= \\frac{1}{T^2}\\sum(y_t-\\hat\\beta x_t)^2 \\\\\n&= \\frac{1}{T^2}\\sum y_t^2 - 2\\hat\\beta\\frac{1}{T^2}\\sum x_ty_t + \\hat\\beta^2\\frac{1}{T^2}\\sum x_t^2 \\\\\n&\\to \\int W_Y^2 dt - 2\\hat\\beta\\int W_X W_Y dt + \\hat\\beta^2 \\int W_X^2 dt \\\\\n&\\to \\int W_Y^2 dt - \\frac{(\\int W_X W_Y dt)^2}{\\int W_X^2 dt}.\n\\end{aligned}\n\\]\nThe \\(t\\) or \\(F\\) statistics also diverge. \\(t\\)-stat has to be divided by \\(\\sqrt T\\) to converge; \\(F\\)-stat needs to be divided by \\(T\\) to converge.\n\\[\nt = \\frac{\\hat\\beta}{\\hat\\sigma}\\sqrt{\\sum x_t^2} =\n\\sqrt{T}\\frac{\\hat\\beta}{\\sqrt{T^{-1}\\hat\\sigma^2}}\\sqrt{T^{-2}\\sum x_t^2} \\to \\sqrt{T}\\cdot C\n\\]\nThus, as sample size \\(T\\) grows, \\(t\\)-test will appear very large and significant, despite \\(y_t\\) and \\(x_t\\) are completely independent."
  },
  {
    "objectID": "22_uni2.html#cures-for-spurious-regression",
    "href": "22_uni2.html#cures-for-spurious-regression",
    "title": "22  Unit Root Process (contd)",
    "section": "22.3 Cures for spurious regression",
    "text": "22.3 Cures for spurious regression\n\nInclude lagged values of both dependent and independent variables in the regression: \\[\ny_t = \\alpha + \\phi y_{t-1} + \\beta x_t + \\gamma x_{t-1} + u_t\n\\]Now there exists a coefficient vector \\([\\phi,\\beta,\\gamma]=[1,0,0]\\) such that \\(u_t\\) is \\(I(0)\\) stationary. In this case, OLS yields consistent estimates for all the coefficients. \\(t\\)-test converges to Gaussian, though \\(F\\)-test of joint hypotheses has non-standard asymptotic distribution. We will come back to this point later.\nDifference the data to stationary: \\[\n\\Delta y_t = \\alpha + \\beta\\Delta x_t + u_t\n\\] Because \\(\\Delta y_t\\) and \\(\\Delta x_t\\) are all \\(I(0)\\). Standard OLS is valid.\nEstimate with Cochrane-Orcutt adjustment for first-order correlations in the residuals. This method is asymptotically equivalent to the second method."
  },
  {
    "objectID": "22_uni2.html#summary",
    "href": "22_uni2.html#summary",
    "title": "22  Unit Root Process (contd)",
    "section": "22.4 Summary",
    "text": "22.4 Summary\n\n\n\n\n\n\nKey Point Summary\n\n\n\n\nThe OLS estimator for unit root coefficient converges to non-standard distributions involving Brownian motions. Thus, standard statistical inferences are meaningless.\nRegressing unit root processes lead to spurious results, because the diverging behavior of \\(t\\)-stats makes artificially significant values.\nInclude lagged values or difference the data to stationary when working with non-stationary time series."
  },
  {
    "objectID": "23_ut.html#dickey-fuller-test",
    "href": "23_ut.html#dickey-fuller-test",
    "title": "23  Unit Root Test",
    "section": "23.1 Dickey-Fuller Test",
    "text": "23.1 Dickey-Fuller Test\nConsider an AR(1) process\n\\[\ny_t = \\phi y_{t-1} + u_t,\n\\]\nassuming no series correlation in the innovations \\(u_t\\sim IID(0,\\sigma^2)\\). We have shown that under the hypothesis \\(\\phi=1\\),\n\\[\nT(\\hat\\phi-1)\\to\\frac{\\int W dW}{\\int W^2 dt}.\n\\]\nWe can the hypothesis \\(H_0:\\ \\phi=1\\) utilizing this distribution. The critical values can be obtained by Monte Carlo simulations. The test was proposed by Fuller (1976).\nThe Dickey-Fuller tests involve three sets of equations depending whether a drift or trend is included, assuming \\(iid\\) innovations.\n\\[\n\\begin{aligned}\n\\Delta y_t &= \\gamma y_{t-1} + u_t \\\\\n\\Delta y_t &= \\alpha_0 + \\gamma y_{t-1} + u_t\\\\\n\\Delta y_t &= \\alpha_0 + \\gamma y_{t-1} + \\alpha_2t + u_t\n\\end{aligned}\n\\]\nTesting \\(\\phi=1\\) is equivalent to testing \\(\\gamma=0\\). The critical values depends on the form of the regression and the sample size (including a drift or trend results in different limiting distributions for \\(\\gamma\\)).\n\nCritical values of Dickey-Fuller tests\n\n\nModel\nHypothesis\n95%\n99%\n\n\n\n\nDefault\n\\(\\gamma=0\\)\n-1.95\n-2.60\n\n\nWith drift\n\\(\\gamma=0\\)\n-2.89\n-3.51\n\n\n\n\\(\\alpha_0=\\gamma=0\\)\n4.71\n6.70\n\n\nWith drift and trend\n\\(\\gamma=0\\)\n-3.45\n-4.04\n\n\n\n\\(\\gamma=\\alpha_2=0\\)\n6.49\n8.73\n\n\n\n\\(\\alpha_0=\\gamma=\\alpha_2=0\\)\n4.88\n6.50"
  },
  {
    "objectID": "23_ut.html#augmented-dickey-fuller-test",
    "href": "23_ut.html#augmented-dickey-fuller-test",
    "title": "23  Unit Root Test",
    "section": "23.2 Augmented Dickey-Fuller Test",
    "text": "23.2 Augmented Dickey-Fuller Test\nThe assumption that \\(\\epsilon_t\\) being uncorrelated is too strong for empirical applications. Suppose the data is generated by an AR(\\(p\\)) process with an unit root,\n\\[\n\\begin{aligned}\na(L) y_t &= \\epsilon_t \\\\\n(1-L)y_t &= \\underbrace{b^{-1}(L)\\epsilon_t}_{u_t}\n\\end{aligned}\n\\]\nwhere \\(a(L) = (1-L)b(L)\\) in which \\(b(L)\\) is stationary. In this case, \\(u_t\\) will be autocorrelated. In empirical works, it is more reasonable to assume \\(u_t\\) being serially correlated.\nIf we difference \\(y_t\\) once, we have\n\\[\n\\begin{aligned}\nb(L)\\Delta y_t &= \\epsilon_t \\\\\ny_t &= y_{t-1} + \\sum_{j=1}^{p}\\beta_j\\Delta y_{t-j} + \\epsilon_t\n\\end{aligned}\n\\]This motivates Dickey-Fuller tests with lags \\(\\{\\Delta y_{t-j}\\}\\). This is called augmented Dickey-Fuller test. The set of equations change to\n\\[\n\\begin{aligned}\n\\Delta y_t &= \\gamma y_{t-1} + \\sum_{j=1}^{p}\\beta_j\\Delta y_{t-j} + u_t \\\\\n\\Delta y_t &= \\alpha_0 + \\gamma y_{t-1} + \\sum_{j=1}^{p}\\beta_j\\Delta y_{t-j} + u_t\\\\\n\\Delta y_t &= \\alpha_0 + \\gamma y_{t-1} + \\alpha_2t + \\sum_{j=1}^{p}\\beta_j\\Delta y_{t-j} + u_t\n\\end{aligned}\n\\]\nThe coefficients on \\(\\Delta y_{t-j}\\) converge to Gaussian. The coefficient on \\(y_{t-1}\\) converges to non-standard distribution. The critical values are unchanged with lags are included."
  },
  {
    "objectID": "23_ut.html#phillips-perron-test",
    "href": "23_ut.html#phillips-perron-test",
    "title": "23  Unit Root Test",
    "section": "23.3 Phillips-Perron Test",
    "text": "23.3 Phillips-Perron Test\nAnother approach to test unit root is proposed by Phillips and Perron (1988), which also assumes autocorrelated errors. Our Brownian motion theories derived from \\(iid\\) innovations can be extended to autocorrelated innovations:\n\\[\n\\xi_T(r) = \\frac{1}{\\sqrt T}\\sum_{t=1}^{Tr} u_t \\to \\omega W(r)\n\\]\nwhere \\(\\omega^2 = \\sum_{-\\infty}^{\\infty}\\gamma_j\\) is the long-run variance for the autocorrelated process \\(\\{u_t\\}\\).\nBut, with autocorrelated errors, the limiting distribution of \\(\\hat\\phi\\) is slightly different, since\n\\[\n\\begin{aligned}\n\\frac{1}{T}\\sum_t y_{t-1}u_t &= \\frac{1}{2T}y_T^2 - \\frac{1}{2T}\\sum_t u_t^2 \\\\\n&\\to \\frac{1}{2}[\\omega^2W^2(1) - \\sigma_u^2] \\\\\n&\\to \\omega^2\\int W dW + \\frac{\\omega^2 - \\sigma_u^2}{2}\n\\end{aligned}\n\\]\nwhere \\(\\frac{\\omega^2-\\sigma_u^2}{2}\\) is a “nuisance” parameter. The Phillips and Perron proposed a test statistics correcting the nuisance parameter:\n\\[\nT(\\hat\\phi -1 ) + \\frac{\\frac{1}{2}\\hat\\omega^2 - \\hat\\sigma_u^2}{\\frac{1}{T^2}\\sum y_t^2} \\to \\frac{\\int W dW}{\\int W^2 dt}\n\\]\nwhere \\(\\hat\\omega^2\\) can be estimated by Newey-West, \\(\\hat\\sigma_u^2\\) is the estimated variance of the residuals. After the correction, the unit root test can be applied to processes with autocorrelated errors."
  },
  {
    "objectID": "24_co.html#cointegration-and-super-consistency",
    "href": "24_co.html#cointegration-and-super-consistency",
    "title": "24  Cointegration",
    "section": "24.1 Cointegration and super-consistency",
    "text": "24.1 Cointegration and super-consistency\nConsider a regression with two random walks \\(x_t\\) and \\(y_t\\):\n\\[\ny_t = \\beta x_t + e_t\n\\]\nwhere \\(e_t\\) is stationary. In this case, \\(y_t\\) and \\(x_t\\) are cointegrated, because the linear combination of the two \\(I(1)\\) processes becomes \\(I(0)\\). If this is the case, we no longer have a spurious regression. In fact, \\(\\hat\\beta\\) is not only consistent, but super-consistent. Consider the OLS estimator\n\\[\nT(\\hat\\beta -\\beta) = \\frac{T^{-1}\\sum x_t e_t}{T^{-2}\\sum x_t^2}\n\\to \\frac{\\int W_1 dW_2}{\\int W_1^2 dt}.\n\\]\nNote that \\(\\int W_1 dW_2\\) is centered at zero. Hence \\(\\hat\\beta\\) is consistent, despite the distribution is non-Gaussian. It converges at rate \\(T\\), faster than the standard case \\(\\sqrt T\\). So it is called super-consistency.\nThe argument extends to general regressions with multiple regressors. As long as there exists a vector of coefficients that makes the non-stationary variables cointegrated, the OLS estimator is super-consistent. For example,\n\\[\ny_t = \\phi y_{t-1} + \\beta x_t + e_t\n\\]\nIf \\(y_t\\) is \\(I(1)\\), then \\(y_t - y_{t-1}\\) is \\(I(0)\\) (cointegrated with itself). Therefore, \\([0, 1]\\) is the cointegration vector that makes \\(e_t\\) stationary even if \\(x_t\\) is not cointegrated with \\(y_t\\). In this case, OLS estimates for \\(\\hat\\rho\\) and \\(\\hat\\beta\\) will be super-consistent.\nThe intuition of super-consistency is that OLS minimizes squared residuals. If the coefficients deviate from the cointegration vector, \\(\\hat e_t^2\\) would diverge. \\(\\hat e_t^2\\) is minimized only when the coefficients coincide with the cointegration vector. That makes OLS converges even faster.\nThe super-consistency is so strong, that the OLS estimators for cointegrated variables are consistent even when there is endogeneity problem. We demonstrate this with an example. Suppose \\(x_t\\) follows a random walk, and \\(y_t\\) cointegrates with \\(x_t\\):\n\\[\n\\begin{aligned}\nx_t &= x_{t-1} + u_t \\\\\ny_t &= \\beta x_t + e_t\n\\end{aligned}\n\\]\nAssume \\(u_t\\) and \\(e_t\\) are correlated with \\(\\mathbb E (e_t^2)=\\mathbb E (u_t^2)=1\\) and \\(\\text{cov}(e_t,u_t)=1\\). For simplicity, also assume \\(e_t = \\phi u_t + \\sqrt{1-\\phi^2}\\eta_t\\) where \\(\\eta_t\\) is \\(iid\\) standard normal. As \\(e_t\\) is correlated with \\(x_t\\) through \\(u_t\\), there is clearly an endogeneity problem. The OLS estimator is given by\n\\[\nT(\\hat\\beta-\\beta) = \\frac{\\frac{1}{T}\\sum x_t e_t}{\\frac{1}{T^2}\\sum x_t^2}\n= \\frac{\\frac{1}{T}\\sum x_{t-1} e_t + \\frac{1}{T}\\sum u_t e_t}{\\frac{1}{T^2}\\sum x_t^2}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\frac{1}{T}\\sum x_{t-1} e_t &= \\frac{\\phi}{T}\\sum x_{t-1} u_t + \\frac{\\sqrt{1-\\phi^2}}{T}\\sum x_{t-1} \\eta_t \\\\\n&\\to \\phi\\int W_1 dW_1 + \\sqrt{1-\\phi^2}\\int W_1 dW_2\n\\end{aligned}\n\\]\nTherefore,\n\\[\nT(\\hat\\beta-\\beta) \\to \\frac{\\phi\\int W_1 dW_1 + \\sqrt{1-\\phi^2}\\int W_1dW_2+\\phi}{\\int W_1^2 dt}.\n\\]\nWhen \\(\\phi\\neq 0\\), that is endogneity exists, the limiting distribution is shifted. As a result \\(\\hat\\beta\\) has a finite sample bias of order \\(\\frac{1}{T}\\). But as \\(T\\to\\infty\\), the estimator is consistent. Figure 24.1 demonstrates the convergence of \\(\\hat\\beta\\) as sample size increases (the true \\(\\beta=0.5\\)).\n\n\n\n\n\nFigure 24.1: Superconsistentcy with cointegrated variables (even with endogeneity)"
  },
  {
    "objectID": "24_co.html#inference-under-cointegration",
    "href": "24_co.html#inference-under-cointegration",
    "title": "24  Cointegration",
    "section": "24.2 Inference under cointegration",
    "text": "24.2 Inference under cointegration\nWe have seen the limiting distributions of persistent regressors could be non-standard. But luckily, in many cases, we can still get Gaussian distributions. This is hard to believe. But it can be shown. Consider a regression of the canonical form — that is a regression with four types of regressors: stationary \\(I(0)\\), non-stationary \\(I(1)\\), constant and trend. It can be shown any regression can be rewritten in the canonical form.\n\\[\ny_t = \\boldsymbol{\\gamma z_t} + e_t\n\\] where\n\\[\n\\boldsymbol z_t =\n\\begin{bmatrix}\nF_1(L) & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\nF_2(L) & G & H & 0 \\\\\nF_3(L) & T & K & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol\\epsilon_t \\\\\n1\\\\\n\\boldsymbol\\eta_t \\\\\nt\n\\end{bmatrix}\n\\]\nand \\(e_t\\) is stationary. Consider the OLS estimator: \\(\\hat\\gamma=\\gamma + (Z'Z)'Z'e\\) with a scaling matrix\n\\[\nQ =\\begin{bmatrix}\n\\sqrt{T}\\boldsymbol{I}_{k_1} & & & \\\\\n& \\sqrt{T} & & \\\\\n& & T\\boldsymbol{I}_{k_3} & \\\\\n& & & T^{3/2}\n\\end{bmatrix}\n\\]\nMultiply them together,\n\\[\n\\begin{aligned}\nQ(\\hat\\gamma - \\gamma) &= (Q^{-1}Z'ZQ^{-1})^{-1} Q^{-1}Z'e \\\\\n&= \\begin{bmatrix}\n\\underset{k_1\\times k_1}{\\text{const}} & 0\\quad 0\\quad 0 \\\\\n0 & \\text{Functions of} \\\\\n0 & \\text{Brownian} \\\\\n0 & \\text{motions} \\\\\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nN(0,?)\\\\\nN(0,?)\\\\\n?\\int W dW ?\\\\\nN(0,?)\n\\end{bmatrix}\n\\end{aligned}\n\\]\nWe don’t care the specific functional forms of the converged distribution. What matters is the speed of convergence. Note that\n\\[\n\\begin{aligned}\n\\sqrt{T}(\\hat\\gamma_1-\\gamma_1) &\\to N(0,?) \\\\\n\\sqrt{T}(\\hat\\gamma_2-\\gamma_2) &\\to \\text{Something 2} \\\\\nT(\\hat\\gamma_3-\\gamma_3) &\\to \\text{Something 3} \\\\\nT^{3/2}(\\hat\\gamma_4-\\gamma_4) &\\to \\text{Something 4}\n\\end{aligned}\n\\]\nConstant and stationary regressors have the slowest converging speed \\(\\sqrt T\\). Only stationary regressors converge to Gaussian distribution.\nNow consider a general regression,\n\\[\ny_t = \\boldsymbol{\\beta x_t} + e_t\n\\]\nwhere \\(e_t\\) is stationary. Sims (1990) shows that we can always find a linear combination \\(\\boldsymbol{Dx_t=z_t}\\) that transforms the regression into a canonical form\n\\[\ny_t = \\boldsymbol{\\gamma z_t} + e_t\n\\]\nwhere \\(\\boldsymbol{\\gamma=\\beta D^{-1}}\\). This means, if a component of \\(\\hat\\beta\\) can be written as a linear combination of \\(\\hat\\gamma_1, \\hat\\gamma_3, \\hat\\gamma_4\\), its distribution will be dominated by the behavior of \\(\\hat\\gamma_1\\) due to its slower convergence speed. As such, it will behave like asymptotic normal and converge at speed \\(\\sqrt T\\).\nIn essence, the coefficients that can be represented as a linear combination involving stationary regressors will be asymptotically normal and converge at rate \\(\\sqrt T\\). Consider an example,\n\\[\ny_t = \\alpha + \\rho_1 y_{t-1} + \\rho_2 y_{t-2} + \\beta_1 x_t + \\beta_2 x_{t-1} + e_t\n\\]\nwhere \\(y_t\\) and \\(x_t\\) are \\(I(1)\\). The regression can be rewritten as\n\\[\n\\begin{aligned}\ny_t &= \\alpha + \\rho_1\\Delta y_{t-1} + (\\rho_1+\\rho_2)y_{t-2} + \\beta_1\\Delta x_t + (\\beta_1+\\beta_2)x_{t-1} + e_t \\\\\n&= \\alpha + \\rho_1\\Delta y_{t-1} + \\lambda y_{t-2} + \\beta_1\\Delta x_t + \\delta x_{t-1} + e_t\n\\end{aligned}\n\\]\nin which, \\(\\rho_1\\) and \\(\\beta_1\\) are coefficients on stationary regressors, therefore converging to Gaussian; \\(\\rho_2=\\lambda-\\rho_1\\) and \\(\\beta_2=\\delta-\\beta_1\\) are linear combinations involving coefficients on stationary regressors, whose distributions will be dominated by that of \\(\\rho_1\\) and \\(\\beta_1\\). Hence, all coefficients will have asymptotically normal distributions. Standard inference applies. We verify the claim with a Monte Carlo simulation.\n\nlibrary(dynlm)\nbeta = sapply(1:1000, function(i) {\n  x = arima.sim(list(order=c(1,1,0),ar=.5), 200)\n  y = arima.sim(list(order=c(0,1,1),ma=.7), 200)\n  coef(dynlm(y ~ L(y,1:2) + L(x,0:1)))\n}) |> t() \n{\n  par(mfrow=c(2,2), mar=c(2,2,2,2))\n  hist(beta[,'L(y, 1:2)1'], freq=F, main=\"y(t-1)\")\n  hist(beta[,'L(y, 1:2)2'], freq=F, main=\"y(t-2)\")\n  hist(beta[,'L(x, 0:1)0'], freq=F, main=\"x(t)\")\n  hist(beta[,'L(x, 0:1)1'], freq=F, main=\"x(t-1)\")\n}"
  },
  {
    "objectID": "24_co.html#conclusion",
    "href": "24_co.html#conclusion",
    "title": "24  Cointegration",
    "section": "24.3 Conclusion",
    "text": "24.3 Conclusion\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nRegressions with non-stationary regressors are likely to be spurious regressions. Do not regress two non-stationary series unless they are cointegrated.\nIncluding lags of dependent and independent variables protests you from spurious regressions.\nPersistence in time series makes statistical inference complicated. Sometimes standard inference works, but not always."
  },
  {
    "objectID": "25_sys.html",
    "href": "25_sys.html",
    "title": "25  System of Equations",
    "section": "",
    "text": "We have discussed how to estimate the effect of one economic variable on another, and the assumptions on which the estimate would have a (dynamic) causal interpretation. But one equation is often inadequate to characterize the economy, as it does not take into account the feedback between economic variables. For example, an oil price shock would have impact on the price levels, which would trigger adjustments in the monetary policy, which would further exert impact on price levels, real output and so on. To capture the intertwined relationships, it would require a system of equations.\nConsider an example of a backward-looking Keynesian system:\n\\[\n\\begin{aligned}\ny_t &= \\phi y_{t-1} -\\psi(r_{t-1}-\\phi_{t-1}) + \\epsilon_t^{IS}\\\\\n\\pi_t &= \\delta\\pi_{t-1} + \\kappa(y_{t-1} - y_{t-1}^n) + \\epsilon_t^{S}\\\\\nr_t &= \\beta\\pi_t + \\gamma(y_t - y_t^n) + \\epsilon_t^{MP}\n\\end{aligned}\n\\]\nThe first equation is the IS curve, which states the negative relationship between output and real interest rate. \\(\\epsilon_t^{IS}\\) is a structural shock of investment-saving decisions that moves the IS curve. We call it structural shock, because it is associated with a structural meaning, not a mere residual from a regression. The second equation describes the Phillips curve, which postulates a positive correlation between inflation and output gap (where \\(y_{t}^n\\) is the potential output level). \\(\\epsilon_t^{S}\\) is the supply shock, which originates from exogenous supply conditions (such as weather), that could also affect inflation. The third equation is the Taylor’s rule for monetary policy, which sets the interest rate in response to inflation and output gap. \\(\\epsilon_t^{MP}\\) is the monetary policy shock, which is the unpredictable part of the monetary policy decision making.\nThe set of equations are called structural equations, in a sense that they describe the structure of the economy according to some economic theories (particularly the Keynesian theory). These equations were very popular in 70s and 80s until Sims (1980) questioned their validity. The fact is, these equations impose a lot of restrictions on the relationships between the variables. For example, why output responds to real interest rate but not inflation? Why interest rate does not enter the equation of inflation? Yes, the equations are justified by the theory. But who knows the theory is correct? In reality, economic variables influence each other, often in a way unknown to theorists. So why not model the economy unrestrictively and let the data tell us the relationships between the variables?\n\\[\n\\begin{aligned}\ny_t &= \\phi_{11} y_{t-1} + \\phi_{12}\\pi_{t-1} + \\phi_{13} r_{t-1} +\\cdots \\\\\n\\pi_t &= \\phi_{21} y_{t-1} + \\phi_{22}\\pi_{t-1} + \\phi_{23} r_{t-1} +\\cdots \\\\\nr_t &= \\phi_{31} y_{t-1} + \\phi_{32}\\pi_{t-1} + \\phi_{33} r_{t-1} +\\cdots \\\\\n\\end{aligned}\n\\]\nThis gives rise to a vector autoregressive system:\n\\[\n\\begin{bmatrix}\ny_t \\\\ \\pi_t \\\\ r_t\n\\end{bmatrix} = \\sum_{j=1}^{p}\n\\begin{bmatrix}\n\\phi_{j,11} & \\phi_{j,12} & \\phi_{j,13} \\\\\n\\phi_{j,21} & \\phi_{j,22} & \\phi_{j,23} \\\\\n\\phi_{j,31} & \\phi_{j,32} & \\phi_{j,33} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\ny_{t-j} \\\\ \\pi_{t-j} \\\\ r_{t-j}\n\\end{bmatrix} +\n\\begin{bmatrix}\nu_t^y \\\\ u_t^\\pi \\\\ u_t^r\n\\end{bmatrix}\n\\]\nThis is called a vector autoregression (VAR). Ever since being proposed by Sims (1980), VARs have been the Swiss knife for empirical macroeconomists. This chapter offers a thorough introduction of this Nobel prize winning technique. We start by introduce the general general vector processes and the estimation methods. We then explain how VARs map to the structural framework (SVAR). We finish the chapter by a discussion on dimension reduction techniques and the cases when a VAR system is not stationary."
  },
  {
    "objectID": "26_var.html#definitions",
    "href": "26_var.html#definitions",
    "title": "26  Vector Processes",
    "section": "26.1 Definitions",
    "text": "26.1 Definitions\nLet \\(\\boldsymbol y_t\\) be an \\(n\\times 1\\) vector. An vector autoregressive process is defined as\n\\[\n\\boldsymbol{y_t = \\alpha + \\Phi_1 y_{t-1} + \\Phi_2 y_{t-2} +\\cdots + \\Phi_p y_{t-p} + \\epsilon_t}\n\\]\nwhere \\(\\boldsymbol\\epsilon_t\\) is the vector white noise with \\(\\mathbb E(\\boldsymbol\\epsilon_t)=\\boldsymbol 0\\) and \\(\\mathbb E(\\boldsymbol{\\epsilon_t\\epsilon_t'})=\\boldsymbol\\Omega\\). In a vector form,\n\\[\n\\begin{bmatrix}\ny_{1,t} \\\\ y_{2,t} \\\\ \\vdots \\\\y_{n,t}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\alpha_1 \\\\ \\alpha_2 \\\\ \\vdots \\\\ \\alpha_n\n\\end{bmatrix} +\n\\sum_{j=1}^{p}\n\\begin{bmatrix}\n\\phi_{j,11} & \\phi_{j,12} & \\dots & \\phi_{j,1n} \\\\\n\\phi_{j,21} & \\phi_{j,22} & \\dots & \\phi_{j,2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\phi_{j,n1} & \\phi_{j,n2} & \\dots & \\phi_{j,nn} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\ny_{1,t-j} \\\\ y_{2,t-j} \\\\ \\vdots \\\\y_{n,t-j}\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_{1,t} \\\\ \\epsilon_{2,t} \\\\ \\vdots \\\\ \\epsilon_{n,t}\n\\end{bmatrix}\n\\]\nEach component \\(y_{j,t}\\) corresponds to \\(T\\) observations in the data. So from the perspective of data, each VAR is represented by a dataset with \\(T\\) rows and \\(n\\) columns. To unpack the matrix notation, the first row of the vector system is\n\\[\n\\begin{aligned}\ny_{1,t} = \\alpha_1\n& + \\phi_{1,11} y_{1,t-1} + \\phi_{1,12} y_{2,t-1} + \\cdots + \\phi_{1,1n} y_{n,t-1} \\\\\n& + \\phi_{2,11} y_{1,t-2} + \\phi_{2,12} y_{2,t-2} + \\cdots + \\phi_{2,1n} y_{n,t-2} \\\\\n&\\:\\vdots \\\\\n& + \\phi_{p,11} y_{1,t-p} + \\phi_{p,12} y_{2,t-p} + \\cdots + \\phi_{p,1n} y_{n,t-p} \\\\\n& + \\epsilon_{1,t}\n\\end{aligned}\n\\]\nSo each variable in a VAR system is a function of the lags of itself and all other variables. In the spirit of Sims, the VAR system is intended to impose minimal restrictions. All variables are treated as endogenous and influencing each other (though we can also include exogenous variables)."
  },
  {
    "objectID": "26_var.html#var-and-vma",
    "href": "26_var.html#var-and-vma",
    "title": "26  Vector Processes",
    "section": "26.2 VAR and VMA",
    "text": "26.2 VAR and VMA\nWe can rewrite it more compactly with the lag operator:\n\\[\n(I_n - \\Phi_1 L -\\Phi_2 L^2 - \\cdots -\\Phi_p L^p) y_t = \\Phi(L) y_t = \\alpha + \\epsilon_t.\n\\]\nSimilarly, we can generalize an MA process to the vector form:\n\\[\ny_t = \\mu + \\epsilon_t + \\Theta_1\\epsilon_{t-1} + \\Theta_2\\epsilon_{t-2} + \\cdots\n= \\mu + \\Theta(L) \\epsilon_t.\n\\]\nSimilar to scalar processes, with stationary \\(y_t\\), VAR and VMA processes can be converted to each other by inverting the lag polynomial\n\\[\n\\Psi(L) = \\Phi^{-1}(L)\n\\]\nwhere the inverse is defined as\n\\[\n[I_n - \\Phi_1 L -\\Phi_2 L^2 - \\cdots][I_n +\\Psi_1 L + \\Psi_2 L^2 + \\cdots] = I_n.\n\\]\nComputationally, we can expand the product of the lag polynomials\n\\[\nI_n + (\\Psi_1-\\Phi_1)L + (\\Psi_2-\\Phi_2-\\Phi_1\\Psi_1)L^2 + \\cdots = I_n\n\\] The coefficients of the inverse lag polynomial can be computed recursively\n\\[\n\\begin{aligned}\n\\Psi_1 &= \\Phi_1 \\\\\n\\Psi_2 &= \\Phi_2 + \\Phi_1\\Psi_1 \\\\\n&\\vdots \\\\\n\\Psi_s &= \\Phi_1\\Psi_{s-1} + \\Phi_2\\Psi_{s-2} + \\cdots + \\Phi_p\\Psi_{s-p}\n\\end{aligned}\n\\]\nwith \\(\\Psi_0=I_n\\), \\(\\Psi_s=0\\) for \\(s<0\\)."
  },
  {
    "objectID": "26_var.html#stationary-conditions",
    "href": "26_var.html#stationary-conditions",
    "title": "26  Vector Processes",
    "section": "26.3 Stationary Conditions",
    "text": "26.3 Stationary Conditions\n\nProposition 26.1 A VAR(\\(p\\)) process is covariance-stationary if all roots of\n\\[\n\\text{det}|I_n- \\Phi_1 z - \\Phi_2 z^2 - \\cdots - \\Phi_p z^p| = 0\n\\]\nlie outside the unit circle (\\(z\\) is a complex scalar).\nThe VAR is said to contain at least one unit root if\n\\[\n\\text{det}|I_n- \\Phi_1 - \\Phi_2 - \\cdots - \\Phi_p | = 0.\n\\]\nAny VMA(\\(q\\)) process is covariance-stationary.\n\n\n\n\n\n\n\nExample\n\n\n\nConsider a two-variable VAR(1) process\n\\[\n\\begin{bmatrix}y_{1t}\\\\y_{2t}\\end{bmatrix}=\n\\begin{bmatrix}0.7 & 0.1 \\\\0.3 & 0.9\\end{bmatrix}\n\\begin{bmatrix}y_{1,t-1}\\\\y_{2,t-2}\\end{bmatrix}+\n\\begin{bmatrix}u_{1t}\\\\u_{2t}\\end{bmatrix}\n\\]\nWe compute the determinant of the matrix\n\\[\n\\text{det}\\left| \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}-\n\\begin{bmatrix}0.7z & 0.1z \\\\0.3z & 0.9z\\end{bmatrix}\n\\right|=0\n\\]\n\\[\n\\begin{aligned}\n(1-0.7z)(1-0.9z)-0.1z\\cdot0.3z=0\n\\end{aligned}\n\\]\nwhich solves to \\(z_1=1\\), \\(z_2=\\frac{5}{3}\\). So the VAR process is not stationary.\n\n\nIf the whole VAR system is stationary it follows that every single component is stationary, but not vice versa (requires proof). Similar to the univariate case, with stationarity, standard OLS and statistical inference applies. In all the sections in the chapter, we assume stationary VARs; we address the unit roots in a VAR at the end of the chapter."
  },
  {
    "objectID": "26_var.html#autocovariance-matrix",
    "href": "26_var.html#autocovariance-matrix",
    "title": "26  Vector Processes",
    "section": "26.4 Autocovariance Matrix",
    "text": "26.4 Autocovariance Matrix\nThe autocovariance matrix for a vector process is defined as\n\\[\n\\Gamma_j = \\mathbb E [(y_t - \\mu)(y_{t-j} - \\mu)'].\n\\]\nFor demeaned \\(y_t\\), we have\n\\[\n\\begin{aligned}\n\\Gamma_j &= \\mathbb{E} (y_t y_{t-j}') = \\mathbb{E}\n\\begin{bmatrix}y_{1,t}\\\\y_{2,t}\\\\\\vdots\\\\y_{n,t}\\end{bmatrix}\n\\begin{bmatrix}y_{1,t-j}&y_{2,t-j}&\\dots&y_{n,t-j}\\end{bmatrix} \\\\[1em]\n&=\\begin{bmatrix}\n\\mathbb{E}(y_{1,t}y_{1,t-j}) & \\mathbb{E}(y_{1,t}y_{2,t-j}) & \\dots & \\mathbb{E}(y_{1,t}y_{n,t-j}) \\\\\n\\mathbb{E}(y_{2,t}y_{1,t-j}) & \\mathbb{E}(y_{2,t}y_{2,t-j}) & \\dots & \\mathbb{E}(y_{2,t}y_{n,t-j}) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbb{E}(y_{n,t}y_{1,t-j}) & \\mathbb{E}(y_{n,t}y_{2,t-j}) & \\dots & \\mathbb{E}(y_{n,t}y_{n,t-j})\n\\end{bmatrix} \\\\[1em]\n&=\\begin{bmatrix}\n\\gamma_{1}(j) & \\gamma_{12}(j) & \\dots & \\gamma_{1n}(j) \\\\\n\\gamma_{21}(j) & \\gamma_{2}(j) & \\dots & \\gamma_{2n}(j) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\gamma_{n1}(j) & \\gamma_{n2}(j) & \\dots & \\gamma_{n}(j) \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]\nNote that \\(\\Gamma_j \\neq \\Gamma_{-j}\\), but \\(\\Gamma_j' = \\Gamma_{-j}\\)."
  },
  {
    "objectID": "27_var2.html#stacked-form",
    "href": "27_var2.html#stacked-form",
    "title": "27  Estimating VAR",
    "section": "27.1 Stacked Form",
    "text": "27.1 Stacked Form\nFor computation, it is more convenient to rewrite the VAR system in transpose:\n\\[\ny_t' = y_{t-1}' A_1' + y_{t-2}' A_2' + \\cdots + y_{t-p}' A_p' + x_t' C' + u_t'\n\\]\nWe can stack observations to represent the whole data set:\n\\[\n\\begin{bmatrix}\ny_1' \\\\ y_2' \\\\ \\vdots \\\\ y_T'\n\\end{bmatrix}_{T\\times n} = \\sum_{j=1}^{p}\n\\begin{bmatrix}\ny_{1-j}' \\\\ y_{2-j}' \\\\ \\vdots \\\\ y_{T-j}'\n\\end{bmatrix}_{T\\times n} \\underset{n\\times n}{A_j'} +\n\\begin{bmatrix}\nx_1' \\\\ x_2' \\\\ \\vdots \\\\ x_T'\n\\end{bmatrix}_{T\\times m} \\underset{m\\times n}{C'} +\n\\begin{bmatrix}\nu_1' \\\\ u_2' \\\\ \\vdots \\\\ u_T'\n\\end{bmatrix}_{T\\times n}\n\\]\nGathering the regressors into a single matrix, one obtains:\n\\[\n\\begin{bmatrix}\ny_1' \\\\ y_2' \\\\ \\vdots \\\\ y_T'\n\\end{bmatrix}_{T\\times n} =\n\\underbrace{\\begin{bmatrix}\ny_0' & y_{-1}' & \\dots & y_{1-p}' & x_1' \\\\\ny_1' & y_0' & \\dots & y_{2-p}' & x_2' \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\ny_{T-1}' & y_{T-2}' & \\dots & y_{T-p}' & x_T'\n\\end{bmatrix}}_{T\\times(np+m)}\n\\underbrace{\\begin{bmatrix}\nA_1' \\\\ A_2' \\\\ \\vdots \\\\ A_p' \\\\ C'\n\\end{bmatrix}}_{(np+m)\\times n} +\n\\begin{bmatrix}\nu_1' \\\\ u_2' \\\\ \\vdots \\\\ u_T'\n\\end{bmatrix}_{T\\times n}\n\\]\nOr, more compactly:\n\\[\n\\underset{T\\times n}{Y} = \\underset{T\\times k}{X}\\ \\underset{k\\times n}{B} +\n\\underset{T\\times n}{U}.\n\\tag{27.2}\\]\nOnce the model is stacked this way, obtaining OLS estimates of the VAR is straightforward. An estimate \\(\\hat B\\) is obtained from:\n\\[\n\\hat B = (X'X)^{-1} X'Y\n\\]\nThis is equivalent to applying OLS on each column variable:\n\\[\n\\begin{bmatrix}\\hat B^{(1)} & \\hat B^{(2)} & \\dots & \\hat B^{(n)}\\end{bmatrix} =\n(X'X)^{-1}X' \\begin{bmatrix}Y^{(1)} & Y^{(2)} & \\dots & Y^{(n)}\\end{bmatrix}\n\\]\nwhere \\(Y^{(i)}\\) denotes \\(i\\)-th column of matrix \\(Y\\). An estimate of the covariance matrix \\(\\Omega\\) can be obtained from:\n\\[\n\\hat\\Omega= \\frac{1}{T-k-1}\\hat{U}'\\hat{U}\n\\]\nUnder the assumptions (1)-(3), the parameters are consistently estimated by OLS regressions. Standard asymptotic results apply."
  },
  {
    "objectID": "27_var2.html#vectorized-form",
    "href": "27_var2.html#vectorized-form",
    "title": "27  Estimating VAR",
    "section": "27.2 Vectorized Form",
    "text": "27.2 Vectorized Form\nAlternatively, one can vectorize the VAR system as:\n\\[\n\\begin{bmatrix}\nY^{(1)} \\\\ Y^{(2)} \\\\ \\vdots \\\\ Y^{(n)}\n\\end{bmatrix}_{nT\\times 1} =\n\\begin{bmatrix}\nX & & & \\\\ & X & & \\\\ & & \\ddots & \\\\ & & & X\n\\end{bmatrix}_{nT \\times nk}\n\\begin{bmatrix}\nB^{(1)} \\\\ B^{(2)} \\\\ \\vdots \\\\ B^{(n)}\n\\end{bmatrix}_{nk \\times 1} +\n\\begin{bmatrix}\nU^{(1)} \\\\ U^{(2)} \\\\ \\vdots \\\\ U^{(n)}\n\\end{bmatrix}_{nT\\times 1}\n\\]\nwhere \\(Y^{(i)}\\) denotes \\(i\\)-th column of matrix \\(Y\\). The vectorized system can be compactly written as\n\\[\ny = \\bar{X}\\beta + u\n\\tag{27.3}\\]\nwhere \\(y = vec(Y)\\), \\(\\bar{X} = I_n \\otimes X\\), \\(\\beta = vec(B)\\), \\(u = vec(U)\\). Also, one has \\(\\mathbb E(uu') = \\bar\\Omega\\), where \\(\\bar\\Omega = \\Omega\\otimes I_T\\). An OLS estimate of the vectorised \\(\\beta\\) can be obtained as:\n\\[\n\\hat\\beta = (\\bar{X}' \\bar{X})^{-1} \\bar{X}'y\n\\]\nIt should be noted that Equation 27.2 and Equation 27.3 are just alternative but equivalent representations of the same VAR model Equation 27.1. We opt to use one representation or the other according to which one is most convenient for our purposes. Equation 27.2 is typically faster to compute (because smaller matrices produce more accurate estimates), while statistical inference works more naturally with the vectorized form."
  },
  {
    "objectID": "28_gc.html#definition",
    "href": "28_gc.html#definition",
    "title": "28  Granger Causality",
    "section": "28.1 Definition",
    "text": "28.1 Definition\nGranger causality conceptualizes the usefulness of some variables in forecasting other variables.\n\nDefinition 28.1 (Granger Causality) \\(x\\) fails to Granger-cause \\(y\\) if for all \\(s >0\\), the MSE of a forecast \\(\\hat y_{t+s}\\) based on \\((y_t,y_{t-1},...x_t,x_{t-1},...)\\) is the same as the MSE of the forecast based on \\((y_t,y_{t-1},...)\\):\n\\[\n\\text{MSE}[\\hat y_{t+s}|y_t,y_{t-1},...] =\n\\text{MSE}[\\hat y_{t+s}|y_t,y_{t-1},...x_t,x_{t-1},...].\n\\]\nEquivalently, we say \\(x\\) is exogenous to \\(y\\), or \\(x\\) is not linearly informative about \\(y\\).\n\nIt must be noted that Granger causality has nothing to do with the causality defined by counterfactuals. The word “causality” is unfortunately misleading. It would be better named as “Granger predictability”."
  },
  {
    "objectID": "28_gc.html#granger-causality-test",
    "href": "28_gc.html#granger-causality-test",
    "title": "28  Granger Causality",
    "section": "28.2 Granger Causality Test",
    "text": "28.2 Granger Causality Test\nConsider the Granger causality test in a single equation setup:\n\\[\ny_t = \\alpha + \\phi_1 y_{t-1}+\\cdots+\\phi_p y_{t-p} +\n\\beta_1 x_{t-1} + \\cdots +\\beta_q x_{t-q} + u_t\n\\]\nTesting whether \\(x\\) Granger-causes \\(y\\) is equivalent to test\n\\[\nH_0: \\beta_1 = \\beta_2=\\cdots=\\beta_q=0\n\\]\nThe test can be done by comparing the residual sum of squares (RSS) with and without \\(x\\) as the regressors. Assuming \\(H_0\\) holds, we would have the restricted model:\n\\[\ny_t = \\alpha + \\phi_1 y_{t-1} + \\cdots + \\phi_p y_{t-p} + u_t^R\n\\]\nCompute RSS for the restricted model:\n\\[\n\\text{RSS}_0 = \\sum_{t=1}^{T} (\\hat u_t^{R})^2\n\\]\nAlso compute the RSS for the unrestricted model, i.e. including all \\(x\\) as the regressors:\n\\[\n\\text{RSS}_1 = \\sum_{t=1}^{T} \\hat u_t^2\n\\]\nThe joint significance can be tested by the \\(F\\) ratio:\n\\[\nS = \\frac{(\\text{RSS}_0 - \\text{RSS}_1)/q}{\\text{RSS}_1/(T-2q-1)} \\sim F(q, T-2q-1).\n\\]"
  },
  {
    "objectID": "28_gc.html#granger-causality-in-var",
    "href": "28_gc.html#granger-causality-in-var",
    "title": "28  Granger Causality",
    "section": "28.3 Granger Causality in VAR",
    "text": "28.3 Granger Causality in VAR\nIn a VAR setting, \\(x\\) does not Granger-cause \\(y\\) if the coefficient matrix are lower triangular for all \\(j\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}y_t \\\\ x_t\\end{bmatrix} &=\n\\begin{bmatrix}\\alpha_1 \\\\ \\alpha_2\\end{bmatrix} +\n\\begin{bmatrix}\\phi_{1,11} & 0 \\\\ \\phi_{1,21} & \\phi_{1,22} \\end{bmatrix}\n\\begin{bmatrix}y_{t-1} \\\\ x_{t-1}\\end{bmatrix} +\n\\begin{bmatrix}\\phi_{2,11} & 0 \\\\ \\phi_{2,21} & \\phi_{2,22}\\end{bmatrix}\n\\begin{bmatrix}y_{t-2} \\\\ x_{t-2}\\end{bmatrix} + \\cdots \\\\[1em]\n&=\n\\begin{bmatrix}\\alpha_1 \\\\ \\alpha_2\\end{bmatrix} +\n\\begin{bmatrix}\\phi_{11}(L) & 0 \\\\ \\phi_{21}(L) & \\phi_{22}(L)\\end{bmatrix}\n\\begin{bmatrix}y_{t} \\\\ x_{t} \\end{bmatrix} +\n\\begin{bmatrix}u_{t} \\\\ v_{t} \\end{bmatrix}\n\\end{aligned}\n\\]\nIt is equivalent to the MA representation:\n\\[\n\\begin{bmatrix}y_t \\\\ x_t\\end{bmatrix} =\n\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix} +\n\\begin{bmatrix}\\theta_{11}(L) & 0 \\\\ \\theta_{21}(L) & \\theta_{22}(L)\\end{bmatrix}\n\\begin{bmatrix}u_t \\\\ v_t\\end{bmatrix}\n\\]\nTo test Granger causality in a VAR setting, we need to introduce the likelihood ratio (LR) test."
  },
  {
    "objectID": "28_gc.html#likelihood-ratio-test",
    "href": "28_gc.html#likelihood-ratio-test",
    "title": "28  Granger Causality",
    "section": "28.4 Likelihood Ratio Test",
    "text": "28.4 Likelihood Ratio Test\nLet’s quickly derive the maximum likelihood estimator (MLE) for the VAR. The joint probability density function is\n\\[\n\\begin{aligned}\nf(y_T,y_{T-1},\\dots,y_1 | y_0,\\dots,y_{1-p};\\Theta)\n&= f(y_T|y_{T-1}\\dots)f(y_{T-1}|y_{T-2}\\dots)\\cdots f(y_1|y_0\\dots) \\\\[1em]\n&= \\prod_{t=1}^{T} f(y_t|y_{t-1},y_{t-2},... ,y_{1-p};\\Theta)\n\\end{aligned}\n\\]\nDefine \\(x_t'=\\begin{bmatrix}1 & y_{t-1} & \\dots & y_{t-p}\\end{bmatrix}\\) the collection of all regressors, and \\(B'=\\begin{bmatrix}\\alpha & \\Phi_1 & \\dots & \\Phi_p\\end{bmatrix}\\) the collection of all parameters. The log-likelihood function can be written as\n\\[\n\\begin{aligned}\n\\ell(\\theta) &= \\sum_{t=1}^T \\log f(y_t|y_{t-1},y_{t-2},... ,y_{1-p};\\theta) \\\\\n&= -\\frac{T}{2}\\log |2\\pi\\Omega^{-1}| - \\frac{1}{2}\\sum_{t=1}^{T}[(y_t - B'x_t)'\\Omega^{-1}(y_t-B'x_t)]\n\\end{aligned}\n\\]\nwhere \\(\\Omega\\) is the variance-covariance matrix of the residuals. Maximizing the log-likelihood function gives the ML estimator:\n\\[\n\\begin{aligned}\n\\hat B'_{n \\times (np+1)} &=\n\\left[\\sum_{t=1}^{T} y_tx_t'\\right]\\left[\\sum_{t=1}^{T} x_t x_t'\\right]^{-1} \\\\\n\\hat\\Omega_{n \\times n} &= \\frac{1}{T} \\sum_{t=1}^{T}\\hat e_t \\hat e_t'\n\\end{aligned}\n\\]\nwhere \\(e_t=[u_t\\ v_t]'\\).\nThe likelihood ratio (LR) test is motivated by the fact that different specifications give different likelihood evaluates. By comparing the likelihood difference, we can test the significance of one specification versus the alternative.\nThe null hypothesis (\\(H_0\\)) could be a specification with a particular lag length, or a particularization with certain exogeneity restrictions. We compute the covariance matrix under the null hypothesis (\\(H_0\\)) and the alternative (\\(H_1\\)) respectively:\n\\[\n\\begin{aligned}\n\\hat\\Omega_0 &= \\frac{1}{T} \\sum_t \\hat e_t(H_0) \\hat e_t(H_0)' \\\\\n\\hat\\Omega_1 &= \\frac{1}{T} \\sum_t \\hat e_t(H_1) \\hat e_t(H_1)' \\\\\n\\end{aligned}\n\\]\nThe corresponding log-likelihoods are\n\\[\n\\begin{aligned}\n\\ell_0^* &= -\\frac{T}{2} \\log |2\\pi\\hat\\Omega_0^{-1}| - \\frac{Tn}{2} \\\\\n\\ell_1^* &= -\\frac{T}{2} \\log |2\\pi\\hat\\Omega_1^{-1}| - \\frac{Tn}{2} \\\\\n\\end{aligned}\n\\]\nThe difference between the log-likelihoods\n\\[\n2(\\ell_1^* - \\ell_0^*) = T(\\log |\\hat\\Omega_0| - \\log |\\hat\\Omega_1|)\n\\]\nhas a \\(\\chi^2\\) distribution with degree of freedom \\(n^2(p_1 - p_0)\\).\nNow consider Granger causality test in a VAR setting\n\\[\n\\begin{bmatrix}y_t \\\\ x_t\\end{bmatrix} =\n\\begin{bmatrix}\\alpha_1 \\\\ \\alpha_2\\end{bmatrix} +\n\\begin{bmatrix}\\phi_{11}(L) & \\phi_{12}(L) \\\\ \\phi_{21}(L) & \\phi_{22}(L)\\end{bmatrix}\n\\begin{bmatrix}y_t \\\\ x_t\\end{bmatrix} +\n\\begin{bmatrix}u_{t} \\\\ v_{t}\\end{bmatrix}\n\\]\nTesting \\(x\\) fails to Granger-cause \\(y\\) is equivalent to test \\(\\phi_{12}=0\\). Therefore, the restricted regression under \\(H_0\\) is\n\\[\ny_t = \\alpha_1 + \\phi_{11}(L) y_t + u_t^{R}\n\\]\nThe unrestricted regression is\n\\[\ny_t = \\alpha_1 + \\phi_{11}(L) y_t + \\phi_{12}(L) x_t + u_{t}^{U}\n\\]\nEstimate the variance-covariance matrices\n\\[\n\\begin{aligned}\n\\hat\\Omega^U &= \\frac{1}{T}\\sum_t u_t^U {u_t^U}' \\\\\n\\hat\\Omega^R &= \\frac{1}{T}\\sum_t u_t^R {u_t^R}' \\\\\n\\end{aligned}\n\\]\nForm the LR test statistics\n\\[\n\\text{LR} = T(\\log |\\hat\\Omega^U| - \\log |\\hat\\Omega^R|) \\sim \\chi^2\n\\]\nIf the LR statistics is significant, it would mean \\(x\\) is informative about \\(y\\) (\\(x\\) Granger-causes \\(y\\)). Otherwise, if it makes no difference including \\(x\\) as regressors, it would mean \\(x\\) fails to Granger-cause \\(y\\)."
  },
  {
    "objectID": "29_svar.html#the-structural-framework",
    "href": "29_svar.html#the-structural-framework",
    "title": "29  Structural VAR",
    "section": "29.1 The Structural Framework",
    "text": "29.1 The Structural Framework\nNow we reconsider the problem of estimating structural shocks. We have introduced the structural shock framework in Chapter 15 the underlying econometric framework of understanding our economy. To briefly recap, we envision our economy as an MA process in which multiple structural shocks are the fundamental driving forces:\n\\[\ny_t =  \\Theta(L) \\epsilon_t\n\\tag{29.1}\\]\nStructural shocks must be distinguished from residuals in a regression. Residuals are prediction errors based on past observations. Residuals can be cross-sectionally or serially correlated. Structural shocks are attached with specific economic meaning. They are also assumed to be unforeseeable and uncorrelated. We do not usually observe structural shocks but they are the conceptualized driving forces in the background.\nIf \\(\\Theta(L)\\) is invertible, we would have\n\\[\n\\Theta^{-1}(L) y_t = \\epsilon_t\n\\]\nwhich is an infinite order AR process. This motivates us to estimate structural shocks via vector autoregressive processes. Suppose we have a VAR process:\n\\[\nA(L) y_t = u_t\n\\tag{29.2}\\]\nwhere \\(u_t = y_t - \\text{Proj}(y_t | y_{t-1},y_{t-2},\\dots)\\) are the projection residuals. The question is, to what extend, or under what conditions, can we identify the structural shocks from this VAR specification?\nThe answer is easier than you might have thought. We only need to identify\n\\[\nu_t = \\Theta_0 \\epsilon_t\n\\tag{29.3}\\]\nwhere \\(\\Theta_0\\) is the first coefficient matrix in the lag polynomial. That is, the condition for identification is that we can find a linear transformation to decompose \\(u_t\\) into \\(\\epsilon_t\\)."
  },
  {
    "objectID": "29_svar.html#invertibility",
    "href": "29_svar.html#invertibility",
    "title": "29  Structural VAR",
    "section": "29.2 Invertibility",
    "text": "29.2 Invertibility\nThe structural MA process is said to be invertible if \\(\\epsilon_t\\) can be linearly determined from current and lagged values of \\(y_t\\):\n\\[\n\\epsilon_t = \\text{Proj}(\\epsilon_t| y_t, y_{t-1},\\dots).\n\\]\nThis means there is no “omitted variable” in the observable space, in the sense that the space spanned by \\(\\{\\epsilon_t, \\epsilon_{t-1},...\\}\\) is fully covered by \\(\\{y_t, y_{t-1},…\\}\\).\n\\[\n\\text{span}\\{\\epsilon_t,\\epsilon_{t-1},...\\} =\n\\text{span}\\{y_t, y_{t-1},...\\} =\n\\text{span}\\{u_t, u_{t-1},...\\}\n\\]\nThis is a strong assumption. Under invertibility, the knowledge of the past true shocks would not even improve the the VAR forecast. But it does not require our VAR system being exhaustive, including everything observable variables in our economy. In a particular application, we would only be interested in a few structural shocks. The invertibility condition requires the observables fully cover the space spanned by the structural shocks of particular interests.\nWith the above condition satisfied, we can show that the identification problem is reduced to identify \\(\\Theta_0\\). Given Equation 29.1 and Equation 29.2, we have\n\\[\nu_t = A(L)y_t = A(L)\\Theta(L)\\epsilon_t \\overset{?}= \\Theta_0 \\epsilon_t\n\\]\nBy definition,\n\\[\n\\begin{aligned}\nu_t &= y_t - \\text{Proj}[y_t | y_{t-1},y_{t-2},...] \\\\[1em]\n&=\\Theta(L)\\epsilon_t - \\text{Proj}[\\Theta(L)\\epsilon_t | y_{t-1},y_{t-2},...]\\\\[1em]\n&=\\Theta_0\\epsilon_t + \\Theta_1\\epsilon_{t-1} +\\cdots+\n\\text{Proj}[\\Theta_0\\epsilon_t + \\Theta_1\\epsilon_{t-1} + \\cdots | y_{t-1},y_{t-2},...] \\\\\n&= \\Theta_0\\epsilon_t - \\Theta_0\\underbrace{\\text{Proj}[\\epsilon_t|y_{t-1},...]}_{=0\\ \\text{by definition}} + \\sum_{j=1}^{\\infty}\\Theta_j\\{\\epsilon_{t-j} - \\underbrace{\\text{Proj}[\\epsilon_{t-j}|y_{t-1},...]}_{=\\epsilon_{t-j}\\ \\text{by invertibility}}\\} \\\\\n&= \\Theta_0\\epsilon_t.\n\\end{aligned}\n\\]\n\nProposition 29.1 (Assumptions of Structural VAR)  \n\nAll variables are stationary;\nThe space spanned by the innovations and the structural shocks coincide such that \\(u_t = \\Theta_0\\epsilon_t\\);\nThe structural process \\(y_t = \\Theta(L)\\epsilon_t\\) is invertible.\n\nUnder the assumptions, identifying \\(\\Theta_0\\) is equivalent to identify the structural shocks \\(\\epsilon_t = \\Theta_0^{-1} u_t\\).\n\nIn essence, structural identification is equivalent to sorting out the contemporaneously correlated residuals into uncorrelated shocks that can be attached to certain economic meanings. As we will see, the decomposition is largely subjective, according to researchers’ understanding of how structural shocks are correlated contemporaneously.\nIf the invertibility assumption fails, that means there exists no mapping from VAR residuals to the structural shocks. Non-invertibility arises when the observed variables fail to span the space of the state variables (structural shocks). If this is the case, we can include more variables to expand the space; or we may choose to simply ignore it if we believe the wedge between the spaces spanned by VAR residuals and structural shocks are small."
  },
  {
    "objectID": "29_svar.html#identification",
    "href": "29_svar.html#identification",
    "title": "29  Structural VAR",
    "section": "29.3 Identification",
    "text": "29.3 Identification\nWith invertibility, the essential task of SVAR is to decompose Equation 29.3 to recover the structural shocks. The key is to estimate \\(\\Theta_0\\). Consider the second-order moments of Equation 29.3:\n\\[\n\\Theta_0\\mathbb{E}(\\epsilon_t\\epsilon_t')\\Theta_0' = \\mathbb{E}(u_tu_t')\n\\]\nEstimating the VAR system Equation 29.2 by OLS gives \\(\\hat\\Omega = \\mathbb{\\hat E}(u_tu_t')\\). By definition, elements of \\(\\epsilon_t\\) are orthogonal to each other, so \\(D=\\mathbb E (\\epsilon_t\\epsilon_t')\\) is diagonal. Estimated \\(\\hat\\Omega\\) gives \\(n(n+1)/2\\) distinct values. Identification of \\(D\\) requires \\(n\\) values. So no more than \\(n(n-1)/2\\) parameters in \\(\\Theta_0\\) can be identified. That means, we cannot identify the full \\(n\\times n\\) matrix \\(\\Theta_0\\) without restrictions.\n\n29.3.1 Recursive restriction\nOne common way to impose restrictions on \\(\\Theta_0\\) is to require it being lower triangular. Thus eliminating \\(n(n-1)/2\\) entries. We also assume the structural shocks have the same magnitude as the residuals, so the diagonal entries are \\(1\\)s. For example, in the three variable Keynesian system, we may assume\n\\[\n\\begin{bmatrix}\nu_t^\\pi \\\\ u_t^y \\\\ u_t^m\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n* & 1 & 0 \\\\\n* & * & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\epsilon_t^{S} \\\\ \\epsilon_t^{IS} \\\\ \\epsilon_t^{MP}\n\\end{bmatrix}\n\\]\nThe recursive structure is equivalent to imposing restrictions on the contemporaneous relationships between variable, or imposing different reaction speed to the structural shocks. In the above example, we assume the observed monetary policy (interest rate) responds to IS shock, supply shock and monetary policy shock contemporaneously; but inflation and output respond to monetary policy shock with a lag (sluggish response). Output responds to IS shock and supply shock contemporaneously, but inflation responds to IS shock with a lag. Of course, one can question the validity of these assumptions, or even the validity of the conceptualization of the three structural shocks. But this is a structural question, not an econometric one. Economists have always been debating what are the proper structures to describe the economy.\nIn the recursive identification scheme, the ordering of the variables is the vital decision to make. Typically, the slow-moving variables are ordered first, and the fast-moving variables last, provided the \\(\\Theta_0\\) is upper triangular. In the literature involving monetary policy, a “slow-r-fast” scheme is widely adopted. That is, low-moving variables such as real output and price levels are ordered before interest rates; and fast-moving variables such as financial market indexes are ordered after interest rates. Because financial market absorbs information in real time, even ahead of the monetary policy decision. But it take time for real variables to materialize the impact of monetary policy changes.\n\n\n29.3.2 Non-recursive restriction\nWe may also impose non-recursive structure based on theories or intuitions. Consider a model constituted of the demand and supply of an agriculture product, and the weather condition that affects the supply of the product. We assume weather does not depend on market behaviors. In addition, the supply but not the demand is influenced by the weather. This results in an identification matrix as follows\n\\[\n\\begin{bmatrix}\nu_t^d \\\\ u_t^s \\\\ u_t^w\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & -\\beta & 0 \\\\\n1 & -\\gamma & -\\delta \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\epsilon_t^{d} \\\\ \\epsilon_t^{s} \\\\ \\epsilon_t^{w}\n\\end{bmatrix}\n\\]\nNote that there are only three parameters to be estimated in the matrix. So \\(\\Theta_0\\) in this case can also be identified.\nStructural VAR literature has invented lots of identification schemes, such as long-run restrictions, sign restrictions, and so on. These are left for the readers to explore themselves.\n\n\n\n\n\n\nTakeaways\n\n\n\n\nReduced-form VARs only require errors be free of serial correlation, but allow cross-sectional correlations. The errors do not have structural interpretation.\nStructural identification means to decompose reduced-form residuals into uncorrelated structural shocks, so that we can attach structural meaning to the identified shocks.\nUnder the assumption of invertibility, structural identification boils down to restricting the contemporaneous correlations between the endogenous variables. There are various identification schemes including Cholesky decomposition, sign restrictions, long-run restrictions, and so on."
  },
  {
    "objectID": "30_irf.html#estimating-svar",
    "href": "30_irf.html#estimating-svar",
    "title": "30  IRF and FEVD",
    "section": "30.1 Estimating SVAR",
    "text": "30.1 Estimating SVAR\nGiven the Structural VAR,\n\\[\n\\Theta_0 y_t = \\Phi_1 y_{t-1} + \\Phi_2 y_{t-2} + \\cdots + \\Phi_p y_{t-p} + \\epsilon_t\n\\]\nWe estimate the reduced-form VAR using the methods in Chapter 27:\n\\[\ny_t = A_1 y_{t-1} + A_2 y_{t-2} + \\cdots + A_p y_{t-p} + u_t\n\\]\nwhere \\(A_j = \\Theta_0^{-1}\\Phi_j\\) and \\(u_t = \\Theta_0^{-1}\\epsilon_t\\). \\(\\Theta_0\\) can be estimated using the identification scheme discussed in Chapter 29, so that the structural parameters can be recovered accordingly.\nHowever, the coefficients of a VAR are difficult to interpret. For empirical analysis, we are particularly interested in estimating the impact of a structural shock on other economic variables (e.g. the impact of monetary policy shock on inflation). We cannot read off this information from the estimated VAR coefficients.\nIn this section, we introduce two reporting techniques: impulse-response functions (IRF) and forecast error variance decomposition (FEVD)."
  },
  {
    "objectID": "30_irf.html#impulse-response-functions",
    "href": "30_irf.html#impulse-response-functions",
    "title": "30  IRF and FEVD",
    "section": "30.2 Impulse-Response Functions",
    "text": "30.2 Impulse-Response Functions\nWith VAR parameters estimated, we can convert it to the MA form:\n\\[\ny_t = A^{-1}(L)u_t = u_t + \\Psi_1 u_{t-1} + \\Psi_2 u_{t-2} + \\cdots\n\\]\nor written in structural shocks:\n\\[\ny_t = \\Theta_0^{-1}\\epsilon_t + \\Psi_1\\Theta_0^{-1}\\epsilon_{t-1} +\n\\Psi_2\\Theta_0^{-1}\\epsilon_{t-2} + \\cdots\n\\]\nWith this structural MA form, we can directly read off the impact of structural shock \\(j\\) on observable variable \\(k\\):\n\\[\\frac{\\partial y_{t+h}^k}{\\partial\\epsilon_t^j} = (\\Psi_h\\Theta_0^{-1})_{kj}\\]\nA sequence of the marginal impacts over time \\(\\left\\{\\frac{\\partial y_{t}^k}{\\partial\\epsilon_t^j}, \\frac{\\partial y_{t+1}^k}{\\partial\\epsilon_t^j}, \\frac{\\partial y_{t+2}^k}{\\partial\\epsilon_t^j}, \\dots\\right\\}\\) constitute the dynamic response of variable \\(k\\) in response to structural shock \\(j\\), also known as the impulse-response function.\nNote that \\(\\frac{\\partial y_{t+h}^k}{\\partial u_t^j}\\) cannot be interpreted as the response of \\(y_{t+h}^k\\) after a shock on \\(u_t^j\\). Because \\(u_t^j\\) could be a linear combination of multiple structural shocks, e.g.\n\\[\nu_t^1 =a_1\\epsilon_t^1 + a_2 \\epsilon_t^2 + a_3 \\epsilon_t^3\n\\]\nTherefore, no structural meaning can be attached to \\(\\frac{\\partial y_{t+h}^k}{\\partial u_t^j}\\). But \\(\\frac{\\partial y_{t+h}^k}{\\partial\\epsilon_t^j}\\) is interpretable, as structural shocks \\(\\epsilon_t\\) are uncorrelated.\nIf \\(\\Theta_0\\) has a recursive structure, \\(\\Theta_0\\) can be found simply by applying decomposition to the OLS-estimated \\(\\hat\\Omega\\):\n\\[\n\\hat\\Omega=LDL'\n\\]\nIf \\(\\hat\\Omega\\) is positive definite, such decomposition always exist, with \\(D\\) diagonal and \\(L\\) lower triangular with \\(1\\)s on the diagonal.\nIf we restrict \\(D\\) to be an identity matrix, the decomposition becomes\n\\[\n\\hat\\Omega = PP'\n\\]\nwhere \\(P = LD^{1/2}\\). This is known as the Cholesky decomposition. By applying the Cholesky decomposition, we have\n\\[\n\\zeta_t = P^{-1}u_t = D^{-1/2}L^{-1}u_t = D^{-1/2}\\epsilon_t\n\\]\nin which \\(1\\) unit shock to \\(\\zeta_t\\) is equivalent to \\(1\\) standard deviation shock to \\(\\epsilon_t\\). The IRFs estimated with Cholesky decomposition thus have the interpretation of response to standard-deviation shocks."
  },
  {
    "objectID": "30_irf.html#irf-standard-errors",
    "href": "30_irf.html#irf-standard-errors",
    "title": "30  IRF and FEVD",
    "section": "30.3 IRF Standard Errors",
    "text": "30.3 IRF Standard Errors\nThe IRF confidence intervals are usually constructed by bootstrapping.\n\nEstimate the VAR by OLS. Save residuals \\(\\hat u_1, \\hat u_2, \\dots, \\hat u_T\\).\nRandomly pick \\(u_1\\) from \\(\\{\\hat u_1, \\hat u_2, \\dots, \\hat u_T\\}\\), which will be used to construct an artificial sample. Generate \\[y_1^{(1)} = \\hat c + \\hat\\Phi_1 y_0 + \\hat\\Phi_2 y_{-1}+\\cdots + \\hat\\Phi_p y_{1-p} + u_1^{(1)}\\]where \\(y_0, y_{-1},\\dots y_{1-p}\\) are pre-sampled values that were actually observed. Take a second draw \\(u_2\\). Generate \\[y_2^{(1)} = \\hat c + \\hat\\Phi_1 y_1 + \\hat\\Phi_2 y_{0}+\\cdots + \\hat\\Phi_p y_{2-p} + u_2^{(1)}\\]Proceed until \\(\\{ y_1^{(1)}, y_2^{(1)},\\dots,y_T^{(1)} \\}\\) are generated. Run OLS on the simulated data, calculate the impulse-response function \\(\\{ \\Psi_h^{(1)} \\}\\).\nRepeat the above step, generate \\(\\{ y_1^{(2)}, y_2^{(2)},\\dots,y_T^{(2)} \\}\\). Estimate the IRF again \\(\\{ \\Psi_h^{(2)} \\}\\). Repeat the process \\(N\\) times and get \\(N\\) IRFs. Sort them by \\(\\Psi^{(1)} \\leq \\Psi^{(2)} \\cdots \\leq \\Psi^{(N)}\\).\nThe \\(\\alpha\\) confidence interval is constructed as \\([\\Psi_{[N\\alpha/2]}, \\Psi_{[N(1-\\alpha/2)]}]\\)."
  },
  {
    "objectID": "30_irf.html#fevd",
    "href": "30_irf.html#fevd",
    "title": "30  IRF and FEVD",
    "section": "30.4 FEVD",
    "text": "30.4 FEVD\nWe would also like to know the relative importance of each structural force, which can be gauged by computing the forecast error variance decomposition (FEVD). Consider the forecast error\n\\[\ny_{t+h} - \\hat y_{t+h|t} = \\Theta_0^{-1}\\epsilon_{t+h} + \\Psi_1\\Theta_0^{-1}\\epsilon_{t+h-1} +\n\\cdots + \\Psi_{h-1}\\Theta_0^{-1}\\epsilon_{t+1}\n\\]\nThe mean squared error (MSE) is\n\\[\n\\begin{aligned}\n\\text{MSE}(\\hat y_{t+h|t})\n&= \\mathbb{E}[(y_{t+h} - \\hat y_{t+h|t})(y_{t+h} - \\hat y_{t+h|t})'] \\\\[1em]\n&= \\Omega + \\Psi_1\\Omega\\Psi_1' + \\Psi_2\\Omega\\Psi_2' + \\cdots + \\Psi_{h-1}\\Omega\\Psi_{h-1}'\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\Omega &= \\mathbb{E}(\\Theta_0^{-1}\\epsilon_{t}\\epsilon_{t}'{\\Theta_0^{-1}}') \\\\\n&= \\begin{bmatrix}\na_1 & a_2 & \\dots & a_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\text{var}(\\epsilon_{1,t}) & & & \\\\\n& \\text{var}(\\epsilon_{2,t}) & & \\\\\n& & \\ddots & \\\\\n& & & \\text{var}(\\epsilon_{n,t}) \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\na_1' \\\\ a_2' \\\\ \\vdots \\\\ a_n'\n\\end{bmatrix} \\\\[1em]\n&= a_1a_1'\\text{var}(\\epsilon_{1,t}) + a_2a_2'\\text{var}(\\epsilon_{2,t}) +\n\\cdots + a_n a_n'\\text{var}(\\epsilon_{n,t})\n\\end{aligned}\n\\]\nTherefore,\n\\[\n\\text{MSE}(\\hat y_{t+h|t}) =\\sum_{j=1}^n \\{ \\text{var}(\\epsilon_{j,t})\n[a_ja_j' + \\Psi_1a_ja_j'\\Psi_1' + \\cdots + \\Psi_{h-1}a_ja_j'\\Psi_{h-1}'] \\}\n\\]\nThe contribution of the \\(j\\)-th structural force to the MSE of the \\(h\\)-period-ahead forecast is given by\n\\[\n\\text{var}(\\epsilon_{j,t})\n[a_ja_j' + \\Psi_1a_ja_j'\\Psi_1' + \\cdots + \\Psi_{h-1}a_ja_j'\\Psi_{h-1}'].\n\\]"
  },
  {
    "objectID": "30_irf.html#example",
    "href": "30_irf.html#example",
    "title": "30  IRF and FEVD",
    "section": "30.5 Example",
    "text": "30.5 Example\n\nlibrary(vars)\ndata = readRDS(\"data/md.Rds\")\ny = cbind(p = data$CPI, y = data$NGDP, r = data$Repo7D) \nvar = VAR(as.ts(y), p = 4)\nirf(var, impulse = \"r\", response = \"p\", ortho = TRUE) |> plot()\n\n\n\n\n\nfevd(var) |> plot()"
  },
  {
    "objectID": "31_dfm.html#principle-component-analysis",
    "href": "31_dfm.html#principle-component-analysis",
    "title": "31  Factor Models",
    "section": "31.1 Principle Component Analysis",
    "text": "31.1 Principle Component Analysis\nQuestion: how to summarize the movements of a large number of time series with fewer time series?\n\n\n\n\n\n50 economic time series in monthly percentage changes\n\n\n\n\nThe native approach is to take the average after some standardization. A better approach is to find a linear combination of them such that:\n\nThe 1st linear combination captures most of the variances among the series;\nThe 2nd linear combination, while being orthogonal to the 1st one, captures most of the remaining variations;\n…\n\nTo formulate the question mathematically, let \\(X = [x_1\\ x_2\\ \\dots\\ x_p]'\\) represent \\(p\\) time series. In the form of data matrix\n\\[\nX = \\begin{bmatrix}\nx_{11} & x_{21} & \\dots & x_{p1} \\\\\nx_{12} & x_{22} & \\dots & x_{p2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{1T} & x_{2T} & \\dots & x_{pT} \\\\\n\\end{bmatrix}_{T \\times p}\n\\]\nWe want to find a linear combination of \\(X\\):\n\\[\na'X = a_1x_1 + a_2x_2 + \\dots + a_px_p\n\\]\nsuch that \\(a'X\\) captures the greatest variance. If \\(X\\) is a 2-dimensional vector \\(X=[x_1\\ x_2]'\\). We want to find a vector \\([a_1\\ a_2]\\) such that projecting the data onto this direction gives the largest variance.\n\n\n\nPCA of a multivariate Gaussian distribution\n\n\nExpressed as an optimization problem, we want to solve\n\\[\n\\max_a \\mathbb{E}(a'XX'a)\n\\]\nwhere we normalize \\(a\\) to a unit vector \\(a'a=1\\). Define the Lagrangian\n\\[\n\\mathcal{L} = a'\\Sigma a - \\lambda_1 (a'a-1)\n\\]\nwhere \\(\\Sigma=\\mathbb{E}(XX')\\). The first-order condition gives\n\\[\n\\frac{\\partial\\mathcal{L}}{\\partial a} = 2\\Sigma a - 2\\lambda_1 a =0\n\\implies \\Sigma a = \\lambda_1 a\n\\]\nThis means, \\(a\\) is an eigenvector of \\(\\Sigma\\). The linear combination has variance\n\\[\na'\\Sigma a = a'\\lambda_1 a= \\lambda_1 a'a = \\lambda_1\n\\]\nwhich is the eigenvalue. So, if we choose \\(a\\) to be the eigenvector associated with the largest eigenvalue of the covariance matrix \\(\\Sigma\\) and project the data onto it, the projected variance will be maximized. The eigenvector points to the direction that the components of \\(X\\) co-variate the most. It is called the first principle component.\nIf we want a second principle component that points to the direction that the data co-variate second to the most, by similar reasoning, we pick the second largest eigenvalue \\(\\lambda_2\\):\n\\[\n\\Sigma b = \\lambda_2 b\n\\]\nThen \\(b\\) is the second principle component. Since \\(\\Sigma\\) is symmetric, we know that the eigenvectors are orthogonal, \\(b\\perp a\\).\n\\[\n\\begin{aligned}\n&\\lambda_1 a'b = (\\lambda_1a)'b = (\\Sigma a)'b = a'\\Sigma'b = a'\\Sigma b = a'\\lambda_2b=\\lambda_2 a'b \\\\[1em]\n&\\implies (\\lambda_1 -\\lambda_2) a'b=0\n\\end{aligned}\n\\]\nSince \\(\\lambda_1\\neq\\lambda_2\\), we have \\(a'b=0\\).\n\n\n\n\n\n\nPCA Procedure\n\n\n\n\nStandardize the dataset \\(\\underset{T \\times n}{X}\\) by mean and variance;\nCalculate the covariance matrix \\(\\underset{n \\times n}{\\Sigma}\\) for the dataset;\nCalculate the eigenvalues and eigenvectors for the covariance matrix;\nSort eigenvalues and their corresponding eigenvectors;\nPick the first \\(k\\) eigenvalues and form a matrix of eigenvectors \\(\\underset{n\\times k}{A}=[a_1\\ a_2\\dots a_k]\\);\nTransform the original matrix \\(\\underset{T\\times k}{Y}=\\underset{T\\times n}{X}\\times \\underset{n\\times k}{A}\\).\n\n\n\nThe transformed matrix \\(Y\\) is the reduced-dimension dataset \\(k << n\\) that captures as much variance as possible of the original dataset. One of the drawbacks of PCA is that the principle components do not have an economic meaning. We do not know how to interpret the principle components except that they represents the co-movements among the original variables. If we want a clear meaning of the principle components, it is suggested we group the original dataset by categories and extract principle components for each category. For example, if we put all price indexes in a group and the principle components of that group would likely be interpreted as the most representative price movement."
  },
  {
    "objectID": "31_dfm.html#factor-augmented-var",
    "href": "31_dfm.html#factor-augmented-var",
    "title": "31  Factor Models",
    "section": "31.2 Factor-augmented VAR",
    "text": "31.2 Factor-augmented VAR\nStandard VAR assumes the “shocks” are identified by the VAR residuals after imposing some restriction. However, due to the “curse of dimensionality”, standard VAR can only include a limited number of variables. Three problems would arise because of that:\n\nSmall number of variables is unlikely to span the space of structural shocks. For example, in the application of identifying monetary policy shocks, if the information set used by the central bank is not fully captured by the VAR, the residuals would not span the space of structural shocks.\nIt is questionable that specific observable measures correspond precisely to the theoretical constructs. For example, the concept of “economic activity” may not be precisely represented by GDP.\nStandard VAR can only generate a limited number of impulse responses that we care about. In many applications, we would care about a wide range of impulse responses from various aspects of the economy.\n\nOne solution is to augment a standard VAR with a few principle components (factors) estimated from big dataset. A factor-augmented VAR (FAVAR) is specified as follows:\n\\[\n\\begin{aligned}\nX_t &= \\Lambda^f f_t + \\Lambda^y y_t + u_t \\\\[1em]\n\\begin{bmatrix}f_t \\\\ y_t\\end{bmatrix} &= \\Phi(L)\n\\begin{bmatrix}f_{t-1} \\\\ y_{t-1}\\end{bmatrix} + v_t\n\\end{aligned}\n\\]\nwhere \\(X_t\\) is an \\(n \\times 1\\) vector representing the information set, which is assumed to be spanned by \\(k \\times 1\\) factors and \\(m \\times 1\\) observable measures. \\(\\Lambda^f\\) is called the factor loading matrix. We assume the factors and observables follow a vector autoregressive process.\nWe follow a two-step procedure to estimate the FAVAR:\n\nEstimate the factors by principle components of \\(X_t\\), denoted by \\(\\hat C_t\\). \\(\\hat F_t\\) is the part of \\(\\hat C_t\\) not spanned by \\(y_t\\).\nEstimate the FAVAR with the factors \\(\\hat F_t\\). Apply identification similar to standard VARs.\n\nWe use the example of Bernanke et al. (2005) as an illustration, in which the authors use an FAVAR to identify the impact of US monetary policy shocks. They treat only the Fed’s policy instrument \\(r_t\\) as observed, all other variables including output and inflation, as unobserved (captured by \\(f_t\\)). They adopt a “slow-r-fast” identification scheme:\n\n\n\n\n\n\n\n\nSlow-moving variables \\(x_t^S\\)\nPolicy instrument \\(r_t\\)\nFast-moving variables \\(x_t^F\\)\n\n\n\n\nOutput\nEmployment\nInflation\n……\nFed fund rate\nAsset price\nFinancial shocks\nNews shocks\n……\n\n\n\nThe FAVAR is specified as\n\\[\n\\begin{aligned}\n\\begin{bmatrix}x_t^S \\\\ x_t^F\\end{bmatrix} &=\n\\begin{bmatrix}\\Lambda_{SS} & 0 & 0 \\\\ \\Lambda_{FS} & \\Lambda_{FR} & \\Lambda_{FF} \\end{bmatrix}\n\\begin{bmatrix}f_t^S \\\\ r_t \\\\ f_t^F\\end{bmatrix} + u_t \\\\\n\\Phi(L)&\\begin{bmatrix}f_t^S \\\\ r_t \\\\ f_t^F\\end{bmatrix} =\n\\begin{bmatrix}\\eta_t^S \\\\ \\eta_t^R \\\\ \\eta_t^F\\end{bmatrix}\n\\end{aligned}\n\\]\nwith recursive identification\n\\[\n\\begin{bmatrix}\\eta_t^S \\\\ \\eta_t^R \\\\ \\eta_t^F\\end{bmatrix} =\n\\begin{bmatrix}\nH_{SS} & 0 & 0 \\\\\nH_{RS} & 1 & 0 \\\\\nH_{FS} & H_{FR} & H_{FF}\n\\end{bmatrix}\n\\begin{bmatrix}\\epsilon_t^S \\\\ \\epsilon_t^R \\\\ \\epsilon_t^F\\end{bmatrix}.\n\\]\nThe IRFs for every variable in \\(X_t\\) can be constructed with the factor loading matrix. FEVDs follow immediately from the coefficients of the MA representation of the VAR system and the variance of the structural shocks.\nBernanke et al. (2005) finds adding the unobserved factors change the result dramatically. The IRF from a standard VAR exhibits the so-called “price puzzle”, that is inflation does not drop immediately after a tightening monetary policy shock. The FAVAR reduces the price puzzle significantly, which indicates the FAVAR indeed incorporates useful information that is missing in standard VARs."
  },
  {
    "objectID": "32_uvar.html#monte-carlo",
    "href": "32_uvar.html#monte-carlo",
    "title": "32  Unit Roots in VAR",
    "section": "32.1 Monte Carlo",
    "text": "32.1 Monte Carlo\nBelow is a Monte Carlo simulation of a 2-dimensional VAR process with unit root, which verifies the Gaussian distribution of its coefficients.\n\nlibrary(tsDyn)\nlibrary(vars)\nset.seed(0)\nbhat = sapply(1:1000, function(i) {\n  # this is a VAR with unit root\n  B = matrix(c(0.7, 0.1, 0.3, 0.9), 2)\n  # simulate the VAR process\n  sim <- VAR.sim(B, n = 300, include = \"none\")\n  mod = VAR(sim); b = coef(mod)\n  # extract the coefficients\n  c(B11 = b$y1['y1.l1', 'Estimate'],\n    B12 = b$y1['y2.l1', 'Estimate'],\n    B21 = b$y2['y1.l1', 'Estimate'],\n    B22 = b$y2['y2.l1', 'Estimate'])\n}) |> t()\n# plot the distribution of the coefficients\n{\n  par(mfrow=c(2,2), mar=c(2,2,2,2))\n  hist(bhat[,'B11'], freq=F, main=\"B11\")\n  hist(bhat[,'B12'], freq=F, main=\"B12\")\n  hist(bhat[,'B21'], freq=F, main=\"B21\")\n  hist(bhat[,'B22'], freq=F, main=\"B22\")\n}\n\n\n\n\nDistributions of the VAR coefficients by Monte Carlo simulation"
  },
  {
    "objectID": "32_uvar.html#conclusions",
    "href": "32_uvar.html#conclusions",
    "title": "32  Unit Roots in VAR",
    "section": "32.2 Conclusions",
    "text": "32.2 Conclusions\nEconomic time series usually comes in seasonally-adjusted (log) levels, which often involve unit roots. Researchers have to make the choice whether to difference the data to stationary or leave it as it is when modelling. There is no single principle to rule them all. It depends on the purpose of the research. It might feel safe to work with stationary time series only. Though stationarity is not necessary for VARs to work properly. Here are the tips from Walter Enders:\n\n\n\n\n\n\nTo difference or not to difference\n\n\n\n\nIf the coefficient of interest can be written as a coefficient on a stationary variable, then a \\(t\\)-test is appropriate.\nYou can use \\(t\\)-tests or \\(F\\)-tests on the stationary variables.\nYou can perform a lag length test on any variable or any set of variables.\nGenerally, you cannot use Granger causality tests concerning the effects of a non-stationary variable.\nThe issue of differencing is important. If the VAR can be written entirely in first differences, hypothesis tests can be performed on any equation or any set of equations using \\(t\\)-tests or \\(F\\)-tests.\nIt is possible to write the VAR in first differences if the variables are \\(I(1)\\) and are not cointegrated. If the variables in question are cointegrated, the VAR cannot be written in first differences."
  },
  {
    "objectID": "33_vecm.html#cointegrated-systems",
    "href": "33_vecm.html#cointegrated-systems",
    "title": "33  VECM*",
    "section": "33.1 Cointegrated Systems",
    "text": "33.1 Cointegrated Systems\n\nDefinition 33.1 An \\(n \\times 1\\) vector \\(y_t\\) is said to be cointegrated if each of its elements individually is \\(I(1)\\) and there exists a non-zero vector \\(a\\in\\mathbb R^n\\) such that \\(a'y_t\\) is stationary. \\(a\\) is called a cointegrating vector.\n\nIf there are \\(h<n\\) linearly independent cointegrating vectors \\((a_1, a_2, \\dots,a_h)\\), then any linear combination \\(k_1a_2 + k_2a_2+\\dots+k_ha_h\\) is also a cointegrating vector. Thus, we say \\((a_1, a_2, \\dots,a_h)\\) form a basis a basis for the space of cointegrating vectors.\nCointegrated systems can be represented by a Vector Error Correlation Model (VECM):\n\\[\n\\Delta y_t = \\Pi y_{t-1} + \\Gamma_1\\Delta y_{t-1} + \\Gamma_2\\Delta y_{t-2} + \\dots +\n\\Gamma_{p-1}\\Delta y_{t-p+1} + \\mu+\\epsilon_t\n\\tag{33.1}\\]\n\nTheorem 33.1 (Engle-Granger Representation Theorem) Any set of \\(I(1)\\) variables are cointegrated if and only if there exists an error correlation (ECM) representation for them.\n\nTherefore, it is inappropriate to model a cointegrated system with differenced VAR. Because the term \\(\\Pi y_{t-1}\\) is missing out, which means a misspecification.\nThe cointegration term can be further factored as \\(\\Pi=\\underset{n \\times h}{A}\\times\\underset{h \\times n}{B'}\\), in which \\(B\\) contains the cointegrating vectors and \\(A\\) hosts the adjustment coefficients. In a bivariate example, it looks like\n\\[\n\\begin{bmatrix}\\Delta y_{1t} \\\\ \\Delta y_{2t}\\end{bmatrix} =\n\\begin{bmatrix}\\alpha_1 \\\\ \\alpha_2\\end{bmatrix}\n\\begin{bmatrix}\\beta_1 & \\beta_2\\end{bmatrix}\n\\begin{bmatrix}y_{1t-1} \\\\ y_{2t-1}\\end{bmatrix} +\n\\sum_{j=1}^{p-1}\n\\begin{bmatrix}\n\\gamma_{j,11} & \\gamma_{j,12} \\\\\n\\gamma_{j,21} & \\gamma_{j,22}\n\\end{bmatrix}\n\\begin{bmatrix}\\Delta y_{1t-j} \\\\ \\Delta y_{2t-j}\\end{bmatrix} +\n\\begin{bmatrix}\\epsilon_{1t} \\\\ \\epsilon_{2t}\\end{bmatrix}\n\\]\nThe economic interpretation is that \\(\\beta_1 y_{1t} + \\beta_2 y_{2t}\\) represents some long-run equilibrium relationship of the two variables. Parameters \\(\\alpha_1,\\alpha_2\\) describe the speed of adjustment, that is how each variable reacts to the deviations from the equilibrium path. Small values of \\(\\alpha_i\\) would imply a relatively unresponsive reaction, which means it takes a long time to return to the equilibrium.\nNote that \\(\\Pi = AB'\\) cannot be a full rank matrix. If there are \\(n\\) independent cointegrating vectors, it follows that any linear combination of the components of \\(y_t\\) is stationary, which effectively means \\(y_t\\) is stationary. \\(\\Pi\\) cannot be zero either. If this is the case, the system is fully characterized by differenced VAR, there is no cointegration. Therefore, for a cointegrated system, it necessitates \\(0<h<n\\).\nA three variable example would be like:\n\\[\n\\begin{bmatrix}\\Delta y_{1t} \\\\ \\Delta y_{2t} \\\\ \\Delta y_{3t}\\end{bmatrix} =\n\\begin{bmatrix}\n\\alpha_{11} & \\alpha_{12}\\\\\n\\alpha_{21} & \\alpha_{22}\\\\\n\\alpha_{31} & \\alpha_{32}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_{11} & \\beta_{12} & \\beta_{13}\\\\\n\\beta_{21} & \\beta_{22} & \\beta_{23}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}y_{1t-1} \\\\ y_{2t-1} \\\\ y_{3t-1}\\end{bmatrix} +\n\\cdots\n\\]\nIn general, if \\(y_t\\) has \\(n\\) non-stationary components, there could be at most \\(n-1\\) cointegrating vectors. The number of cointegrating vectors is also called the cointegrating rank.\nNote that a single equation ECM is equivalent to an ARDL model:\n\\[\n\\begin{aligned}\n&\\Delta y_t = \\alpha(y_{t-1} - \\delta-\\beta x_{t-1}) + \\gamma\\Delta x_t + u_t \\\\\n\\Leftrightarrow &\\ y_t = (\\alpha+1)y_{t-1} + \\gamma x_t - (\\alpha\\beta+\\gamma)x_{t-1}\n-\\alpha\\delta + u_t \\\\\n\\Leftrightarrow &\\ y_t = b_1y_{t-1} + b_2x_t + b_3x_{t-1} + c + u_t\n\\end{aligned}\n\\]\nGiven a (possibly) cointegrated system, we would like to know if any cointegrating relations exist and how many cointegrating vectors there are. Johansen (1991) provides a likelihood-based method to test and estimate a cointegrated system. But before we introduce the Johansen’s method, we need the prerequisite knowledge of canonical correlations."
  },
  {
    "objectID": "33_vecm.html#canonical-correlation",
    "href": "33_vecm.html#canonical-correlation",
    "title": "33  VECM*",
    "section": "33.2 Canonical Correlation",
    "text": "33.2 Canonical Correlation\nPrinciple component analysis (PCA) finds a linear combination of \\([x_1\\ x_2\\ \\dots x_n]\\) that produces the largest variance. What if we want to extend the analysis to the correlations between two datasets: \\(\\underset{T \\times n}{X} = [x_1\\ x_2\\ \\dots x_n]\\) and \\(\\underset{T \\times m}{Y} = [y_1\\ y_2\\ \\dots y_m]\\)? The cross-dataset covariance matrix is\n\\[\n\\underset{n\\times m}{\\Sigma_{XY}} =\n\\begin{bmatrix}\n\\text{cov}(x_1,y_1) & \\text{cov}(x_1,y_2) & \\dots & \\text{cov}(x_1,y_m) \\\\\n\\text{cov}(x_2,y_1) & \\text{cov}(x_2,y_2) & \\dots & \\text{cov}(x_2,y_m) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{cov}(x_n,y_1) & \\text{cov}(x_n,y_2) & \\dots & \\text{cov}(x_n,y_m) \\\\\n\\end{bmatrix}\n\\]\nCanonical correlation analysis (CCA) seeks two vectors \\(a \\in \\mathbb R^n\\) and \\(b \\in \\mathbb R^m\\) such that \\(a'X\\) and \\(b'Y\\) maximize the correlation \\(\\rho =\\text{corr}(a'X, b'Y)\\). The random transformed random variable \\(U = a'X\\) and \\(V = b'Y\\) are the first pair of canonical variables. The second pair of canonical variables are orthogonal to the first pair and maximize the same correlation, and so on.\nSuppose we want to choose \\(a\\) and \\(b\\) to maximize\n\\[\n\\rho = \\frac{a'\\Sigma_{XY}b}{\\sqrt{a'\\Sigma_{XX}a}\\sqrt{b'\\Sigma_{YY}b}}\n\\]\nWe may impose the constraint such that \\(a'\\Sigma_{XX}a\\) and \\(b'\\Sigma_{YY}b\\) normalize to \\(1\\). Form the Lagrangian\n\\[\n\\mathcal{L} = a'\\Sigma_{XY}b - \\frac{\\mu}{2}(a'\\Sigma_{XX}a - 1) -\\frac{\\nu}{2}(b'\\Sigma_{YY}b -1)\n\\]\nThe first-order conditions are\n\\[\n\\begin{aligned}\n&\\frac{\\partial\\mathcal{L}}{\\partial a} = \\Sigma_{XY}b -\\mu\\ \\Sigma_{XX}a = 0 \\\\\n&\\frac{\\partial\\mathcal{L}}{\\partial b} = \\Sigma_{YX}a -\\nu\\ \\Sigma_{YY}b = 0 \\\\\n\\end{aligned}\n\\]\nwhich implies\n\\[\n\\begin{aligned}\n\\Sigma_{XX}^{-1}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX} a &= \\mu\\nu a =\\lambda a \\\\[1em]\n\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1}\\Sigma_{XY} b &= \\mu\\nu b =\\lambda b\n\\end{aligned}\n\\]\nTherefore, \\(a\\) is the eigenvector of \\(\\Sigma_{XX}^{-1}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\). The associated eigenvalue \\(\\lambda\\) is equivalent to the maximized correlation squared \\(\\hat\\rho^2\\). To see this, just multiply the first-order condition by \\(a'\\):\n\\[\na'\\Sigma_{XY}b = \\mu\\cdot a'\\Sigma_{XX}a=\\mu=\\rho^*\n\\]\nSymmetrically, \\(b\\) is the eigenvector of \\(\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1}\\Sigma_{XY}\\).\nSo the canonical correlation can be computed as follows:\n\nCompute the eigenvalues and eigenvectors of \\[\\Sigma_{XX}^{-1}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\]\nSort the eigenvalues as \\(\\lambda_1 \\geq \\lambda_2 \\geq\\dots\\geq \\lambda_n\\) and \\(a_1,a_2,\\dots,a_n\\) are the corresponding eigenvectors such that \\(a'\\Sigma_{XX}a=1\\).\nCompute the eigenvalues and eigenvectors of \\[\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1}\\Sigma_{XY}\\]\nSort the eigenvalues as \\(\\lambda_1 \\geq \\lambda_2 \\geq\\dots\\geq \\lambda_m\\) and \\(b_1,b_2,\\dots,b_m\\) are the corresponding eigenvectors such that \\(b'\\Sigma_{XX}b=1\\).\n\\((a_k,b_k)\\) is the \\(k\\)-th pair of canonical variables, where \\(k\\leq\\min\\{m,n\\}\\); and \\(\\gamma_k\\) is the \\(k\\)-th largest canonical correlation.\n\nNote that, if \\(\\underset{n \\times k}{A} = [a_1\\ a_2\\ \\dots a_k]\\), \\(\\underset{m \\times k}{B} = [b_1\\ b_2\\ \\dots b_k]\\), we would have \\(A'\\Sigma_{XY}B = \\Lambda\\) where\n\\[\n\\Lambda = \\begin{bmatrix}\n\\rho_1 & &  \\\\\n& \\rho_2 &  \\\\\n& & \\ddots &  \\\\\n& & & \\rho_k\n\\end{bmatrix}\n\\]\nis a diagonal matrix. Therefore, canonical variables transform the covariance matrix between \\(X\\) and \\(Y\\) into a diagonal matrix, where entries on the diagonal best summarize the correlations between the two datasets."
  },
  {
    "objectID": "33_vecm.html#johansens-procedure",
    "href": "33_vecm.html#johansens-procedure",
    "title": "33  VECM*",
    "section": "33.3 Johansen’s Procedure",
    "text": "33.3 Johansen’s Procedure\n\nEstimate by OLS two regressions \\[\n\\begin{aligned}\n\\Delta y_t &= \\hat\\Psi_0 + \\hat\\Psi_1\\Delta y_{t-1} + \\dots +\\hat\\Psi_{p-1}\\Delta y_{t-p+1} +\\hat u_t \\\\\ny_{t-1} &= \\hat\\Theta_0 + \\hat\\Theta_1\\Delta y_{t-1} + \\dots +\\hat\\Theta_{p-1}\\Delta y_{t-p+1} +\\hat v_t \\\\\n\\end{aligned}\n\\] Save \\(\\hat u_t\\) and \\(\\hat v_t\\).\nCompute the canonical correlations between \\(\\hat u_t\\) and \\(\\hat v_t\\). Find the eigenvalues of \\(\\Sigma_{vv}^{-1}\\Sigma_{vu}\\Sigma_{uu}^{-1}\\Sigma_{uv}\\). Sort them from the largest to the smallest: \\(\\hat\\lambda_1 \\geq \\hat\\lambda_2 \\geq \\dots\\geq \\hat\\lambda_n\\). Then the maximum log-likelihood function subject to the constraint that there are \\(h\\) cointegrating relations is given by \\[\n\\ell^*=-\\frac{Tn}{2}\\log(2\\pi)-\\frac{Tn}{2}-\\frac{T}{2}\\log|\\hat\\Sigma_{uu}|\n-\\frac{T}{2}\\sum_{i=1}^{h}\\log(1-\\hat\\lambda_i)\n\\] The test for there being \\(h\\) cointegrating relations is equivalent to testing \\[\n-\\frac{T}{2}\\sum_{i=h+1}^{n}\\log(1-\\hat\\lambda_i) = 0.\n\\]\nCalculate the MLE of the parameters. The cointegrating matrix is given by \\[\\hat B = \\begin{bmatrix}\\hat b_1 & \\hat b_2 & \\dots & \\hat b_h\\end{bmatrix}\\] where \\(\\hat b_i\\) are the eigenvectors used to normalize \\(v_t\\). The adjustment matrix and other parameters are given by \\[\n\\begin{aligned}\n\\hat A &= \\hat\\Sigma_{uv}\\hat B\\\\\n\\hat\\Pi &= \\hat A\\hat B' \\\\\n\\hat\\Gamma_i &= \\hat\\Psi_i - \\hat\\Pi\\hat\\Theta_i\\\\\n\\mu &= \\hat\\Psi_0 - \\hat\\Pi\\hat\\Theta_0.\n\\end{aligned}\n\\]\n\nTo understand this procedure, note that if we treat \\(\\Pi\\) as given, the MLE for Equation 33.1 is equivalent to estimating the coefficients by OLS:\n\\[\n\\Delta y_t - \\Pi y_{t-1} = \\hat\\Gamma_1\\Delta y_{t-1} + \\hat\\Gamma_2\\Delta y_{t-2} +\n\\dots + \\hat\\Gamma_{p-1}\\Delta y_{t-p+1} + \\hat\\mu+ \\hat\\epsilon_t\n\\]\nThe log-likelihood function becomes\n\\[\n\\ell(\\Pi, \\Omega) = -\\frac{T}{2}\\log |2\\pi\\Omega| - \\frac{1}{2}\\sum_{t=1}^{T}\n(\\hat\\epsilon_t'\\Omega^{-1}\\hat\\epsilon_t).\n\\]\nStep 1 does this in two separate regressions, in which\n\\[\n\\begin{aligned}\n\\hat\\Gamma_i &= \\hat\\Psi_i - \\Pi\\hat\\Theta_i \\\\\n\\hat\\epsilon_t &= \\hat u_t - \\Pi\\hat v_t \\\\\n\\end{aligned}\n\\]\nThus, the log-likelihood can be rewritten as\n\\[\n\\ell(\\Pi, \\Omega) = -\\frac{T}{2}\\log |2\\pi\\Omega| - \\frac{1}{2}\\sum_{t=1}^{T}\n[(\\hat u_t - \\Pi\\hat v_t)'\\Omega^{-1} (\\hat u_t - \\Pi \\hat v_t)]\n\\]\nFurther concentrating \\(\\Omega\\):\n\\[\n\\hat\\Omega = \\frac{1}{T}\\sum_{t=1}^{T}[(\\hat u_t - \\Pi\\hat v_t)(\\hat u_t - \\Pi \\hat v_t)']\n\\]\nSubstituting this into the \\(\\ell\\) function\n\\[\n\\ell(\\Pi) = -\\frac{Tn}{2}\\log (2\\pi) - \\frac{Tn}{2}- \\frac{T}{2}\\log\n\\left|\\frac{1}{T}\\sum_{t=1}^{T}[(\\hat u_t - \\Pi\\hat v_t)(\\hat u_t - \\Pi \\hat v_t)']\\right|\n\\]\nThus, maximizing \\(\\ell\\) is equivalent to minimizing\n\\[\n\\left|\\frac{1}{T}\\sum_{t=1}^{T}[(\\hat u_t - \\Pi\\hat v_t)(\\hat u_t - \\Pi \\hat v_t)']\\right|\n\\]\nby choosing \\(\\Pi\\). If \\(u_t\\) is a single variable, the optimal \\(\\hat\\Pi\\) that minimizes \\(|T^{-1}\\sum_t (u_t-\\Pi v_t)^2|\\) would be simply the OLS estimator. Similarly, for the vector case, we have\n\\[\n\\hat\\Pi = \\left(\\frac{1}{T}\\sum_t u_t v_t'\\right)\\left(\\frac{1}{T}\\sum_t v_t v_t'\\right)^{-1}\n\\]\nSuppose we have the canonical decomposition \\(A'\\Sigma_{uv}B = \\Lambda\\) where \\(\\Lambda\\) is a diagonal matrix of canonical correlations, and \\(A'\\Sigma_{uu}A=I\\), \\(B'\\Sigma_{vv}B=I\\). The estimator can be reduced to\n\\[\n\\begin{aligned}\n\\hat\\Pi &= (A'^{-1}\\Lambda B^{-1})(B'^{-1}B^{-1})^{-1} = A'^{-1}\\Lambda B' \\\\[1em]\n&= A'^{-1}\\begin{bmatrix}r_1  \\\\ & r_2 \\\\ & & \\ddots \\\\ & & & r_n\\end{bmatrix} B'\n\\end{aligned}\n\\]\nThe minimized ‘squared residuals’ is\n\\[\n\\begin{aligned}\n\\left|\\frac{1}{T}\\sum_{t=1}^{T}[(\\hat u_t - \\Pi\\hat v_t)(\\hat u_t - \\Pi \\hat v_t)']\\right|\n&= \\left| A'^{-1} \\frac{1}{T}\\sum_{t=1}^{T}(A'u_t - \\Lambda B'v_t)(A'u_t - \\Lambda B'v_t)' A^{-1} \\right| \\\\[1em]\n&= |A|^{-2} |I-\\Lambda\\Lambda'| \\\\[1em]\n&= |A|^{-2} \\begin{vmatrix}1-r_1^2 \\\\ & 1-r_2^2 \\\\ & & \\ddots \\\\ & & & 1-r_n^2\\end{vmatrix} \\\\[1em]\n&= |A|^{-2} \\prod_{i=1}^{n} (1-\\lambda_i).\n\\end{aligned}\n\\]\nThis explains the likelihood function in Step 2. The MLE for \\(\\hat\\Pi\\) above is subject to no constraint. However, if there is any cointegrating relations, \\(\\Pi\\) cannot be full rank. If we restrict the rank of \\(\\Pi\\) to \\(h\\), the minimized squared residuals is achieved by picking the \\(h\\) largest \\(\\lambda\\)s."
  },
  {
    "objectID": "33_vecm.html#hypothesis-testing",
    "href": "33_vecm.html#hypothesis-testing",
    "title": "33  VECM*",
    "section": "33.4 Hypothesis Testing",
    "text": "33.4 Hypothesis Testing\n\nTest 1: At most h cointegrations\nHypothesis:\n\\[\n\\begin{aligned}\n&H_0: \\text{there are no more than }h\\text{ cointegrating relations}\\\\\n&H_1: \\text{there are more than }h\\text{ cointegrating relations}\\\\\n\\end{aligned}\n\\]\nTest statistics:\n\\[\n\\lambda_{\\text{trace}} = 2(\\ell_1^* - \\ell_0^*) = -T\\sum_{i=h+1}^{n}\\log(1-\\hat\\lambda_i)\n\\]\nIf \\(H_0\\) is true, \\(\\lambda_{\\text{trace}}\\) should be close to zero. The critical values are provided by the table below. Case 1 means there is no constant or deterministic trend; Case 2 contains constants in cointegrating vectors but no deterministic trend; Case 3 contains deterministic trend.\n\nCritical values of Johansen’s likelihood ratio test of the null hypothesis of \\(h\\) integrating relations against the alternative of no restrictions\n\n\n\\(n-h\\)\n\\(T\\)\n0.1\n0.05\n0.025\n0.001\n\n\n\n\nCase 1\n\n\n\n\n\n\n\n1\n400\n2.86\n3.84\n4.93\n6.51\n\n\n2\n400\n10.47\n12.53\n14.43\n16.31\n\n\n…\n\n\n\n\n\n\n\nCase 2\n\n\n\n\n\n\n\n1\n400\n6.69\n8.08\n9.66\n11.58\n\n\n2\n400\n15.58\n17.84\n19.61\n21.96\n\n\n…\n\n\n\n\n\n\n\nCase 3\n\n\n\n\n\n\n\n1\n400\n2.82\n3.96\n5.33\n6.94\n\n\n2\n400\n13.34\n15.20\n17.30\n19.31\n\n\n…\n\n\n\n\n\n\n\n\n\n\nTest 2: h cointegrations vs h+1\nHypothesis:\n\\[\n\\begin{aligned}\n&H_0: \\text{there are }h\\text{ cointegrating relations}\\\\\n&H_1: \\text{there are }h+1\\text{ cointegrating relations}\\\\\n\\end{aligned}\n\\]\nTest statistics:\n\\[\n\\lambda_{\\text{max}} = 2(\\ell_1^* -\\ell_0^*)=-T\\log(1-\\hat\\lambda_{h+1})\n\\]\nThe critical values are given as below.\n\nCritical values of Johansen’s likelihood ratio test of the null hypothesis of \\(h\\) integrating relations against the alternative of \\(h+1\\) relations\n\n\n\\(n-h\\)\n\\(T\\)\n0.1\n0.05\n0.025\n0.001\n\n\n\n\nCase 1\n\n\n\n\n\n\n\n1\n400\n2.86\n3.84\n4.96\n6.51\n\n\n2\n400\n9.52\n11.44\n13.27\n15.69\n\n\n…\n\n\n\n\n\n\n\nCase 2\n\n\n\n\n\n\n\n1\n400\n6.69\n8.08\n9.66\n11.58\n\n\n2\n400\n12.78\n14.60\n16.40\n18.78\n\n\n…\n\n\n\n\n\n\n\nCase 3\n\n\n\n\n\n\n\n1\n400\n2.82\n3.96\n5.33\n6.94\n\n\n2\n400\n12.10\n14.04\n15.81\n17.94\n\n\n…"
  },
  {
    "objectID": "34_bayes.html#the-sunrise-problem",
    "href": "34_bayes.html#the-sunrise-problem",
    "title": "34  Intro to Bayes",
    "section": "34.1 The Sunrise Problem",
    "text": "34.1 The Sunrise Problem\nQuestion: What is the probability that the sun will rise tomorrow?\nWe do not consider the physics here. Suppose we want to answer this question by purely statistics. What we need to do is to observe how many days the sun had risen in the past, and make some inference about the future. If we had collect the data on the past \\(n\\) days, we would have observed the sun had risen everyday for sure (the sun rises even in cloudy or rainy days). If we want to calculate the probability of a sunrise event, denoted by \\(A\\), the frequentist approach would give \\(P(A) = \\frac{n}{n} = 1\\). The probability is always \\(1\\) no matter how many observations we have. That’s a bit quirky, even though we haven’t looked at the confidence interval. The 100 percent probability does not sound correct, as nothing can be so certain.\nLet’s have a look at how Laplace in the 18th century solves this problem. Let \\(x_t\\) be an random variable such that\n\\[\nx_t = \\begin{cases}\n1, \\quad\\text{the sun rise in day } t \\text{ with probability }\\theta\\\\\n0, \\quad\\text{otherwise}\n\\end{cases}\n\\]\nIn other words, \\(x_t\\) follows a Bernoulli distribution \\(x_t\\sim\\text{Bern}(\\theta)\\). There is an unknown parameter \\(\\theta\\), which is our goal to estimate. Before we have observed any data, we have no knowledge about this \\(\\theta\\). We assume it is distributed uniformly, \\(\\theta\\sim\\text{Unif}(0,1)\\). That is, it can be any value between \\(0\\) and \\(1\\).\nSuppose we have observed the data for \\(n\\) days: \\(x_1,x_2,\\dots,x_n\\). Assume these events are \\(i.i.d\\). Define \\(S_n\\) as the total number of sunrises that had happened:\n\\[\nS_n = x_1 + x_2 + \\dots + x_n\n\\]\nWe know \\(S_n\\) follows a Binomial distribution \\(S_n\\sim\\text{Bin}(n,\\theta)\\), with the probability mass function\n\\[\nP(S_n = k | \\theta) = \\binom{n}{k}\\theta^k (1-\\theta)^{n-k}\n\\]\nOur goal is to find: \\(\\theta | S_n=?\\) An estimation of the probability of a sunrise after observing the data.\nRecall that the Bayesian rule allows us to invert the conditional probability:\n\\[\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\\]\nUsing this formula, we have\n\\[\n\\begin{aligned}\nP(\\theta|S_n=k) &=\\frac{P(S_n=k|\\theta) P(\\theta)}{P(S_n=k)}=\n\\frac{P(S_n=k|\\theta) P(\\theta)}{\\int_0^1 P(S_n=k|\\theta) P(\\theta)d\\theta}\\\\[1em]\n&= \\frac{\\binom{n}{k}\\theta^k (1-\\theta)^{n-k}\\cdot\\mathbb{I}(0\\leq\\theta\\leq 1)}\n{\\int_0^1\\binom{n}{k}\\theta^k (1-\\theta)^{n-k}\\cdot\\mathbb{I}(0\\leq\\theta\\leq 1)d\\theta}\\\\[1em]\n&= \\begin{cases}\\frac{\\binom{n}{k}\\theta^k (1-\\theta)^{n-k}}\n{\\int_0^1\\binom{n}{k}\\theta^k (1-\\theta)^{n-k}d\\theta}&\\quad\\text{if}\\ 0\\leq\\theta\\leq 1\\\\\n0&\\quad\\text{otherwise}\n\\end{cases}\n\\end{aligned}\n\\]\nIf \\(k=n\\), we have\n\\[\nP(\\theta|S_n=n) = \\frac{\\theta^n}{\\int_0^1\\theta^n d\\theta}=(n+1)\\theta^n\n\\]\nfor \\(0\\leq\\theta\\leq 1\\). Now we are ready to calculate the probability of the sun rising tomorrow after observing \\(n\\) sunrises:\n\\[\n\\begin{aligned}\nP(x_{n+1}=1|S_n=n) &= \\int_0^1 P(x_{n+1}=1|\\theta)P(\\theta|S_n=n)d\\theta \\\\\n&=\\int_0^1 \\theta\\cdot(n+1)\\theta^n d\\theta \\\\\n&= \\frac{n+1}{n+2}\n\\end{aligned}\n\\]\nAs \\(n\\to\\infty\\), the probability approaches \\(1\\). Personally, I think this is much more reasonable answer than the frequentist approach, in which you always get probability \\(1\\)."
  },
  {
    "objectID": "34_bayes.html#the-bayesian-approach",
    "href": "34_bayes.html#the-bayesian-approach",
    "title": "34  Intro to Bayes",
    "section": "34.2 The Bayesian Approach",
    "text": "34.2 The Bayesian Approach\nThis illustration literally shows every tenet of the Bayesian approach. We start with an prior distribution about an unknown parameter \\(\\theta\\). In the previous example, we model it as a uniform distribution because of our ignorance. But the prior can be any distribution reflecting our subjective belief about the parameter before we see the data. Note how this contrasts with the frequentist approach, in which the parameter is a fixed unknown number.\nThe principle of Bayesian analysis is then to combine the prior information with the information contained in the data to obtain an updated distribution accounting for both sources of information, known as the posterior distribution. This is done by using the Bayes rule:\n\\[\n\\underbrace{p(\\theta|X)}_{\\text{posterior}} = \\frac{\\overbrace{p(X|\\theta)}^{\\text{likelihood}} \\times\n\\overbrace{p(\\theta)}^{\\text{prior}}}\n{\\underbrace{p(X)}_{\\text{normalizing scalar}}}\n\\]\nThe posterior distribution \\(p(\\theta|X)\\) is the central object for Bayesian inference as it combines all the information we have about \\(\\theta\\). Note that \\(p(\\theta|X)\\) is a function of \\(\\theta\\). Since the denominator \\(p(X)\\) is independent of \\(\\theta\\), it only plays the role of a normalizing constant to ensure the posterior is a valid probability density function that integrates to \\(1\\). It is therefore convenient to ignore it and rewrite the posterior as\n\\[\np(\\theta|X) \\propto p(X|\\theta)p(\\theta)\n\\]\nOne difficulty of Bayesian inference is that the denominator \\(p(X)\\) is often impossible to compute, especially for high dimensional parameters:\n\\[\np(X) = \\int_{\\theta_1}\\int_{\\theta_2}\\dots\\int_{\\theta_n} p(X,\\theta_1,\\theta_2,\\dots,\\theta_n)d\\theta_1d\\theta_2\\dots d\\theta_n\n\\]\nBut the relative frequencies of parameter values are easy to compute\n\\[\n\\frac{p(\\theta_A|X)}{p(\\theta_B|X)}=\\frac{p(X|\\theta_A)p(\\theta_A)}{p(X|\\theta_B)p(\\theta_B)}\n\\]\nThis allows us to sample from the posterior distribution even the \\(pdf\\) of the distribution is unknown. We will return to this point when we discuss computational Bayesian methods.\nThe relative weight of the prior versus the data in determining the posterior depends on (i) how strong the prior is, and (ii) how many data we have. If the prior is so strong (very small variance / uncertainty) that seeing the data will not change our beliefs, the posterior would be mostly determined by the prior. On the contrary, if the data is so abundant that the evidence overwhelms any prior belief, the impact of prior would be negligible."
  },
  {
    "objectID": "34_bayes.html#frequentist-vs-bayesian",
    "href": "34_bayes.html#frequentist-vs-bayesian",
    "title": "34  Intro to Bayes",
    "section": "34.3 Frequentist vs Bayesian",
    "text": "34.3 Frequentist vs Bayesian\nFrequentists and Bayesians hold different philosophy about statistics. Frequentists view our sample as the result of one of an infinite number of exactly repeated experiments. The data are randomly sampled from a fixed population distribution. The unknown parameters are properties of the population, and therefore are fixed. The purpose of statistics is to make inference about the population parameters (the ultimate truth) with limited samples. The uncertainty associated with this process arises from sampling. Because we do not have the entire population, each sample only tells partial truth about the population. Therefore our inference about the parameters can never be perfect due to sampling errors. Frequentists conduct hypothesis tests assuming a hypothesis (about the population parameter) is true and calculating the probability of obtaining the observed sample data.\nIn Bayesians’ world view, probability is an expression of subjective beliefs (a measure of certainty in a belief), which can be updated in light of new data. Parameters are probabilistic rather than fixed, which reflects the uncertainties about the parameters. The essence of Bayesian inference is to update the probability of a ‘hypothesis’ given the data we have obtained. The Bayes’ rule is all we need. All information is summarized in the posterior probability and there is not need for explicit hypothesis testing.\n\n\n\n\n\n\n\nFrequentist\nBayesian\n\n\n\n\nProbability is the limit of frequency\nProbability is uncertainty\n\n\nParameters are fixed unknown numbers\nParameters are random variables\n\n\nData is a random sample from the population\nData is fixed/given\n\n\nLLN/CLT\nBayes’ rule\n\n\n\nIn time series analysis, there are good reasons to be Bayesian. Perhaps the frequentist perspective makes sense in a cross section, where it is intuitive to image taking different samples from the population. However, in time series we have only one realization. It is difficult to imagine where we would obtain another sample. It is more natural to take a Bayesian perspective. For example, we have some prior belief on how inflation and unemployment might be related (the Phillips curve), then we update our belief with data.\nFrequentists often criticize Bayesians’ priors as entirely subjective. Bayesians would respond that frequentists also have prior assumptions that they are not even aware of. Frequentist inference utilizes the LLN and CLT, which inevitably assumes the speed of convergence. In settings like VAR models, where there are a large number of parameters to estimate but only a limited amount of observations. Are the asymptotically properties really plausible? Bayesians believe it would be better to make our assumptions explicit.\nApart from the philosophical difference, in practice Frequentists and Bayesians might well give similar results (though the results should be interpreted differently). After all, if the data is plenty, the influence of priors would diminish to zero."
  },
  {
    "objectID": "35_blm.html#linear-regression-with-known-sigma2",
    "href": "35_blm.html#linear-regression-with-known-sigma2",
    "title": "35  Linear Model",
    "section": "35.1 Linear Regression with Known \\(\\sigma^2\\)",
    "text": "35.1 Linear Regression with Known \\(\\sigma^2\\)\nLet’s use the Bayesian principle to estimate a simple linear regression:\n\\[\ny_t = x_t \\beta + \\epsilon_t\n\\] where \\(\\epsilon_t\\sim N(0,\\sigma^2)\\). For simplicity, we assume \\(\\sigma^2\\) is known. So the only unknown parameter is \\(\\beta\\). Assume it has a Gaussian prior\n\\[\n\\beta \\sim N(\\beta_0, V_\\beta)\n\\]\nGaussian prior is a handy prior to express our belief about the mean and the degree of certainty of that belief (expressed by the variance).\nTraditional OLS works without specifying the distribution of \\(\\epsilon_t\\). However, for Bayesian inference to work, we always need to specify the full distribution of the model. With Gaussian errors, we have\n\\[\n(\\boldsymbol Y | \\beta) \\sim N(\\boldsymbol X\\beta, \\sigma^2\\boldsymbol I_T)\n\\]\nUnder the \\(i.i.d\\) assumption, the joint likelihood function is\n\\[\n\\begin{aligned}\np(\\boldsymbol Y|\\beta) &= \\prod_{t=1}^{T} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\ne^{-\\frac{1}{2\\sigma^2}(y_t - x_t\\beta)^2} \\\\[1em]\n&= (2\\pi\\sigma^2)^{-\\frac{T}{2}}\ne^{-\\frac{1}{2\\sigma^2}\\sum_t (y_t - x_t\\beta)^2}\n\\end{aligned}\n\\]\nBy the Bayes rule, the posterior distribution is\n\\[\n\\begin{aligned}\np(\\beta|\\boldsymbol Y) & \\propto p(\\boldsymbol Y|\\beta)p(\\beta) \\\\[1em]\n&\\propto e^{-\\frac{1}{2\\sigma^2}\\sum_t (y_t - x_t\\beta)^2}\n\\cdot e^{-\\frac{1}{2V_\\beta}(\\beta-\\beta_0)^2} \\\\[1em]\n&\\propto e^{-\\frac{1}{2}\\left(\\frac{\\sum x_t}{\\sigma^2} + \\frac{1}{V_\\beta}\\right)\n\\beta^2+ \\left(\\frac{\\sum x_ty_t}{\\sigma^2} + \\frac{\\beta_0}{V_\\beta}\\right)\\beta}\n\\end{aligned}\n\\]\nwhich is the kernel of a Gaussian distribution. Therefore,\n\\[\np(\\beta|\\boldsymbol Y) \\sim N(\\hat\\beta, D_\\beta) \\propto\ne^{-\\frac{1}{2D_\\beta}\\beta^2 + \\frac{\\hat\\beta}{D_\\beta}\\beta}\n\\]\nwhere\n\\[\nD_\\beta = \\left(\\frac{\\sum x_t}{\\sigma^2} + \\frac{1}{V_\\beta}\\right)^{-1},\\\n\\hat\\beta = \\left(\\frac{\\sum x_ty_t}{\\sigma^2} + \\frac{\\beta_0}{V_\\beta}\\right)D_\\beta.\n\\]\nNote that if we have a very loose prior \\(V_\\beta\\to\\infty\\), or abundant data \\(N\\to\\infty\\), we would have\n\\[\nD_\\beta = \\frac{\\sigma^2}{\\sum x_t},\\ \\hat\\beta = \\frac{\\sum x_ty_t}{\\sum x_t}\n\\]\nwhich is exactly the same as the OLS estimator.\nSo with a Gaussian prior and a Gaussian likelihood, the posterior distribution is also Gaussian. It is this particular choice of the prior and the likelihood function that the posterior has a closed-form solution. Not many prior choice has this property. This is what we called a conjugate prior."
  },
  {
    "objectID": "35_blm.html#linear-regression-with-unknown-sigma2",
    "href": "35_blm.html#linear-regression-with-unknown-sigma2",
    "title": "35  Linear Model",
    "section": "35.2 Linear Regression with Unknown \\(\\sigma^2\\)",
    "text": "35.2 Linear Regression with Unknown \\(\\sigma^2\\)\nFor simplicity, we have assumed the variance \\(\\sigma^2\\) is known. In reality, if \\(\\sigma^2\\) is unknown, we also need to assign it a prior distribution. A common choice is an inverse-Gamma distribution:\n\\[\n\\sigma^2 \\sim IG(\\nu_0, S_0)\n\\]\nwhose density function is given by\n\\[\np(\\sigma^2) = \\frac{S_0^{\\nu_0}}{\\Gamma(\\nu_0)}(\\sigma^2)^{-(\\nu_0+1)}e^{-\\frac{S_0}{\\sigma^2}}\n\\]\nOne reason of this choice is that an inverse-Gamma can never be negative. Another reason is that it is also a conjugate prior. It can be shown, with an inverse-Gamma as the prior for the variance and an Gaussian likelihood, the posterior for \\(\\sigma^2\\) is also an inverse-Gamma:\n\\[\n(\\sigma^2 | \\boldsymbol Y,\\beta) \\sim IG\\left(\\nu_0 + \\frac{T}{2},\nS_0 +\\frac{1}{2}\\sum_{t=0}^{T} (y_t-x_t\\beta)^2\\right).\n\\]"
  },
  {
    "objectID": "35_blm.html#credible-interval",
    "href": "35_blm.html#credible-interval",
    "title": "35  Linear Model",
    "section": "35.3 Credible Interval",
    "text": "35.3 Credible Interval\nOnce the posterior distribution is obtained, the question becomes how to report and interpret the results. Similar to conventional OLS results, we would like to report the mean or median of the parameter, and the associated “credible interval”. The credible interval is directly obtained from the distribution:\n\\[\nP(\\beta_L\\leq\\beta\\leq\\beta_U) = \\alpha\n\\]\nwhich indicate that \\(\\beta\\) falls between the range \\([\\beta_L, \\beta_U]\\) with a probability \\(\\alpha\\). In a frequentist approach, a \\(p\\)-value is not the probability of the parameter, nor does confidence interval represent the distribution of the parameter. However, the credible interval obtained from a Bayesian posterior is the probability for particular values of the parameter. It is more straightforward to interpret. After all, parameters are themselves probabilistic in a Bayesian world."
  },
  {
    "objectID": "36_bvar.html#vectorized-form",
    "href": "36_bvar.html#vectorized-form",
    "title": "36  Bayesian VAR",
    "section": "36.1 Vectorized Form",
    "text": "36.1 Vectorized Form\nFor Bayesian inference, it is easier to work with the vectorized form of a VAR in Chapter 27:\n\\[\ny = \\bar{X}\\beta + u\n\\]\nwhere \\(y = vec(Y)\\), \\(\\bar{X} = I_n \\otimes X\\), \\(\\beta = vec(B)\\), \\(u = vec(U)\\), and \\(\\bar\\Sigma = \\Sigma\\otimes I_T\\). For a VAR with \\(n\\) variables and \\(p\\) lags, the vectorized form looks like\n\\[\n\\begin{bmatrix}\ny_{1,1}\\\\\\vdots\\\\y_{1,T}\\\\\\vdots\\\\\ny_{n,1}\\\\\\vdots\\\\y_{n,T}\n\\end{bmatrix} =\n\\begin{bmatrix}\ny_0' & \\dots & y_{1-p}' & & 0 & \\dots & 0 \\\\\n\\vdots &\\ddots &\\vdots & &\\vdots & &\\vdots\\\\\ny_{T-1}'&\\dots &y_{T-p}'& & 0 & \\dots & 0 \\\\\n0 &\\dots & 0 & \\ddots & y_0' & \\dots &y_{1-p}'\\\\\n\\vdots& & \\vdots & & \\vdots & \\ddots &\\vdots \\\\\n0 &\\dots & 0 & & y_{T-1}' & \\dots & y_{T-p}'\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nA_1^{(1)} \\\\\\vdots\\\\A_p^{(1)}\\\\\\vdots\\\\\nA_1^{(n)} \\\\\\vdots\\\\A_p^{(n)}\n\\end{bmatrix} +\n\\begin{bmatrix}\nu_{1,1}\\\\\\vdots\\\\u_{1,T}\\\\\\vdots\\\\\nu_{n,1}\\\\\\vdots\\\\u_{n,T}\n\\end{bmatrix}\n\\]\nAssume multivariate Gaussian distribution for the residuals \\(u\\sim N(0, \\bar\\Sigma)\\), the likelihood for \\(y\\) is given by\n\\[\np(y|\\beta) = |2\\pi\\bar\\Sigma|^{-1/2}\\exp\\left[-\\frac{1}{2}(y-\\bar{X}\\beta)'\n\\bar\\Sigma^{-1}(y-\\bar{X}\\beta)\\right]\n\\] Also assume that \\(\\beta\\) has a multivariate Gaussian prior\n\\[\n\\beta \\sim N(\\beta_0, \\Omega_0)\n\\]\nThe key is to specify \\(\\beta_0\\) and \\(\\Omega_0\\). We now introduce one of the simplest and yet the most popular prior setting for VAR models."
  },
  {
    "objectID": "36_bvar.html#minnesota-prior",
    "href": "36_bvar.html#minnesota-prior",
    "title": "36  Bayesian VAR",
    "section": "36.2 Minnesota Prior",
    "text": "36.2 Minnesota Prior\nThe Minnesota prior is proposed by Litterman (1986). It is assumed that the VAR residual covariance matrix \\(\\Sigma\\) is known. The only prior required is for parameters \\(\\beta\\). The essence of the Minnesota prior is to shrink the parameters of longer lags to zero.\nThe prior setting for \\(\\beta_0\\) is as following: as most observed macroeconomic variables seem to be characterized by a unit root, our prior belief should be that each endogenous variable included in the model presents a unit root in its first own lags, and the coefficients equal to zero for further lags and cross-variable coefficients. Therefore,\n\\[\n\\mathbb{E}[A_k^{(ij)}] = \\begin{cases}\n1, &\\text{if } i=j, k=1\\\\\n0, &\\text{otherwise}\n\\end{cases}\n\\]\nFor stationary variables, the coefficient \\(1\\) can be replaced by, say, \\(0.8\\). Regarding the uncertainty of our belief, expressed in \\(\\Omega_0\\), it is assumed that no covariance exists between terms in \\(\\beta\\) so that \\(\\Omega_0\\) is diagonal. Furthermore, our prior shall become stronger (variance becomes smaller) for longer lags that they are closer to zero (i.e. shrinking longer lags to zero). Besides, correlations with lags on other variables are likely weaker than the correlations on their own lags (stronger shrinkage on coefficients relating to other variables).\n\\[\n\\text{Var}[A_k^{(ij)}]=\\lambda_1\\lambda_2^{\\mathbb{1}(i\\neq j)}\\frac{1}{k^{\\lambda_3}}\\frac{\\sigma_i^2}{\\sigma_j^2}\n\\]\nwhere \\(\\lambda_1\\) controls the overall tightness, \\(\\lambda_2\\) controls the tightness on cross-variable coefficients, \\(\\lambda_3\\) controls the speed at which coefficients on longer lags shrink to zero. \\(\\sigma_i^2\\) and \\(\\sigma_j^2\\) denote the OLS residual variance of the auto-regressive models estimated for variables \\(i\\) and \\(j\\).\nFinally, if any exogenous variables are included in the model, they should have priors centered at zero with large variance, as little is known about exogenous variables. A typical set of values for these hype-parameters found in the literature are: \\(\\lambda_1=0.1\\), \\(\\lambda_2=0.5\\), \\(\\lambda_3=1\\text{ or } 2\\), \\(\\lambda_4\\) for exogenous variables should be greater than \\(100\\).\nSince the Minnesota prior assumes \\(\\Sigma\\) is known, one has to obtain it beforehand. One method is to set the diagonal of \\(\\Sigma\\) equal to the residual variance of individual AR models run on each variable in the VAR. Alternatively, one can use the variance-covariance matrix of the VAR estimated by OLS."
  },
  {
    "objectID": "36_bvar.html#the-posterior",
    "href": "36_bvar.html#the-posterior",
    "title": "36  Bayesian VAR",
    "section": "36.3 The Posterior",
    "text": "36.3 The Posterior\nOnce the prior is determined, we can derive the posterior as follows\n\\[\n\\begin{aligned}\np(\\beta | y) &\\propto p(y|\\beta)p(\\beta) \\\\[1em]\n&\\propto\\exp\\left[-\\frac{1}{2}(y-\\bar{X}\\beta)'\\bar{\\Sigma}^{-1}(y-\\bar{X}\\beta)\\right]\n\\times\\exp\\left[-\\frac{1}{2}(\\beta-\\beta_0)'\\Omega_0^{-1}(\\beta-\\beta_0)\\right]\n\\end{aligned}\n\\]\nAfter some manipulation, it can be shown that\n\\[\np(\\beta|y)\\propto\\exp\\left[-\\frac{1}{2}(\\beta-\\bar\\beta)'\\bar\\Omega^{-1}(\\beta-\\bar\\beta)\\right]\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\bar\\Omega &= [\\Omega_0^{-1} + \\Sigma^{-1}\\otimes X'X]^{-1} \\\\[1em]\n\\bar\\beta &= \\bar\\Omega[\\Omega_0^{-1}\\beta_0 + (\\Sigma^{-1}\\otimes X')y]\n\\end{aligned}\n\\]\nThis is again the kernel of a multivariate Gaussian distribution. Therefore, the posterior distribution of \\(\\beta\\) is characterized by\n\\[\np(\\beta|y) \\sim N(\\bar\\beta, \\bar\\Omega).\n\\]\nOnce an estimate for \\(\\beta\\) is obtained, one can compute the IRF or FEVD accordingly. Typically in a Bayesian procedure, with one draw of \\(\\beta^{(1)}\\) from the posterior distribution, we compute one round of \\(\\text{IRF}^{(1)}\\); with a second draw of \\(\\beta^{(2)}\\), we compute another round of \\(\\text{IRF}^{(2)}\\), and so on. With a collection of IRFs, we can get the median and the credible bands for the IRF."
  },
  {
    "objectID": "37_cp.html",
    "href": "37_cp.html",
    "title": "37  Conjugate Priors",
    "section": "",
    "text": "We have mentioned, many of the times, we do not have closed-form solution for the posterior. However, there is a class of models — pairs of likelihoods and priors — that an analytic posterior exists. These pairs of likelihoods and pairs are referred as conjugate.\n\nSome common conjugate pairs\n\n\n\n\n\n\n\nLikelihood\nPrior\nPosterior\n\n\n\n\nBernoulli\n\\(\\text{Beta}(\\alpha,\\beta)\\)\n\\(\\text{Beta}(\\alpha+\\sum x_i,\\beta+n-\\sum x_i)\\)\n\n\nBinomial\n\\(\\text{Beta}(\\alpha,\\beta)\\)\n\\(\\text{Beta}(\\alpha+\\sum x_i,\\beta+\\sum n_i-\\sum x_i)\\)\n\n\nMultinomial\n\\(\\text{Dirichlet}(\\alpha)\\)\n\\(\\text{Dirichlet}(\\alpha+\\sum x_i)\\)\n\n\nNormal (known \\(\\sigma^2\\))\n\\(N(\\mu_0, \\sigma_0^2)\\)\n\\(N\\left(\\frac{1}{\\frac{1}{\\sigma_0^2}+\\frac{n}{\\sigma^2}}\\left(\\frac{\\mu_0}{\\sigma_0^2}+\\frac{\\sum x_i}{\\sigma^2}\\right),\\left(\\frac{1}{\\sigma_0^2}+\\frac{n}{\\sigma^2}\\right)^{-1}\\right)\\)\n\n\nNormal (known \\(\\mu\\))\n\\(IG(\\alpha,\\beta)\\)\n\\(IG \\left(\\frac{\\alpha+n}{2}, \\frac{\\beta+\\sum(x_i-\\mu)^2}{2}\\right)\\)\n\n\nPossion\n\\(\\Gamma(\\alpha,\\beta)\\)\n\\(\\Gamma(\\alpha+\\sum x_i,\\beta+n)\\)\n\n\n\nUsing conjugate priors, we can plug-in the data into the formula and get the exact posterior distribution. But the limitation is obvious. We are confined to use a given set of distributions, whereas other distributions do not have conjugate properties."
  },
  {
    "objectID": "38_gib.html#computational-bayes",
    "href": "38_gib.html#computational-bayes",
    "title": "38  Gibbs Sampling",
    "section": "38.1 Computational Bayes",
    "text": "38.1 Computational Bayes\nIf we move away from conjugate priors, we are likely to have no closed-form solution for the posterior distribution. But we can approximate the posterior distribution by some computational algorithms. The class of these algorithms are famously known as Markov chain Monte Carlo (MCMC) methods. That is, one can construct a Markov chain that has the desired distribution as its equilibrium distribution.\nOne strategy to approximate the shape of a distribution is through random sampling. If we can draw random samples from a distribution, as the sample size grows, sampling frequency will approach the probability density. Properties of a distribution can also be estimated from finite samples, e.g. \\(\\frac{1}{n}\\sum_{i=1}^{n} x_i\\to\\mathbb{E}(x)\\).\nThe problem is how to draw samples from a distribution without knowing its PDF. It seems to be an impossible task. But what if the conditional PDF is easier to compute?"
  },
  {
    "objectID": "38_gib.html#gibbs-sampler-for-linear-regression-models",
    "href": "38_gib.html#gibbs-sampler-for-linear-regression-models",
    "title": "38  Gibbs Sampling",
    "section": "38.2 Gibbs Sampler for Linear Regression Models",
    "text": "38.2 Gibbs Sampler for Linear Regression Models\nConsider the example in Chapter 35 with unknown variance. The joint distribution of \\(p(\\beta,\\sigma^2 | Y)\\) is hard to come by. But the condition distribution is easier to compute:\n\\[\n\\begin{aligned}\np(\\beta|\\boldsymbol Y, \\sigma^2) &\\sim N(\\hat\\beta, D_\\beta) \\\\\np(\\sigma^2 | \\boldsymbol Y,\\beta) &\\sim IG(\\bar\\nu, \\bar S)\n\\end{aligned}\n\\]\nSuppose we know how to draw random samples from the normal distribution and the inverse-Gamma distribution (in fact, if the CDF is known, a random sample can be generated by \\(y=\\text{CDF}^{-1}(x)\\), where \\(x \\sim U(0,1)\\)), we can generate samples \\(\\{(\\beta,\\sigma^2)\\}\\) by the following algorithm.\n\n\n\n\n\n\nAlgorithm (Gibbs Sampling)\n\n\n\nPick some initial value \\(\\beta^{(0)}=a_0\\) and \\(\\sigma^{2(0)} = b_0\\). Repeat the following steps for \\(i=1...N\\):\n\nDraw \\(\\sigma^{2(i)}\\sim p(\\sigma^2 | Y, \\beta^{(i-1)})\\) (Inverse-Gamma);\nDraw \\(\\beta^{(i)}\\sim p(\\beta| Y, \\sigma^{2(i)})\\) (Normal).\n\n\n\nThe most efficient sampling is direct independent sampling \\((\\beta,\\sigma^2)\\) from the posterior distribution. If this is not feasible, Gibbs sampler is a reasonable alternative. It can be imagined, in the stationary situation, sampling from the conditional distributions would be statistically identical to sampling from the joint probability distribution. However, Gibbs sampling can be highly inefficient if the posterior variables are highly correlated."
  },
  {
    "objectID": "38_gib.html#gibbs-sampler-for-general-models",
    "href": "38_gib.html#gibbs-sampler-for-general-models",
    "title": "38  Gibbs Sampling",
    "section": "38.3 Gibbs Sampler for General Models",
    "text": "38.3 Gibbs Sampler for General Models\nHere is the more general Gibbs algorithm. Suppose we have a model with \\(k\\) parameters \\(\\boldsymbol\\theta=(\\theta_1,\\theta_2,\\dots,\\theta_k)\\). Assume we can derive the exact expressions for each of the conditional distributions: \\(p(\\theta_i|\\theta_1,\\dots,\\theta_{i-1},\\theta_{i+1},\\dots,\\theta_k, Y)\\). Further assume we can generate independent samples from each of them. Then the Gibbs sampler runs as follows.\n\n\n\n\n\n\nAlgorithm (Gibbs Sampling)\n\n\n\nInitialize the algorithm with \\((\\theta_1^{(0)},\\theta_2^{(0)},\\dots,\\theta_k^{(0)})\\). Then repeat the following steps:\n\nChoose a random parameter ordering, e.g. \\((\\theta_3, \\theta_1,\\theta_2, …)\\). Denote the parameters with the new ordering \\((\\theta_1,\\theta_2,\\theta_3,…)\\).\nSample from the conditional distribution for each parameter using the most up-to-date parameters. That is, draw samples from the following distributions subsequently:\n\\[\n\\begin{aligned}\n& p(\\theta_1^{(i)} | \\theta_2^{(i-1)}, \\theta_3^{(i-1)},\\dots,\\theta_k^{(i-1)}, Y) \\\\\n& p(\\theta_2^{(i)} | \\theta_1^{(i)}, \\theta_3^{(i-1)},\\dots,\\theta_k^{(i-1)}, Y) \\\\\n& p(\\theta_3^{(i)} | \\theta_1^{(i)}, \\theta_2^{(i)},\\dots,\\theta_k^{(i-1)}, Y) \\\\\n& \\vdots \\\\\n& p(\\theta_k^{(i)} | \\theta_1^{(i)}, \\theta_2^{(i)},\\dots,\\theta_{k-1}^{(i)}, Y)\n\\end{aligned}\n\\]\n\nRepeat the process until the algorithm is reasonably converged."
  },
  {
    "objectID": "39_mh.html#dependent-sampling",
    "href": "39_mh.html#dependent-sampling",
    "title": "39  Metropolis–Hastings",
    "section": "39.1 Dependent Sampling",
    "text": "39.1 Dependent Sampling\nWe have mentioned, despite the posterior density is hard to compute, it is easy to determine the relative frequencies of the parameter values:\n\\[\n\\frac{p(\\theta_A|X)}{p(\\theta_B|X)}=\\frac{p(X|\\theta_A)p(\\theta_A)}{p(X|\\theta_B)p(\\theta_B)}\n\\tag{39.1}\\]\nTo generate a sample that approximate the posterior distribution, we require the values with higher probability density to be sampled proportionally more often.\nImagine we are exploring an unknown map. We start somewhere, each step forward depends on the current location. We require some areas (with high density) be explored more than others. This means the exploration moves cannot be purely “random walk”. We need some rules to dictate the “direction” of the next move. So that the exploration approximates the relative frequency suggested by Equation 39.1.\nNote that this exploration (sampling) algorithm necessarily has dependencies. The next sample value depends on the current sample value. This is called dependent sampling as opposed to independent sampling. Dependent sampling naturally takes more samples to reach a reasonable approximation of the posterior than independent sampling. Because of the dependency, each move gives less information than independent sampling."
  },
  {
    "objectID": "39_mh.html#random-walk-metropolis",
    "href": "39_mh.html#random-walk-metropolis",
    "title": "39  Metropolis–Hastings",
    "section": "39.2 Random Walk Metropolis",
    "text": "39.2 Random Walk Metropolis\nWe now introduce one of the most famous MCMC algorithm:\n\n\n\n\n\n\nAlgorithm (Random Walk Metropolis-Hastings)\n\n\n\nChoose an arbitrary point \\(\\theta_t\\) to be the first sample value in the posterior space. Propose the next value according to a random walk:\n\\[\n\\theta_{t+1} = \\theta_{t}+\\epsilon_{t+1}\n\\]\nIf the proposed value has a higher density than the current value, we accept the proposal and move forward. Precisely, we accept the proposal with probability\n\\[\nr=\\begin{cases}\n1,&\\text{ if }p(\\theta_{t+1}|X)\\geq p(\\theta_t|X) \\\\\n\\frac{p(\\theta_{t+1}|X)}{p(\\theta_{t}|X)}, &\\text{ if }p(\\theta_{t+1}|X)< p(\\theta_t|X)\n\\end{cases}\n\\]\n\\(r\\) is the probability that we accept \\(\\theta_{t+1}\\) as our next sample value.\n\n\nThis Metropolis criteria is the only one that will work. It strikes the perfect balance between random exploring and paying attention to the posterior shape — If we pay too little attention, we get uniform sampling; if we pay too much attention, we will get stuck on a mode forever. It can be proved the algorithm converges to the posterior distribution. That is, if the algorithm reaches the posterior density, it stays there."
  },
  {
    "objectID": "39_mh.html#remarks",
    "href": "39_mh.html#remarks",
    "title": "39  Metropolis–Hastings",
    "section": "39.3 Remarks",
    "text": "39.3 Remarks\nStep size. The rate at which Metropolis converges to the posterior distribution is highly sensitive to the step size. If the step size is too small, we obtain a density that is highly dependent on the initial value. It takes a long time to find areas of high density. On the other hand, if the step size is too big, we would reject the majority of proposals, since most of the parameter space is low and flat, and we would get a highly autocorrelated chain with low number of effective samples. We would tune the step size so that rate of acceptance is optimized (0.44 for one-dimensional models, 0.23 for high-dimensional models).\nMultiple chains. It is never a good idea to run a single chain (exploring from one starting point forever). Different chains (starting from different initial values) are likely exploring different density areas of the posterior space. It takes longer time for a single chain to explore the entire space. If different chains converge to the same distribution, we are confident the whole posterior space is explored.\nConvergence. We can compare the within-chain variance and the between-chain variance to gauge the convergence.\n\\[\n\\begin{aligned}\n\\text{within-chain variance: } & W=\\frac{1}{m}\\sum_{j=1}^m s_j^2 \\\\\n\\text{between-chain variance: } & B=\\frac{n}{m-1}\\sum_{j=1}^m (\\bar\\theta_j-\\bar\\theta)^2 \\\\\n\\end{aligned}\n\\]\nwhere \\(s_j^2\\) is the sample variance of chain \\(j\\), \\(\\theta\\) represents the sample mean, \\(m\\) is the number of chains. The convergence can be gauged by the \\(R\\)-ratio:\n\\[\n\\hat R=\\sqrt{\\frac{W+\\frac{1}{n}(B-W)}{W}}\n\\]\nInitially, \\(\\hat R >> 1\\). The better the convergence, the closer \\(\\hat R\\) is to \\(1\\).\nWarm-up. The first part of the chain is selected in a haphazard fashion, and unlikely to be representative of the posterior. So we should not include the first few samples in our final posterior sample. Usually, it is recommended to discard the first half of the chains that appear to have converged as a default method.\nEffective sample size. Dependent sampling naturally converges slower than independent sampling. For independent sampling, CLT predicts \\(\\sqrt{T}(\\hat\\theta - \\theta)\\to N(0, \\sigma^2)\\). We say the convergence speed is \\(T^{-1/2}\\). The effective sample size, \\(n_{\\text{eff}}\\), for a dependent sampler is defined so that its convergence speed is \\(n_{\\text{eff}}^{-1/2}\\).\nThinning. Once convergence is reached, we could make our samples look “more independent” if we keep only every tenth, or hundredth, of the samples. These will naturally be less correlated than the original samples. This process is known as “thinning”."
  },
  {
    "objectID": "ref.html",
    "href": "ref.html",
    "title": "References",
    "section": "",
    "text": "Hamilton, James D. 1994. Time Series Analysis. Princeton University Press.\nEnders, Walter. 2008. Applied Econometric Time Series. John Wiley & Sons.\nVerbeek, Marno. 2008. A Guide to Modern Econometrics. John Wiley & Sons.\nHayashi, Fumio. 2011. Econometrics. Princeton University Press.\nHansen, Bruce. 2022. Econometrics. Princeton University Press.\nMikusheva, Anna, and Paul Schrimpf. 2007. Time Series Analysis. MIT OpenCourseWare.\nHyndman, Rob J, and George Athanasopoulos. 2018. Forecasting: Principles and Practice (2nd Edition). OTexts.com/fpp2.\nStock, James H, and Mark W Watson. 2020. Introduction to Econometrics. Pearson.\nStock, James H, and Mark W Watson. 2016. “Dynamic Factor Models, Factor-Augmented Vector Autoregressions, and Structural Vector Autoregressions in Macroeconomics.” In Handbook of Macroeconomics, 2:415–525.\nRamey, V.A., 2016. Macroeconomic Shocks and Their Propagation. In Handbook of Macroeconomics, 2, pp.71-162.\nChan, Joshua CC. 2017. “Notes on Bayesian Macroeconometrics.” Unpublished Manuscript.\nDieppe, Alistair, Romain Legrand, and Björn Van Roye. 2016. The Bayesian Estimation, Analysis and Regression (BEAR) Toolbox. Eupearn Central Bank."
  }
]