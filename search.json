[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis for Economists",
    "section": "",
    "text": "Preface\nEmpirical macroeconomics frequently involves the analysis of time series data, encompassing variables such as GDP, inflation, and interest rates, employing methodologies distinct from those utilized in cross-sectional studies. The goal of this book is to bridge the gap between introductory time series textbooks and theoretical econometrics. In the realm of empirical research, a rudimentary comprehension of the subject matter often proves insufficient. While computational tasks can be executed through simple computer commands, practitioners must go beyond surface-level in order to understand the intricacies and limitations of the involved techniques. Conversely, an exhaustive exploration of advanced econometric theories would be excessive for practical purposes. For instance, introductory textbooks would caution against applying OLS on non-stationary time series, citing the potential risk of spurious regression. Students often accept this as a rule of thumb without a grasp of its underlying rationale. Yet, delving into intricate topics such as Itô calculus is deemed unnecessary for empirical researchers.\nThis book seeks to acquaint readers with essential time series topics crucial for understanding and conducting empirical research, with a specific focus on macroeconomic applications. In addition to introducing basic concepts and applications, such as running a regression and interpreting the result, the book endeavors to elevate comprehension to a deeper level by elucidating the “why” alongside the “what” and “how.” However, the objective is not to provide an exhaustive treatment replete with formal proofs; rather, emphasis is placed on providing intuitive explanations. Consequently, readers may encounter instances of non-rigorous proofs where a more formal approach is deemed unnecessary for an understanding required by applied works. This book can be read as intermediary materials between undergraduate econometrics and more rigorous treatments of the subject, such as Hamilton’s Time Series Analysis.\nThe materials presented are drawn from or influenced by various sources, which are listed in the References at the end of the book without being cited individually in the context. Regarding notations, I use lowercase letters for random variables, such as \\(x_t\\) and \\(y_t\\). Realizations of random variables are expressed as \\(x_1\\), \\(x_2\\), and so on. The context will make it clear whether I am referring to a random variable or its realizations. Capital letters are reserved for matrices, such as \\(A\\) and \\(B\\). Vectors and matrices are sometimes written in bold for emphasizing, such as \\(\\boldsymbol X\\) and \\(\\boldsymbol y_t\\), or in plain format as scalars if that does not lead to confusion. Greek letters are preferred for parameters, such as \\(\\alpha\\) and \\(\\beta\\). Estimators are indicated with a hat, such as \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\).\nI use the statistical language R whenever programming is involved. I am aware that there are many time series solutions available in R. To avoid burdening readers with excessive packages, I stick to base R as much as possible with a little help from the zoo package.\nI would like to emphasize that my knowledge and understanding of the subject are limited, and I acknowledge that there may be mistakes or areas where I could have provided a more accurate explanation. I deeply appreciate any feedback or corrections from readers that could improve the accuracy and clarity of this book."
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "1  Time Series Data",
    "section": "",
    "text": "Raw data: The raw values without any transformation. We are not so interested in the raw data, as it is hard to read information from it. Take the GDP plot as an example (Figure 1.1, upper-left subplot). There is an overall upward trend. But we are more interested in: how much does the economy grow this year? Is it better or worse than last year? The answers are not obvious from the raw data. Besides, there are obvious seasonal fluctuations. Usually the first quarter has the lowest value in a whole year, due to the Spring Festival, which significantly reduces the working days in the first quarter. The seasonal fluctuations prohibit us from sensibly comparing two consecutive values.\nGrowth rate: The headline GDP growth is usually derived by comparing the current quarter with the same quarter from last year. \\(g=\\frac{x_t - x_{t-4}}{x_{t-4}}\\times 100.\\) This makes sense. As mentioned above, due to seasonal patterns, comparing two consecutive quarters directly does not make sense. The year-on-year growth rate directly tells us how fast the economy grows. However, by dividing the past values, it loses the absolute level information. For instance, it is hard to tell after the pandemic, whether or not the economy recovers from its pre-pandemic output level. Besides, it is sensitive to the values of last year. For example, due to the pandemic, the GDP for 2020 is exceptionally low, which makes growth rate for 2021 exceptionally high. This is undesirable, because it does not mean the economy in 2021 is actually good. We would like a growth rate that shirks off past burdens.\nThat’s why we sometimes prefer (annualized) quarterly growth rate. \\(g=\\frac{x_t-x_{t-1}}{x_{t-1}}\\times 400.\\) Due to seasonally patterns, two consecutive quarters are not comparable directly. A first quarter value is usually much lower than the fourth quarter of last year due to holidays, which does not necessarily mean the economy condition is getting worse. Since this pattern is the same every year, it is possible to remove the seasonal fluctuations. This is called seasonally adjustment. We won’t cover seasonally adjustment in detail, but the next section will give some intuitions on how this can possibly be done. After seasonally adjusting the time series, we can calculate the growth rate based on two consecutive values (annualized by multiplying \\(4\\)). The bottom-right panel of Figure 1.1 is the seasonally-adjusted quarterly growth. Note that it is no longer biased upward in 2021 as the YoY growth.\nSeasonally-adjusted series: This is usually the data format we prefer in time series analysis. FRED reports both seasonally-adjusted and non-seasonally-adjusted series. Seasonal adjustment algorithm is a science in itself. Popular algorithms include X-13-ARIMA developed by the United States Census Bureau, TRAMO/SEATS developed by the Bank of Spain, and so on.\n\n\n\n\n\nFigure 1.1: Quarterly GDP Time Series (Unit: RMB Billion or %)\n\n\n\n\nLog levels and log growth rates: We like to work with log levels. A lot of economic time series exhibit exponential growth, such as GDP. Taking logs convert them to linear. Another amazing thing about logs is the difference of two log values can be interpreted as percentage growth. We know from Taylor expansion that for small values of \\(\\Delta x\\) : \\(\\ln(\\Delta x +1) \\approx \\Delta x\\). Therefore,\n\\[\n\\ln x_t - \\ln x_{t-1} = \\ln\\left(\\frac{x_t}{x_{t-1}}\\right) = \\ln\\left(\\frac{x_t-x_{t-1}}{x_{t-1}}+1\\right) \\approx \\frac{x_t-x_{t-1}}{x_{t-1}}.\n\\]\nSo it is very handy to just difference the log levels to get the growth rates. Log difference can also be interpreted as the continuously compounded rate of change, if assuming\n\\[\n\\frac{x_t}{x_{t-1}}=e^g \\implies g = \\ln x_t - \\ln x_{t-1}.\n\\]\nLog difference also has the property of summability: summing up a series of log differences gives the log level provided the initial level. It is not as handy if you want to recover the level values from a series of percentage growth.\n\\[\n\\ln x_t = x_0 + \\sum_{j=1}^{t} (\\ln x_j - \\ln x_{j-1}).\n\\]\n\n\n\n\n\n\nTip\n\n\n\nBuying vs. renting a home, which is better? Compute the NPV:\n\\[\n\\text{NPV}=\\sum_{t=0}^T \\frac{C_t}{(1+r)^t}=\\int_0^T C(t) e^{-rt} dt.\n\\]"
  },
  {
    "objectID": "decomp.html#time-series-components",
    "href": "decomp.html#time-series-components",
    "title": "2  Decomposition",
    "section": "2.1 Time Series Components",
    "text": "2.1 Time Series Components\nIt is helpful to think about a time series as composed of different components: a trend component, a seasonal component, and a remainder.\n\\[x_t = T_t + S_t + R_t.\\]\nThe formula assumes the “additive” composition. This assumption is appropriate if the magnitude of the fluctuations does not vary with the absolute levels of the time series. If the magnitude of fluctuations is proportional to the absolute levels, a “multiplicative” decomposition is more appropriate:\n\\[\nx_t = T_t \\times S_t \\times R_t.\n\\]\nNote that a multiplicative decomposition of a time series is equivalent to an additive decomposition on its log levels:\n\\[\n\\ln x_t = \\ln T_t + \\ln S_t + \\ln R_t.\n\\]\nDecomposing a time series allows us to extract information that is not obvious from the original time series. It also allows us to manipulate the time series. For example, if the seasonal component can be estimated, we can remove it to obtain seasonally-adjusted series, \\(x_t^{SA} = x_t - S_t\\), or \\(x_t^{SA} = x_t/S_t\\). The question is how to estimate the components given a time series."
  },
  {
    "objectID": "decomp.html#moving-averages",
    "href": "decomp.html#moving-averages",
    "title": "2  Decomposition",
    "section": "2.2 Moving Averages",
    "text": "2.2 Moving Averages\nMoving averages turn out to be handy in estiming trend-cycles by averaging out noisy fluctuations. A moving average of order \\(m\\) (assuming \\(m\\) is an odd number) is defined as\n\\[\n\\text{MA}(x_t,m) = \\frac{1}{m}\\sum_{j=-k}^{k} x_{t+j},\n\\]\nwhere \\(m=2k + 1\\). For example, a moving average of order \\(3\\) is\n\\[\n\\text{MA}(x_t, 3) = \\frac{1}{3}(x_{t-1} + x_t + x_{t+1}).\n\\]\nNote that \\(x_t\\) is centered right in the middle and the average is symmetric. This also means, if we apply this formula to real data, the first and last observation will have to be discarded. If the order \\(m\\) is an even number, the formula will no longer be symmetric. To overcome this, we can estimate a moving average over another moving average. For example, we can estimate a moving average of order \\(4\\), followed by a moving average of order \\(2\\). This is denoted as \\(2 \\times 4\\)-MA. Mathematically,\n\\[\n\\begin{aligned}\n\\text{MA}(x_t, 2 \\times 4) &= \\frac{1}{2}[\\text{MA}(x_{t-1}, 4) + \\text{MA}(x_t, 4)] \\\\\n&= \\frac{1}{2}\\left[\\frac{1}{4}(x_{t-2} + x_{t-1} + x_t + x_{t+1}) + \\frac{1}{4}(x_{t-1} + x_t + x_{t+1} + x_{t+2})\\right] \\\\\n&= \\frac{1}{8}x_{t-2} + \\frac{1}{4}x_{t-1} + \\frac{1}{4}x_{t} + \\frac{1}{4}x_{t+1} + \\frac{1}{8}x_{t+2}.\n\\end{aligned}\n\\]\nNote that how the \\(2\\times4\\)-MA averages out the seasonality for time series with seasonal period \\(4\\), e.g. quarterly series. The formula puts equal weight on every quarter — the first and last terms refer the same quarter and their weights combined to \\(\\frac{1}{4}\\).\nIn general, we can use \\(m\\)-MA to estimate the trend if the seasonal period is an odd number, and use \\(2\\times m\\)-MA if the seasonal period is an even number.\n\ndata = readRDS(\"data/gdp.Rds\")  # a `zoo` object\ngdp2x4MA = ma(ma(data$GDP,4),2) # from `forecast` package\nts.plot(cbind(data$GDP, gdp2x4MA), col=1:2)\n\n\n\n\nFigure 2.1: Quarterly GDP with 2x4-MA estimate of the trend-cycle"
  },
  {
    "objectID": "decomp.html#classical-decomposition",
    "href": "decomp.html#classical-decomposition",
    "title": "2  Decomposition",
    "section": "2.3 Classical Decomposition",
    "text": "2.3 Classical Decomposition\nMoving averages give us everything we need to perform classical decomposition. Classical decomposition, invented 1920s, is the simplest method decompose a time series into trend, seasonality and remainder. It is outdated nowadays and has been replaced by more advanced algorithms. Nonetheless, it serves as a good example for introductory purpose on how time series decomposition could possibly be achieved.\nThe algorithm for additive decomposition is as follows.\n\nEstimate the trend component \\(T_t\\) by applying moving averages. If the seasonal period is an odd number, apply the \\(m\\)-th order MA. If the seasonal period is even, apply the \\(2\\times m\\) MA.\nCalculate the detrended series \\(x_t - T_t\\).\nCalculate the seasonal component \\(S_t\\) by averaging all the detrended values of the season. For example, for quarterly series, the value of \\(S_t\\) for Q1 would be the average of all values in Q1. This assumes the seasonal component is constant over time. \\(S_t\\) is then adjusted to ensure all values summed up to zero.\nSubtracting the seasonal component to get the remainder \\(R_t = x_t-T_t-S_t\\).\n\n\nlog(data$GDP) |> decompose() |> plot()\n\n\n\n\nFigure 2.2: Classical multiplicative decomposition of quarterly GDP\n\n\n\n\nThe example performs additive decomposition to the logged quarterly GDP series. Note how the constant seasonal component is removed, leaving the smooth and nice-looking up-growing trend. The remainder component tells us the irregular ups and downs of the economy around the trend-cycle. Isn’t it amazing that a simple decomposition of the time series tells us a lot about the economy?"
  },
  {
    "objectID": "decomp.html#seasonal-adjustment",
    "href": "decomp.html#seasonal-adjustment",
    "title": "2  Decomposition",
    "section": "2.4 Seasonal Adjustment",
    "text": "2.4 Seasonal Adjustment\nBy decomposing a time series into trend, seasonality and remainder, it readily gives us a method for seasonal adjustment. Simply subtracting the seasonal component from the original data, or equivalently, summing up the trend and the remainder components, would give us the seasonally-adjusted series.\nThe following example compares the seasonally-adjusted series using the classical decomposition with the state-of-the-art X-13ARIMA-SEATS algorithm. Despite the former is far more rudimentary than the latter, they look quite close if we simply eye-balling the plot. By taking first-order differences, we can see the series based on classical decomposition is more volatile, suggesting the classical decomposition is less robust to unusual values.\n\nlogdata = log(data) |> window(start=2000)\nseasadj = as.ts(logdata$GDP) - decompose(logdata$GDP)$seasonal\n\npar(mfrow=c(1,2), mar=rep(2,4))\nts.plot(cbind(seasadj, logdata$GDPSA), col=1:2)\nts.plot(diff(cbind(seasadj, logdata$GDPSA)), col=1:2)\n\n\n\n\nComparing classical decomposition and X-13"
  },
  {
    "objectID": "autocov.html#autocorrelation",
    "href": "autocov.html#autocorrelation",
    "title": "3  ACF and PACF",
    "section": "3.1 Autocorrelation",
    "text": "3.1 Autocorrelation\nThe temporal dependence is characterized by the correlation between \\(y_t\\) and its own lags \\(y_{t-k}\\).\n\nDefinition 3.1 The \\(k\\)-th order autocovariance of \\(y_t\\) is defined as\n\\[\\gamma_k = \\text{cov}(y_t, y_{t-k}).\\]\nThe \\(k\\)-th order autocorrelation is defined as\n\\[\\rho_k = \\frac{\\text{cov}(y_t, y_{t-k})}{\\text{var}(y_t)} = \\frac{\\gamma_k}{\\gamma_0}.\\]\n\nIf we plot the autocorrelation as a function of the lag length \\(k\\), we get the autocorrelation function (ACF). Here is an example of the ACF of China’s monthly export growth (log-difference). The lag on the horizontal axis is counted by seasonal period. Because it is monthly data, 1 period is 12 months. We can see the autocorrelation is the strongest for the first two lags. Longer lags are barely significant. There are spikes with 12-month and 24-month lags, indicating the seasonality is not fully removed from the series.\n\ndata = readRDS(\"data/md.Rds\")\nacf(data$Export, main='Autocorrelation')\n\n\n\n\nFigure 3.1: ACF for monthly export growth"
  },
  {
    "objectID": "autocov.html#partial-autocorrelation",
    "href": "autocov.html#partial-autocorrelation",
    "title": "3  ACF and PACF",
    "section": "3.2 Partial Autocorrelation",
    "text": "3.2 Partial Autocorrelation\nACF measures the correlation between \\(y_t\\) and \\(y_{t-k}\\) regardless of their relationships with the intermediate variables \\(y_{t-1},y_{t-2},\\dots,y_{t-k+1}\\). Even if \\(y_t\\) is only correlated with the first-order lag, it is automatically made correlated with the \\(k\\)-th order lag through intermediate variables. Sometime we are interested in the correlation between \\(y_t\\) and \\(y_{t-k}\\) partialling out the influence of intermediate variables.\n\nDefinition 3.2 The partial autocorrelation function (PACF) considers the correlation between the remaining parts in \\(y_t\\) and \\(y_{t-k}\\) after partialling out the intermediate effect of \\(y_{t-1},y_{t-2},\\dots,y_{t-k+1}\\).\n\\[\n\\phi_k = \\begin{cases}\n\\text{corr}(y_t, y_{t-1})=\\rho_{_1}, \\text{ if } k=1;\\\\\n\\text{corr}(r_{y_t|y_{t-1},\\dots,y_{t-k+1}}, r_{y_{t-k}|y_{t-1},\\dots,y_{t-k+1}}), \\text{ if } k\\geq 2;\n\\end{cases}\n\\]\nwhere \\(r_{y|x}\\) means the remainder in \\(y\\) after partialling out the intermediate effect of \\(x\\).\n\nIn practice, \\(\\phi_k\\) can be estimated by the regression\n\\[\ny_t = \\mu + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_k y_{t-k} + \\epsilon_t.\n\\]\nThe estimated coefficient \\(\\hat\\phi_k\\) is the partial autocorrelation after controlling the intermediate lags.\n\npacf(data$Export, main='Partial Autocorrelation')\n\n\n\n\nFigure 3.2: PACF for monthly export growth"
  },
  {
    "objectID": "stationary.html#stationary-process",
    "href": "stationary.html#stationary-process",
    "title": "4  Stationarity",
    "section": "4.1 Stationary Process",
    "text": "4.1 Stationary Process\n\nDefinition 4.1 A stochastic process is said to be strictly stationary if its properties are unaffected by a change of time origin. In other words, the joint distribution at any set of time is not affect by an arbitrary shift along the time axis.\n\n\nDefinition 4.2 A stochastic process is called covariance stationary (or weak stationary) if its means, variances, and covariances are independent of time. Formally, a process \\(\\{y_t\\}\\) is covariance stationary if for all \\(t\\) it holds that\n\n\\(\\mathbb{E}(y_t) = \\mu < \\infty\\);\n\\(\\text{var}(y_t) = \\gamma_{_0} < \\infty\\);\n\\(\\text{cov}(y_t,y_{t-k})=\\gamma_k\\), for \\(k=1,2,3,\\dots\\)\n\n\nStationarity is an important concept in time series analysis. It basically says the statistical properties of a time series are stable over time. Otherwise, if the statistical properties vary with time, statistics estimated from past values, such autocorrelations, would be much less meaningful. Strict stationarity requires the joint distribution being stable, that is moments of any order would be stable over time. In practice, mostly we only care about the first- and second-order moments, that is means and variances and covariances. Therefore, covariance stationary is sufficient.\nFigure 4.1 shows some examples of stationary and non-stationary time series. Only the first one is stationary (it is generated from \\(i.i.d\\) normal distribution). The second one is not stationary as its mean is not constant over time. The third one is not stationary as its variance is not constant. The last one is not stationary either, because its covariance is not constant.\n\n\n\n\n\nFigure 4.1: Stationary and non-stationary time series\n\n\n\n\nReal-life time series are rarely stationary. But they can be transformed to (quasi) stationary by differencing. Figure 4.2 shows some examples of the first-order (log) differences of real-life time series. They more or less exhibit some properties of stationarity, but not perfectly stationary. The series can be further “stationarized” by taking a second-order difference. But these examples are acceptable to be treated as stationary in our models. Even if they are not perfectly stationary, the model can be thought of being used to “extract” their stationary properties.\n\n\n\n\n\nFigure 4.2: Stationary and non-stationary time series (real life)\n\n\n\n\n\nProposition 4.1 For stationary series, it holds that \\(\\gamma_k = \\gamma_{-k}\\).\n\n\nProof. By definition,\n\\[\n\\gamma_k = \\mathbb{E}[(y_t-\\mu)(y_{t-k}-\\mu)],\n\\]\n\\[\n\\gamma_{-k} = \\mathbb{E}[(y_t-\\mu)(y_{t+k}-\\mu)].\n\\]\nSince \\(y_t\\) is stationary, \\(\\gamma_k\\) is invariant with time. Let \\(t'=t+k\\), we have\n\\[\n\\begin{aligned}\n\\gamma_{k} &= \\mathbb{E}[(y_{t'}-\\mu)(y_{t'-k}-\\mu)] \\\\\n&= \\mathbb{E}[(y_{t+k}-\\mu)(y_{t}-\\mu)] \\\\\n&= \\gamma_{-k}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "stationary.html#ergodicity",
    "href": "stationary.html#ergodicity",
    "title": "4  Stationarity",
    "section": "4.2 Ergodicity",
    "text": "4.2 Ergodicity\nTemporal dependence is an important feature of time series variables. This dependence is both a bless and a curve. Autocorrelation enables us to make predictions based on past experiences. However, as we will see in later chapters, it also invalidates theorems that usually require \\(iid\\) assumptions. Ideally, we would like the temporal dependence to be not too strong. This is the property of ergodicity.\n\nDefinition 4.3 A stationary process \\(\\{y_t\\}\\) is ergodic if\n\\[\n\\lim_{n\\to\\infty} |\\mathbb{E}[f(y_t...y_{t+k})g(y_{t+n}...y_{t+n+k})]|=|\\mathbb{E}[f(y_t...y_{t+k})]||\\mathbb{E}[g(y_{t+n}...y_{t+n+k})]|.\n\\]\n\nHeuristically, ergodicity means if two random variables are positioned far enough in the sequence, they become almost independent. In other words, ergodicity is a restriction on dependency. An ergodic process allows serial correlation, but the serial correlation disappears if the two observations are far apart. Ergodicity is important because as we will see in later chapters, the Law of Large Numbers or the Central Limit Theorem will not hold without it.\n\nTheorem 4.1 A stationary time series is ergodic if \\(\\sum_{k=0}^{\\infty} |\\gamma_k| < \\infty\\).\n\n\nProof. A rigorous proof is not necessary. It is enough to give an intuition why autocorrelation disappears for far apart variables. Note that \\(\\sum_{k=0}^{\\infty} |\\gamma_k|\\) is monotonic and increasing, it converges. Therefore, \\(\\gamma_k \\to 0\\) by Cauchy Criterion."
  },
  {
    "objectID": "stationary.html#white-noise",
    "href": "stationary.html#white-noise",
    "title": "4  Stationarity",
    "section": "4.3 White Noise",
    "text": "4.3 White Noise\nWhite noise is a special stationary process that is an important building block of many time series models.\n\nDefinition 4.4 A stochastic process \\(w_t\\) is called white noise if its has constant mean \\(0\\) and variance \\(\\sigma^2\\) and no serial correlation \\(\\text{cov}(w_t, w_{t-k})=0\\) for any \\(k \\neq 0\\). The white noise process is denoted as\n\\[\nw_t \\sim \\text{WN}(0, \\sigma^2).\n\\]\n\nThis is the weakest requirement for while noise. It only requires no serial correlation. We may impose further assumptions. If every \\(w_t\\) is independent, it becomes independent white noise \\(w_t \\sim \\perp\\text{WN}(0, \\sigma^2)\\). Independence does not imply identical distribution. If every \\(w_t\\) is independently and identically distributed, it is called \\(i.i.d\\) white noise, \\(w_t \\overset{iid}{\\sim} \\text{WN}(0, \\sigma^2)\\). If the distribution is normal, it becomes the most perfect white noise, that is \\(i.i.d\\) Gaussian white noise, \\(w_t \\overset{iid}{\\sim} N(0, \\sigma^2)\\). The first plot of Figure 4.1 is a demonstration of the \\(i.i.d\\) Gaussian white noise. In most cases, the weakest form of white noise is sufficient.\n\n\n\n\n\n\nExercise\n\n\n\nProve that a while noise process is stationary."
  },
  {
    "objectID": "models.html#classification",
    "href": "models.html#classification",
    "title": "5  Model vs Spec",
    "section": "5.1 Classification",
    "text": "5.1 Classification\nTime series models can be broadly sorted into four categories based on whether we are dealing with stationary or non-stationary time series, or whether the model involves only one variable or multiple variables.\n\nTime series model classification\n\n\n\nStationary\nNonstationary\n\n\n\n\nUnivariate\nARMA\nUnit root\n\n\nMultivariate\nVAR\nCointegration"
  },
  {
    "objectID": "models.html#model-vs-spec",
    "href": "models.html#model-vs-spec",
    "title": "5  Model vs Spec",
    "section": "5.2 Model vs Spec",
    "text": "5.2 Model vs Spec\nWe use the word “model” rather loosely in economics and econometrics. Anything that deals with the quantified relationships between variables can be called a model. A general equilibrium model is a model. A regression is also a model.\nTo make things less confusing, we would use the word “model” more restrictively in this chapter. We reserve the word model to those representing the data generating processes (DGPs). That is, when we write down a model in an equation, we literally mean it. If we say \\(y_t\\) follows an AR(1) model:\n\\[\n\\begin{aligned}\ny_t &= \\phi y_{t-1} + \\epsilon_t,\\\\\n\\epsilon_t &\\sim N(0,\\sigma^2).\n\\end{aligned}\n\\]\nWe literally mean \\(y_t\\) is determined by its previous value and an contemporary innovation drawn from a Gaussian distribution.\nA model is distinguished from a specification. Suppose \\(\\{y_t\\}\\) represent the GDP series, we can estimate a regression:\n\\[\ny_t = \\phi y_{t-1} + e_t\n\\]\nThis is a specification not a model. Because the DGP of GDP data is unknown, definitely not an AR(1). We can nontheless fit this spec with the data and get an estimated \\(\\hat\\phi\\). If \\(e_t\\) satisfies some nice properties, for example, uncorrelated with the regressor, then we know this \\(\\hat\\phi\\) is consistent.\nWhen we run regressions with real-life data, we are actually working with specifications. They are not the DGPs of the random variables. But they allow us to recover some useful information from the data when certain assumptions are met. Mostly we are interested in the relationships between variables. A specification describes this relationship, even though it does not describe the full DGP.\nThis chapter deals with models in the abstract sense. The next chapter will discuss how to fit a model or a spec with real data."
  },
  {
    "objectID": "ar.html#ar1-process",
    "href": "ar.html#ar1-process",
    "title": "6  AR Models",
    "section": "6.1 AR(1) Process",
    "text": "6.1 AR(1) Process\nWe start with the simplest time series model — autoregressive model, or AR model. The simplest from of AR model is AR(1), which involves only one lag,\n\\[\ny_t = \\mu + \\phi y_{t-1} + \\epsilon_t,\n\\tag{6.1}\\]\nwhere \\(\\epsilon_t \\sim \\text{WN}(0,\\sigma^2)\\). The model can be extended to include more lags. An AR(\\(p\\)) model is defined as\n\\[\ny_t = \\mu + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} +  \\epsilon_t.\n\\]\nWe focus on AR(1) first. The model states that the value of \\(y_t\\) is determined by a constant, its previous value, and a random innovation. We call the last term \\(\\epsilon_t\\) innovation, not an error term. It is not an error, it is a random contribution that is unknown until time \\(t\\). It should also not be confused with the so-called “structural shock”, which is attached with a structural meaning and will be discussed in later chapters.\nThe model is probabilistic, as oppose to deterministic, in the sense that some information is unknown or deliberately omitted, so that we do not know the deterministic outcome, but only a probability distribution.\n\n\n\n\n\n\nNote\n\n\n\nThink about tossing a coin: if every piece of information is incorporated in the model, including the initial speed and position, the air resistance, and so on; then we can figure out the exact outcome, whether the coin will land on its head or tail. But this is unrealistic. Omitting all these information, we can model the process as a Bernoulli distribution. The probability model will not give a deterministic outcome, but only a distribution with each possible value associated with a probability.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe assumption that a process is only determined by its past values and a white noise innovation seems very restrictive. But it is not. Think about the three assumptions for technical analysis of the stock market (there are still many investors believing this): (1) The market discounts everything, (2) prices move in trends and counter-trends, and (3) price action is repetitive, with certain patterns reoccurring. Effectively, it is saying we can predict the stock market by the past price patterns. If we were to write a model for the stock market based on these assumptions, AR(\\(p\\)) isn’t a bad choice at all.\n\n\nNote that the model can be rewritten as\n\\[\ny_t - \\frac{\\mu}{1-\\phi} = \\phi\\left(y_{t-1} - \\frac{\\mu}{1-\\phi}\\right) + \\epsilon_t,\n\\]\nassuming \\(\\phi\\neq 1\\). If we define \\(\\tilde{y_t}=y_t - \\frac{\\mu}{1-\\phi}\\), we can get rid of the constant term:\n\\[\n\\tilde{y}_t = \\phi\\tilde{y}_{t-1} + \\epsilon_t.\n\\tag{6.2}\\]\nIt can be easily shown, if \\(y_t\\) is stationary, \\(\\frac{\\mu}{1-\\phi}\\) is the stationary mean. Because this mechanical transformation can always be done to remove the constant. We can simply ignore the constant term without lost of generality.\n\n\n\n\n\n\nNote\n\n\n\nWorking with demeaned variables greatly simplify the notation. For example, assuming \\(\\mathbb{E}(y_t)=0\\), the variance is simply the second-order moment \\(\\mathbb{E}(y_t^2)\\); the covariance can be written as \\(\\mathbb{E}(y_ty_{t-k})\\).\n\n\nFor a constant-free AR(1) model, we can rewrite the model as follows:\n\\[\n\\begin{aligned}\ny_t &= \\phi y_{t-1} + \\epsilon_t \\\\\n&= \\phi( \\phi y_{t-2} + \\epsilon_{t-1}) + \\epsilon_t \\\\\n&= \\phi^2 y_{t-2} + \\phi\\epsilon_{t-1} + \\epsilon_t \\\\\n&= \\phi^2 (\\phi y_{t-3} + \\epsilon_{t-2}) + \\phi\\epsilon_{t-1} + \\epsilon_t \\\\\n&= \\phi^3 y_{t-3} + \\phi^2\\epsilon_{t-2} + \\phi\\epsilon_{t-1} + \\epsilon_t \\\\\n&\\;\\vdots \\\\\n&= \\phi^t y_0 + \\sum_{j=0}^{t-1} \\phi^j\\epsilon_{t-j} \\\\\n&= \\sum_{j=0}^{\\infty} \\phi^j\\epsilon_{t-j}.\n\\end{aligned}\n\\tag{6.3}\\]\nThe exercise shows an AR(1) process can be reduced to an MA process, which will be discussed in the next section. It says the value of \\(y_t\\) is determined by its initial value (if it has one) and the accumulated innovations in the past. It is our deeds in history that shapes our world today.\n\n\n\n\n\n\nNote\n\n\n\nThe property that an AR process can be rewritten as an infinite MA process with absolute summable coefficients \\(\\sum_{j=0}^{\\infty}|\\phi^j|<\\infty\\) is called causal. This must not be confused with the causal effect in econometrics (defines in the ceteris paribus sense). To avoid confusion, we avoid use this term as much as possible.\n\n\nNow we focus our attention on the critical parameter \\(\\phi\\). If \\(|\\phi|>1\\), the process is explosive. We are not interested in explosive processes. If a real-world time series grows exponentially, we take logarithm to transform it to linear. So in most of our discussions, we rule out the case of explosive behaviour.\nIf \\(|\\phi|<1\\), \\(\\phi^j\\to 0\\) as \\(j\\to\\infty\\). This means the influence of innovations far away in the past decays to zero. We will show that the series is stationary and ergodic.\nIf \\(|\\phi|=1\\), we have \\(y_t = \\sum_{j=0}^{\\infty} \\text{sgn}(\\phi)^j\\epsilon_{t-j} = \\sum_{j=0}^{\\infty}\\tilde{\\epsilon}_{t-j}\\). This means the influence of past innovations will not decay no matter how distant away they are. This is known as a unit root process, which will be covered in later chapters. But it is clear that the process is not stationary. Consider the variance of \\(y_t\\) conditioned on an initial value:\n\\[\n\\text{var}(y_t|y_0) = \\text{var}(\\sum_{j=0}^{t-1}\\epsilon_{t-j})=\\sum_{j=0}^{t-1}\\text{var}(\\epsilon_{t-j})=\\sum_{j=0}^{t-1}\\sigma^2=t\\sigma^2.\n\\]\nThe variance is increasing with time. It is not constant. Figure 6.1 simulates the AR(1) with \\(\\phi=0.5\\) and \\(\\phi=1\\) respectively.\n\ny = arima.sim(list(ar=0.5), n=1000)\nz = arima.sim(list(order=c(0,1,0)), n=1000)\nplot(cbind(y,z), plot.type=\"multiple\", nc=2, ann=F, \n     mar.multi=rep(2,4), oma.multi = rep(0,4))\n\n\n\n\nFigure 6.1: Simulation of AR(1) processes\n\n\n\n\n\nProposition 6.1 An AR(1) process with \\(|\\phi|<1\\) is covariance stationary.\n\n\nProof. Let’s compute the mean, variance and covariance for the AR(1) process.\n\\[\n\\mathbb{E}(y_t) = \\mathbb{E}\\left[\\sum_{j=0}^{\\infty} \\phi^j\\epsilon_{t-j}\\right] = \\sum_{j=0}^{\\infty} \\phi^j\\mathbb{E}[\\epsilon_{t-j}]=0.\n\\]\n\\[\n\\begin{aligned}\n\\text{var}(y_t)\n&= \\text{var}\\left[\\sum_{j=0}^{\\infty} \\phi^j\\epsilon_{t-j}\\right]\n= \\sum_{j=0}^{\\infty} \\phi^j\\text{var}[\\epsilon_{t-j}] \\\\\n&= \\sigma^2 \\sum_{j=0}^{\\infty} \\phi^j\n=\\frac{\\sigma^2}{1-\\phi}.\n\\end{aligned}\n\\]\nFor the covariances,\n\\[\n\\begin{aligned}\n\\gamma_1 &= \\mathbb{E}(y_ty_{t-1})\n= \\mathbb{E}((\\phi y_{t-1} + \\epsilon_t)y_{t-1}) \\\\\n&= \\mathbb{E}(\\phi y_{t-1}^2 + \\epsilon_t y_{t-1}) \\\\\n&= \\phi\\mathbb{E}(y_{t-1}^2) + 0 \\\\\n&= \\frac{\\phi\\sigma^2}{1-\\phi};\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\gamma_2 &= \\mathbb{E}(y_ty_{t-2})\n= \\mathbb{E}((\\phi y_{t-1} + \\epsilon_t)y_{t-2}) \\\\\n&= \\mathbb{E}(\\phi y_{t-1}y_{t-2} + \\epsilon_t y_{t-2}) \\\\\n&= \\phi\\mathbb{E}(y_{t-1}y_{t-2}) \\\\\n&= \\phi\\gamma_1 = \\frac{\\phi^2\\sigma^2}{1-\\phi};\\\\\n&\\;\\vdots \\\\\n\\gamma_j &= \\frac{\\phi^j\\sigma^2}{1-\\phi}.\n\\end{aligned}\n\\]\nAll of them are independent of time \\(t\\). By Definition 4.2, the process is covariance stationary.\n\nSo the ACF decays gradually as \\(\\phi^j \\to 0\\). What about the PACF? Estimating the PACF is equivalent to regressing \\(y_t\\) on its lags. Since there is only one lag, the PACF should have non-zero value only for the first lag, and zeros for all other lags.\n\npar(mfrow=c(1,2), mar=c(2,4,1,1))\nacf(y); pacf(y)\n\n\n\n\nFigure 6.2: ACF and PACF for AR(1) process"
  },
  {
    "objectID": "ar.html#lag-operator",
    "href": "ar.html#lag-operator",
    "title": "6  AR Models",
    "section": "6.2 Lag Operator",
    "text": "6.2 Lag Operator\nTo facilitate easy manipulation of lags, we introduce the lag operator:\n\\[\nLy_t = y_{t-1}.\n\\]\nThe AR(1) process can be written with the lag operator:\n\\[\ny_t = \\phi Ly_t+ \\epsilon_t \\implies (1-\\phi L)y_t = \\epsilon_t.\n\\]\nThe lag operator \\(L\\) can be manipulated just as polynomials. It looks weird, but it actually works. Do a few exercises to convince yourself.\n\\[\nL^2 y_t = L(Ly_t) = Ly_{t-1} = y_{t-2}.\n\\]\n\\[\n\\begin{aligned}\n(1-L)^2y_t &= (1-L)(y_t - y_{t-1}) \\\\\n&= (y_t-y_{t-1})-(y_{t-1}-y_{t-2}) \\\\\n&= y_t - 2y_{t-1} + y_{t-2} \\\\\n&=(1-2L+L^2)y_t.\n\\end{aligned}\n\\]\nWe can even inverse a lag polynomial (provided \\(|\\phi|<1\\)),\n\\[\n\\begin{aligned}\n(1-\\phi L)y_t &= \\epsilon_t \\\\\n\\implies y_t &= (1-\\phi L)^{-1}\\epsilon_t =\\sum_{j=0}^{\\infty} \\phi^j L^j \\epsilon_t = \\sum_{j=0}^{\\infty} \\phi^j \\epsilon_{t-j}.\n\\end{aligned}\n\\]\nWe reach the same conclusion as Equation 6.3 with the lag operator."
  },
  {
    "objectID": "ar.html#arp-process",
    "href": "ar.html#arp-process",
    "title": "6  AR Models",
    "section": "6.3 AR(p) Process",
    "text": "6.3 AR(p) Process\nWe now generalize the conclusions above to AR(\\(p\\)) processes. With the help of the lag operator, an AR(\\(p\\)) process can be written as\n\\[\n(1-\\phi_1 L-\\phi_2 L^2-\\dots-\\phi_p L^p) y_t = \\epsilon_t,\n\\]\nor even more parsimoniously,\n\\[\n\\phi(L)y_t = \\epsilon_t.\n\\]\nNote that we ignore the constant term, which can always be removed by redefine \\(\\tilde{y}_t = y_t - \\frac{\\mu}{1-\\phi_1-\\phi_2-\\dots-\\phi_p}\\).\nTo derive the MA representation, we need to figure out \\(\\phi^{-1}(L)\\). By the Fundamental Theorem of Algebra, we know the polynomial \\(\\phi(z)\\) has \\(p\\) roots in the complex space. So the lag polynomial can be factored as\n\\[\n(1-\\lambda_1L)(1-\\lambda_2L)\\dots(1-\\lambda_pL) y_t = \\epsilon_t,\n\\]\nwhere \\(z=\\lambda_i^{-1}\\) is the \\(i\\)-th root of \\(\\phi(z)\\). If the roots are outside the unit circle, \\(|\\lambda_i|<1\\) means each of the left hand terms is inversible.\n\\[\n\\begin{aligned}\ny_t &= \\frac{1}{(1-\\lambda_1L)(1-\\lambda_2L)\\dots(1-\\lambda_pL)}\\epsilon_t \\\\\n&= \\left(\\frac{c_1}{1-\\lambda_1L} + \\frac{c_2}{1-\\lambda_2L} + \\dots + \\frac{c_p}{1-\\lambda_pL}\\right)\\epsilon_t \\\\\n&= \\sum_{j=0}^{\\infty}(c_1\\lambda_1^j+c_2\\lambda_2^j+\\dots+c_p\\lambda_p^j)L^j\\epsilon_t \\\\\n&= \\sum_{j=0}^{\\infty} \\theta_j\\epsilon_{t-j}, \\text{ where }\\theta_j=c_1\\lambda_1^j+\\dots+c_p\\lambda_p^j.\n\\end{aligned}\n\\]\nIt follows that this process has constant mean and variance. For the covariances, given\n\\[\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t,\n\\]\nMultiply both sides by \\(y_t\\) and take expectation,\n\\[\n\\mathbb{E}[y_t^2] = \\phi_1 \\mathbb{E}[y_ty_{t-1}] + \\phi_2 \\mathbb{E}[y_ty_{t-2}] + \\dots + \\phi_p \\mathbb{E}[y_ty_{t-p}],\n\\]\n\\[\n\\gamma_0 = \\phi_1\\gamma_{-1} + \\phi_2\\gamma_{-2} + \\dots + \\phi_p\\gamma_{-p}.\n\\]\nSimilarly, multiply both sides by \\(y_{t-1},\\dots,y_{t-j}\\), we have\n\\[\n\\begin{aligned}\n\\gamma_1 &= \\phi_1\\gamma_{0} + \\phi_2\\gamma_{-1} + \\dots + \\phi_p\\gamma_{-p+1},\\\\\n&\\;\\vdots\\\\\n\\gamma_j &= \\phi_1\\gamma_{j-1} + \\phi_2\\gamma_{j-2} + \\dots + \\phi_p\\gamma_{j-p}.\n\\end{aligned}\n\\]\nThis is called the Yule-Walker equation. The first \\(p\\) unknowns \\(\\gamma_0,\\dots,\\gamma_{p-1}\\) can be solved by the first \\(p\\) equations. The rest can then be solved iteratively.\nIt can be shown all of the covariances are invariant with time. Therefore, under the condition all \\(|\\lambda_i|<1\\), the AR(\\(p\\)) process is stationary.\nFor the PACF, a regression of \\(y_t\\) over its lags would recover \\(p\\) non-zero coefficients. Longer lags should have coefficients insignificantly different from zero.\n\ny = arima.sim(list(ar=c(2.4, -1.91, 0.5)), n=3000)\npar(mfrow=c(1,2), mar=c(2,4,1,1))\nacf(y); pacf(y)\n\n\n\n\nFigure 6.3: ACF and PACF for AR(p) process\n\n\n\n\n\nProposition 6.2 An AR(\\(p\\)) process is stationary if all the roots of \\(\\phi(z)\\) are outside the unit circle.\n\n\nProposition 6.3 An AR(\\(p\\)) process is characterized by (i) an ACF that is infinite in extend but tails of gradually; and (ii) a PACF that is (close to) zero for lags after \\(p\\)."
  },
  {
    "objectID": "ma.html#ma1-process",
    "href": "ma.html#ma1-process",
    "title": "7  MA Models",
    "section": "7.1 MA(1) Process",
    "text": "7.1 MA(1) Process\nAgain, let’s start with the simplest moving average model. A first-order moving average process, or MA(1), is defined as\n\\[\ny_t = \\mu + \\epsilon_t + \\theta\\epsilon_{t-1},\n\\tag{7.1}\\]\nwhere \\(\\{\\epsilon_t\\} \\sim \\text{WN}(0, \\sigma^2)\\) are uncorrelated innovations. The MA model says the current value \\(y_t\\) is a moving average of past innovations (in MA(1), the weight on \\(\\epsilon_{t-1}\\) is \\(\\theta\\)). MA models directly relate the observable variable to past innovations. If we know the past innvation \\(\\epsilon_{t-j}\\), we can easily figure out its contribution to the outcome variable (unlike AR models where the effect of a past innovation is transmitted through \\(y_{t-j},\\dots,y_{t-1}\\)). So MA models are the preferred analytic tool in many applications, despite it looks odd from the eyes of regression modelers. You may wonder how it is possible to estimate such a model. We will put off the estimation techniques to the next chapter.\nIt is clear that \\(y_t\\) has a constant mean, \\(\\mathbb{E}(y_t) = \\mu\\). We can omit the constant if we work with the demeaned series \\(\\tilde{y}_t = y_t - \\mu\\). Without loss of generality, we assume for the rest \\(\\{y_t\\}\\) has zero mean, so the model is simplified as\n\\[\ny_t = \\epsilon_t + \\theta\\epsilon_{t-1}.\n\\tag{7.2}\\]\nLet’s compute its variance and covariances:\n\\[\n\\begin{aligned}\n\\gamma_0 &= \\text{var} (\\epsilon_t + \\theta\\epsilon_{t-1}) = \\text{var}(\\epsilon_t) + \\theta^2\\text{var}(\\epsilon_{t-1}) = (1+\\theta^2)\\sigma^2;\\\\\n\\gamma_1 &= \\text{cov}(y_t, y_{t-1}) = \\text{cov}(\\epsilon_{t} + \\theta\\epsilon_{t-1}, \\epsilon_{t-1} + \\theta\\epsilon_{t-2}) = \\text{cov} (\\theta\\epsilon_{t-1}, \\epsilon_{t-1} + \\theta\\epsilon_{t-2}) = \\theta\\sigma^2; \\\\\n\\gamma_2 &= \\text{cov}(y_t, y_{t-2}) = \\text{cov}(\\epsilon_{t} + \\theta\\epsilon_{t-1}, \\epsilon_{t-2} + \\theta\\epsilon_{t-3}) = 0; \\\\\n&\\vdots\\\\\n\\gamma_j &= 0 \\text{ for } |j|\\geq 2.\n\\end{aligned}\n\\]\nIt is clear that the MA(1) process is stationary. And the ACF cuts off after the first lag. Because more distant lags \\(y_{t-k}\\) are constituted by even more distant innovations \\(\\epsilon_{t-k}, \\epsilon_{t-k-1}, ...\\) which has no relevance for \\(y_t\\) given the MA(1) structure.\nWe have seen AR processes are equivalent to MA(\\(\\infty\\)) processes. Similar results hold for MA models. Rewrite the MA(1) process with the lag operator, assuming \\(|\\theta| < 1\\),\n\\[\ny_t = (1 + \\theta L) \\epsilon_t \\Leftrightarrow\n(1+\\theta L)^{-1} y_t = \\epsilon_t \\Leftrightarrow\n\\sum_{j=0}^{\\infty} (-\\theta)^j y_{t-j} = \\epsilon_t.\n\\]\nThat means an MA(1) is equivalent to an AR(\\(\\infty\\)) process if \\((1+\\theta L)\\) is invertible. This shows AR and MA are really the same family of models. The model AR or MA is chosen by parsimonious principle. For example, an AR model with many lags can possibly be modeled by a parsimonious MA model.\nSince an MA(1) is equivalent to some AR(\\(\\infty\\)) process, the PACF of an MA(1) should tail off gradually.\n\ny = arima.sim(list(ma=0.8), n=2000)\npar(mfrow=c(1,2), mar=c(1,4,1,1))\nacf(y); pacf(y)\n\n\n\n\nFigure 7.1: ACF and PACF of MA(1) process\n\n\n\n\n\n\n\n\n\n\nInvertibility\n\n\n\nIf \\(|\\theta|>1\\), \\(\\theta(L)\\) is not invertible. Define another MA(1) process,\n\\[\ny_t = \\epsilon_t + \\theta^{-1}\\epsilon_{t-1},\\quad\\epsilon_t\\sim\\text{WN}(0,\\theta^2\\sigma^2).\n\\]\nWe can verify that its variance and covariances are exactly the same as Equation 7.2. For non-invertible MA process, as long as \\(\\theta(L)\\) avoids unit root, we can always find an invertible process that shares the same ACF. This means, for a stationary MA process, it makes no harm to just assume it is invertible."
  },
  {
    "objectID": "ma.html#maq-process",
    "href": "ma.html#maq-process",
    "title": "7  MA Models",
    "section": "7.2 MA(q) Process",
    "text": "7.2 MA(q) Process\nA \\(q\\)-th order moving average, or MA(\\(q\\)) process, is written as\n\\[\ny_t = \\mu + \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\dots + \\theta_q\\epsilon_{t-q},\n\\tag{7.3}\\]\nwhere \\(\\{\\epsilon_t\\} \\sim \\text{WN}(0, \\sigma^2)\\).\n\nProposition 7.1 An MA(\\(q\\)) process is stationary.\n\n\nProof. We will show that the mean, variance and covariances of MA(\\(q\\)) are all invariant with time.\n\\[\n\\mathbb{E}(y_t)= \\mu.\n\\]\nAssume for the rest, \\(\\{y_t\\}\\) is demeaned.\n\\[\n\\begin{aligned}\n\\gamma_0 &= \\mathbb{E}(y_t^2) = \\mathbb{E}[(\\epsilon_t + \\theta_1\\epsilon_{t-1} + \\dots + \\theta_q\\epsilon_{t-q})^2] \\\\\n&= \\mathbb{E}[\\epsilon^2] + \\theta_1^2\\mathbb{E}[\\epsilon_{t-1}^2] + \\dots + \\theta_q^2\\mathbb{E}[\\epsilon_{t-q}^2] \\\\\n&= (1+\\theta_1^2+\\dots+\\theta_q^2)\\sigma^2; \\\\\n\\gamma_1 &= \\mathbb{E}[y_ty_{t-1}] = \\mathbb{E}[(\\epsilon_t + \\theta_1\\epsilon_{t-1} + \\dots + \\theta_q\\epsilon_{t-q}) \\\\\n&\\hspace{10.2em} (\\epsilon_{t-1} + \\dots + \\theta_{q-1}\\epsilon_{t-q} + \\theta_q\\epsilon_{t-q-1})] \\\\\n&= \\theta_1\\mathbb{E}[\\epsilon_{t-1}^2] + \\theta_2\\theta_1\\mathbb{E}[\\epsilon_{t-2}^2] + \\dots + \\theta_q\\theta_{q-1}\\mathbb{E}[\\epsilon_{t-q}^2] \\\\\n&= (\\theta_1 + \\theta_2\\theta_1 + \\dots + \\theta_q\\theta_{q-1})\\sigma^2;\\\\\n&\\vdots\\\\\n\\gamma_j &= \\mathbb{E}[y_ty_{t-j}] = \\mathbb{E}[(\\epsilon_t + \\dots + \\theta_j\\epsilon_{t-j} + \\dots + \\theta_q\\epsilon_{t-q}) \\\\\n&\\hspace{12.5em} (\\epsilon_{t-j} + \\dots + \\theta_{q-j}\\epsilon_{t-q} + \\dots + \\theta_q\\epsilon_{t-q-j})] \\\\\n&= \\theta_j\\mathbb{E}[\\epsilon_{t-j}^2] + \\theta_{j+1}\\theta_1\\mathbb{E}[\\epsilon_{t-j-1}^2] + \\dots + \\theta_q\\theta_{q-j}\\mathbb{E}[\\epsilon_{t-q}^2] \\\\\n&= (\\theta_j + \\theta_{j+1}\\theta_1 + \\dots + \\theta_q\\theta_{q-j})\\sigma^2, \\text{ for } j\\leq q; \\\\\n\\gamma_j &= 0, \\text{ for } j > q.\n\\end{aligned}\n\\]\n\n\nProposition 7.2 An MA(\\(q\\)) process is invertible iff the roots of \\(\\theta(z)\\) are outside the unit circle.\n\n\nProposition 7.3 An MA(\\(q\\)) process is characterized by (i) an ACF that is (close to) zero after \\(q\\) lags; and (i) a PACF that is infinite in extend but tails of gradually."
  },
  {
    "objectID": "ma.html#mainfty-process",
    "href": "ma.html#mainfty-process",
    "title": "7  MA Models",
    "section": "7.3 MA(\\(\\infty\\)) Process",
    "text": "7.3 MA(\\(\\infty\\)) Process\nMA(\\(\\infty\\)) is a special case deserves attention. Partly because all ARMA processes can be reduced to MA(\\(\\infty\\)) processes. In addition to MA(\\(q\\)) processes, we need more conditions for MA(\\(\\infty\\)) to be stationary. Consider the variance of\n\\[\ny_t = \\sum_{j=0}^{\\infty}\\theta_j\\epsilon_{t-j},\n\\]\n\\[\n\\gamma_0 = \\mathbb{E}[y_t^2] = \\mathbb{E}\\left[\\left(\\sum_{j=0}^{\\infty}\\theta_j\\epsilon_{t-j}\\right)^2\\right] = \\left(\\sum_{j=0}^{\\infty}\\theta_j^2\\right)\\sigma^2.\n\\]\nIt only make sense if \\(\\sum_{j=0}^{\\infty}\\theta_j^2<\\infty\\). This property is called square summable.\n\nProposition 7.4 An MA(\\(\\infty\\)) process is stationary if the coefficients \\(\\{\\theta_j\\}\\) are square summable."
  },
  {
    "objectID": "arma.html#armapq",
    "href": "arma.html#armapq",
    "title": "8  ARMA Models",
    "section": "8.1 ARMA(p,q)",
    "text": "8.1 ARMA(p,q)\nARMA(\\(p\\), \\(q\\)) is a mixed autoregressive and moving average process.\n\\[\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} +\n\\epsilon_t + \\theta_1\\epsilon_{t-1} + \\dots + \\theta_q\\epsilon_{t-q},\n\\]\nor\n\\[\n\\phi(L) y_t = \\theta(L) \\epsilon_t,\n\\]\nwhere \\(\\{\\epsilon_{t}\\} \\sim \\text{WN}(0, \\sigma^2)\\).\nThe MA part is always stationary as shown in Proposition 7.1. The stationarity of an ARMA process solely depends on the AR part. The condition is the same as Proposition 6.2.\nAssume \\(\\phi^{-1}(L)\\) exist, then the ARMA(\\(p\\),\\(q\\)) process can be reduce to MA(\\(\\infty\\)) process:\n\\[\ny_t = \\phi^{-1}(L)\\theta(L)\\epsilon_t = \\psi(L) \\epsilon_t,\n\\]\nwhere \\(\\psi(L) = \\phi^{-1}(L)\\theta(L)\\).\n\n\n\n\n\n\nExercise\n\n\n\nCompute the MA equivalence for ARMA(1,1)."
  },
  {
    "objectID": "arma.html#arimapdq",
    "href": "arma.html#arimapdq",
    "title": "8  ARMA Models",
    "section": "8.2 ARIMA(p,d,q)",
    "text": "8.2 ARIMA(p,d,q)\nARMA(\\(p\\),\\(q\\)) is used to model stationary time series. If \\(y_t\\) is not stationary, we can transform it to stationary and model it with an ARMA model. If the first-order difference \\((1-L)y_t = y_t - y_{t-1}\\) is stationary, then we say \\(y_t\\) is integrated of order 1. If it requires \\(d\\)-th order difference to be stationary, \\((1-L)^dy_t\\), we say it is integrated of order \\(d\\). The ARMA model involves integrated time series is called ARIMA model:\n\\[\n\\phi(L)(1-L)^d y_t = \\theta(L)\\epsilon_t.\n\\]"
  },
  {
    "objectID": "wold.html#wold-decomposition",
    "href": "wold.html#wold-decomposition",
    "title": "9  Wold Theorem",
    "section": "9.1 Wold Decomposition",
    "text": "9.1 Wold Decomposition\nSo far we have spent a lot of effort with ARMA models, which are the indispensable components of any time series textbook. The following theorem justifies its importance. The Wold Decomposition Theorem basically says every covariance-stationary process has an ARMA representation. Therefore, with long enough lags, any covariance-stationary process can be approximated arbitrarily well by ARMA models. This is a very bold conclusion to make. It sets up the generality of ARMA models, which makes it one of the most important theorems in time series analysis.\n\nTheorem 9.1 (Wold Decomposition Theorem) Every covariance-stationary time series \\(y_t\\) can be written as the sum of two time series, one deterministic and one stochastic. Formally,\n\\[\ny_t = \\eta_t + \\sum_{j=0}^{\\infty} b_j\\epsilon_{t-j},\n\\]\nwhere \\(\\eta_t \\in I_{-\\infty}\\) is a deterministic time series (such as one represented by a sine wave); \\(\\epsilon_t\\) is an uncorrelated innovation sequence with \\(\\mathbb{E}[\\epsilon_t]=0\\), \\(\\mathbb{E}[\\epsilon_t\\epsilon_{t-j}]=0\\) for \\(j\\neq 0\\); and \\(\\{b_j\\}\\) are square summable, \\(\\sum_{j=0}^{\\infty}|b_j|^2<\\infty\\).\n\n\nProof. We will prove the theorem by constructing the innovation sequence \\(\\{e_t\\}\\) and showing it satisfies the conditions stated. Let \\(e_t = y_t - \\hat{\\mathbb{E}}(y_t|I_{t-1}) = y_t - a(L)y_{t-1}\\), where \\(\\hat{\\mathbb{E}}(y_t|I_{t-1})\\) is the best linear predictor (BLP) of \\(y_t\\) based on information set at \\(t-1\\). \\(a(L)\\) does not depend on \\(t\\) because \\(y_t\\) is covariance stationary. As the best linear predictor, \\(a(L)\\) solves\n\\[\\min_{\\{a_j\\}} \\mathbb{E} (y_t - \\sum_{j=1}^{\\infty}a_jy_{t-j})^2.\\] The first-order conditions with respect to \\(a_j\\) gives\n\\[\n\\begin{aligned}\n\\mathbb{E}[y_{t-j}(y_t-\\sum_{j=1}^{\\infty}a_jy_{t-j})] &= 0, \\\\\n\\implies \\mathbb{E}[y_{t-j}e_t] &=0.\n\\end{aligned}\n\\]\nWe now verify that \\(e_t\\) satisfies the white noise conditions. Without loss of generality, we may assume \\(\\mathbb{E}(y_t)=0\\), it follows that \\(\\mathbb{E}(e_t)=0\\). \\(\\text{var}(e_t)=\\mathbb{E}(y_t-a(L)y_t)^2\\) is a function of covariance of \\(y_t\\) and \\(a_j\\), none of which varies with time. So \\(\\text{var}(e_t)=\\sigma^2\\) is constant. Utilizing the first-order condition, \\(\\mathbb{E}[e_te_{t-j}] = \\mathbb{E}[e_t(y_{t-j}-a(L)y_{t-j})] = 0.\\)\nRepeatedly substituting for \\(y_{t-k}\\) gives\n\\[\n\\begin{aligned}\ny_t &= e_t + \\sum_{k=1}^{\\infty} a_ky_{t-k} \\\\\n&= e_t + a_1(e_{t-1} + \\sum_{k=1}^{\\infty} a_ky_{t-1-k}) + \\sum_{k=2}^{\\infty} a_ky_{t-k}\\\\\n&= e_t + a_1 e_{t-1} + \\sum_{k=1}^{\\infty} \\tilde{a}_ky_{t-k-1} \\\\\n&= e_t + a_1 e_{t-1} + \\eta_t^1 \\\\\n&\\quad\\vdots\\\\\n&= \\sum_{j=0}^{k} c_j e_{t-j} + \\eta_t^k,\n\\end{aligned}\n\\] where \\(\\eta_t^k \\in I_{t-k-1}\\). As \\(k\\to\\infty\\), we have \\(v_t = y_t - \\sum_{j=0}^{\\infty}c_je_{t-j} \\in I_{-\\infty}\\).\n\nLet’s appreciate this theorem for a while. The property of stationarity can be loosely understood as having stable patterns over time. The Wold Theorem states that any such patterns can be captured by ARMA models. In other words, ARMA models are effective in modelling stable patterns repeated over time, in so far as only 2nd-order moments are of concern. Even if the time series is not entirely stationary, if we model it with ARMA, it can be thought as extracting the stationary patterns. Figure 9.1 demonstrates the ARIMA modelling of monthly export.\n\nlibrary(zoo)\ndata = read.csv.zoo(\"data/md.csv\", FUN = as.yearmon, regular = TRUE)\ny = data$Export\nmod = arima(y, order = c(2,0,1))\nyhat = y - mod$residuals\nplot(cbind(y, yhat), plot.type = \"s\", col = 1:2, ann = F)\n\n\n\n\nFigure 9.1: Monthly export modelled with ARIMA(2,0,1)"
  },
  {
    "objectID": "wold.html#causality-and-invertibility",
    "href": "wold.html#causality-and-invertibility",
    "title": "9  Wold Theorem",
    "section": "9.2 Causality and Invertibility*",
    "text": "9.2 Causality and Invertibility*\nWe have seen that AR models can be rewritten as MA models and vice versa, suggesting the ARMA representation of a stochastic process is not unique. We have also seen that a non-invertible MA process can be equivalently represented by an invertible MA process. For example, the following MA(1) processes have the same ACF:\n\\[\n\\begin{aligned}\nx_t &= w_t + \\frac{1}{5} w_{t-1}, & w_t\\sim\\text{WN}(0,25);\\\\\ny_t &= v_t + 5 v_{t-1}, & v_t\\sim\\text{WN}(0, 1).\n\\end{aligned}\n\\]\nThe same property holds for AR processes. In Chapter 6, we state that an AR(1) process is explosive if \\(|\\phi|>1\\). This is not entirely rigorous. Consider an AR(1) process,\n\\[\ny_t = \\phi y_{t-1} + \\epsilon_t, \\text{ where } |\\phi| > 1.\n\\]\nMultiply both sides by \\(\\phi^{-1}\\),\n\\[\n\\phi^{-1} y_t = y_{t-1} + \\phi^{-1}\\epsilon_t,\n\\]\nRewrite it as an MA process,\n\\[\n\\begin{aligned}\ny_t &= \\phi^{-1} y_{t+1} - \\phi^{-1}\\epsilon_{t+1} \\\\\n&= \\phi^{-1} (\\phi^{-1} y_{t+2} - \\phi^{-1}\\epsilon_{t+2}) - \\phi^{-1}\\epsilon_{t+1} \\\\\n&\\;\\vdots \\\\\n&= \\sum_{j=1}^{\\infty} -\\phi^{-j}\\epsilon_{t+j}.\n\\end{aligned}\n\\]\nGiven \\(|\\phi^{-1}|<1\\), the process is stationary, expressed as discounted innovations in the future (despite this looks quite odd). In fact, for an non-causal AR process, we can find a causal AR process that generates the same ACF (remember the term causal means an AR process can be converted to an MA process with absolute summable coefficients).\nThe problem is given an ARMA equation, it is not enough to uniquely pin down a stochastic process. Both the explosive process and the stationary process can be a solution to \\(y_t = \\phi y_{t-1} + \\epsilon_t\\). But for a stationary process expressed as an AR model with \\(|\\phi|>1\\), we can always find an AR(1) process with \\(|\\tilde\\phi|<1\\) and a different white noise sequence \\(\\{\\tilde\\epsilon_t\\}\\) that generate the same ACF.\nThe following theorems state the conditions for the existence of stationary solutions, and the possibility of rewriting non-causal or non-invertible ARMA representations as causal and invertible ones. Since it is always possible to do so, it loses nothing to stick with causal and invertible ARMA processes when modelling stationary time series.\n\nTheorem 9.2 A unique stationary solution to the ARMA process \\(\\phi(L)y_t = \\theta(L)\\epsilon_t\\) exists iff \\(\\phi\\) and \\(\\theta\\) have no common factors and the roots of \\(\\phi(z)\\) avoid the unit circle:\n\\[\n|\\phi(z)|=1 \\implies \\phi(z) = 1-\\phi_1z-\\dots-\\phi_pz^p \\neq 0.\n\\]\n\n\nTheorem 9.3 Let \\(\\{y_t\\}\\) be a stationary ARMA process defined by \\(\\phi(L)y_t = \\theta(L)\\epsilon_t\\). If the roots of \\(\\theta(z)\\) avoid unit circle, then there are polynomials \\(\\tilde{\\phi}\\) and \\(\\tilde\\theta\\) and a white noise sequence \\(\\tilde\\epsilon\\) such that \\(\\{y_t\\}\\) satisfies \\(\\tilde\\phi(L)y_t = \\tilde\\theta(L)\\tilde\\epsilon_t\\), and this is a causal and invertible ARMA process."
  },
  {
    "objectID": "ols.html#chapter-overview",
    "href": "ols.html#chapter-overview",
    "title": "10  Preliminaries",
    "section": "10.1 Chapter Overview",
    "text": "10.1 Chapter Overview\nThis chapter serves two purposes. One is to introduce the techniques for estimating time series models. The other is to explain the concept of dynamic causal effect. We join the two topics in one chapter because both of them can be done via a regression framework. Maximum likelihood estimation plays a pivotal role in estimating time series models. Nonetheless, starting with OLS always make things easier. We start with a quick review of the basic OLS concepts that are familiar to any students in econometrics, that is the regressions applied to cross-sectional \\(iid\\) observations. We then extend it to time series data. We will see it is not as straightforward as one might expect, as intertemporal dependencies between observation need additional treatment. In the second half of the chapter, we will explain the concept of dynamic causal effect, that is the causal effect of an intervention on outcome variables. Similar to cross-sectional studies, we need to define the causal effect relative to counterfactuals. With time series data, the counterfactuals have to be defined across time rather across individuals."
  },
  {
    "objectID": "ols.html#asymptotic-theorems-for-i.i.d-random-variables",
    "href": "ols.html#asymptotic-theorems-for-i.i.d-random-variables",
    "title": "10  Preliminaries",
    "section": "10.2 Asymptotic Theorems for i.i.d Random Variables",
    "text": "10.2 Asymptotic Theorems for i.i.d Random Variables\n\nTheorem 10.1 (Law of Large Numbers) Let \\(\\{x_i\\}\\) be \\(iid\\) random variables with \\(\\mathbb{E}(x_i)=\\mu\\) and \\(\\text{Var}(x_i)=\\sigma^2<\\infty\\). Define \\(\\bar{x}_n = \\frac{1}{n}\\sum_{i=1}^n x_i\\). Then \\(\\bar{x}_n \\overset{p}{\\to} \\mu\\) as \\(n \\to \\infty\\).\n\n\nProof. We will give an non-rigorous proof, but nonetheless shows the tenets. It is easy to see \\(\\mathbb{E}(\\bar{x}_n) = \\mu\\). Consider the variance,\n\\[\n\\text{Var}(\\bar x_n) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n x_i\\right) \\overset{iid}= \\frac{1}{n^2}\\sum_{i=1}^n\\text{Var}(x_i) = \\frac{\\sigma^2}{n}\\to 0.\n\\]\nThat is \\(\\bar x_n\\) converges to \\(\\mu\\) with probability 1 as \\(n\\to\\infty\\). Note that we can move the variance inside the summation operator because \\(x_i\\) are \\(iid\\), in which all the covariance terms are 0.\n\n\nTheorem 10.2 (Central Limit Theorem) Let \\(\\{x_i\\}\\) be \\(iid\\) random variables with \\(\\mathbb{E}(x_i)=\\mu\\) and \\(\\text{Var}(x_i)=\\sigma^2<\\infty\\). Define \\(\\bar{x}_n = \\frac{1}{n}\\sum_{i=1}^n x_i\\). Then\n\\[\n\\frac{\\bar x_n - \\mu}{\\sigma/\\sqrt{n}} \\overset{d}\\to N(0,1).\n\\]\n\n\nProof. Without loss of generality, assume \\(x_i\\) is demeaned and standardized to have standard deviation 1. It remains to show \\(\\sqrt{n} \\bar x_n \\to N(0,1)\\). Define the moment generating function (MGF) for \\(\\sqrt{n}\\bar x_n\\):\n\\[M_{\\sqrt{n}\\bar x_n}(t) =\\mathbb{E}[e^{(\\sqrt{n}^{-1}\\sum_{i=1}^n x_i)t}] \\overset{iid}= \\{\\mathbb{E}[e^{(n^{-1/2}x_i)t}]\\}^n.\\]\nEvaluate the MGF for each \\(x_i\\):\n\\[\n\\mathbb{E}[e^{(n^{-1/2}x_i)t}] = 1 + \\mathbb{E}(n^{-1/2}x_i)t + \\mathbb{E}(n^{-1}x_i^2)t^2 + \\cdots =1 + \\frac{t^2}{2n} + o(n^{-1}).\n\\]\nSubstituting back,\n\\[\nM_{\\sqrt{n}\\bar x_n}(t) = \\left[1 + \\frac{t^2}{2n} + o(n^{-1})\\right]^n = \\left[\\left(1 + \\frac{t^2}{2n} \\right)^{\\frac{2n}{t^2}}\\right]^{\\frac{t^2}{2}}\\to e^{\\frac{t^2}{2}}.\n\\]\nNote that we drop the \\(o(n^{-1})\\) because it converges faster than \\(\\frac{1}{n}\\). \\(e^{\\frac{t^2}{2}}\\) is the MGF for standard normal distribution. Hence, the theorem is proved."
  },
  {
    "objectID": "ols.html#ols-for-i.i.d-random-variables",
    "href": "ols.html#ols-for-i.i.d-random-variables",
    "title": "10  Preliminaries",
    "section": "10.3 OLS for i.i.d Random Variables",
    "text": "10.3 OLS for i.i.d Random Variables\nWe now give a very quick review of OLS with \\(iid\\) random variables. These materials are assumed familiar to the readers. We do not intend to introduce them in any detail. This section is a quick snapshot of some key concepts, so that we could contrast them with the time series regression introduced in the next section.\nA linear regression model postulates the joint distribution of \\((y_i, x_i)\\) follows a linear relationship,\n\\[\ny_i = x_i'\\beta + \\epsilon_i.\n\\]\nExpressed in terms of data matrix,\n\\[\n\\begin{bmatrix}\ny_1\\\\y_2\\\\\\vdots\\\\y_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nx_{11}, x_{12}, \\dots, x_{1p}\\\\\nx_{21}, x_{22}, \\dots, x_{2p}\\\\\n\\ddots \\\\\nx_{n1}, x_{n2}, \\dots, x_{np}\\\\\n\\end{bmatrix}'\n\\begin{bmatrix}\n\\beta_1\\\\\\beta_2\\\\\\vdots\\\\\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_1\\\\\\epsilon_2\\\\\\vdots\\\\\\epsilon_n\n\\end{bmatrix}.\n\\]\nFrom the perspective of dataset, the matrix matrix is fixed in the sense that they are just numbers in the dataset. But for statistical analysis, we view each entry in the matrix as random, that is as a realization of a random process.\nTo estimate the parameter \\(\\beta\\) from sample data, OLS seeks to minimize the squared residuals\n\\[\n\\min_{\\beta} \\sum_{i=1}^{n}(y_i - x_i'\\beta)^2.\n\\]\nThe first-order condition implies,\n\\[\n\\begin{aligned}\n&\\sum_i x_i(y_i - x_i'\\beta) =0, \\\\\n&\\sum_i x_iy_i - \\sum_i x_ix_i'\\beta=0, \\\\\n&\\hat\\beta = \\left(\\sum_i x_ix_i'\\right)^{-1} \\left(\\sum_i x_iy_i\\right) \\\\\n&= \\beta + \\left(\\sum_i x_ix_i'\\right)^{-1} \\left(\\sum_i x_i\\epsilon_i\\right).\n\\end{aligned}\n\\]\nUnder the Gauss-Markov assumptions, particularly \\(\\mathbb{E}(\\epsilon_i|x_j)=0\\) and \\(\\text{var}(\\epsilon|X)=\\sigma^2 I\\) (homoskedasticity and nonautocorrelation), the OLS estimator is BLUE (Best Linear Unbiased Estimator).\nUnder the assumption of \\(iid\\) random variables and homoskedasticity, we invoke the LLN and CLT to derive the asymptotic distribution for the OLS estimator,\n\\[\n\\begin{aligned}\n\\sqrt{n}(\\hat\\beta-\\beta) &= \\left(\\frac{1}{n}\\sum_i x_ix_i'\\right)^{-1} \\left(\\sqrt{n}\\frac{1}{n}\\sum_i x_i\\epsilon_i\\right) \\\\[1em]\n&\\to [\\mathbb{E}(x_ix_i')]^{-1} \\text{N}(0, \\mathbb{E}(x_i\\epsilon_i\\epsilon_i'x_i')) \\\\[1em]\n&\\to \\text{N}(0, \\sigma^2[\\mathbb{E}(x_ix_i')]^{-1}).\n\\end{aligned}\n\\]\nNote how the \\(iid\\) assumption is required throughout the process. The following section will show how to extend the OLS to non-\\(iid\\) random variables and how it leads to modification of the results."
  },
  {
    "objectID": "ols2.html#asymptotic-theorems-for-dependent-random-variables",
    "href": "ols2.html#asymptotic-theorems-for-dependent-random-variables",
    "title": "11  OLS for Time Series",
    "section": "11.1 Asymptotic Theorems for Dependent Random Variables",
    "text": "11.1 Asymptotic Theorems for Dependent Random Variables\nThe asymptotic theorems and regressions that work for \\(iid\\) random variable do not immediately apply to time series. Consider the proof for Theorem 10.1, without the \\(iid\\) assumption we have\n\\[\n\\begin{aligned}\n\\text{var}\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_i\\right)\n&= \\frac{1}{n^2}\\sum_{i=1}^n\\sum_{j=1}^n \\text{cov}(x_i, x_j) \\\\\n&= \\frac{1}{n^2}[\\text{cov}(x_1, x_1) + \\text{cov}(x_1, x_2) + \\cdots + \\text{cov}(x_1, x_n)+ \\\\\n&\\hspace{3em} \\text{cov}(x_2, x_1) + \\text{cov}(x_2, x_2) + \\cdots + \\text{cov}(x_2, x_n)+ \\\\\n&\\hspace{3em}\\vdots\\\\\n&\\hspace{3em} \\text{cov}(x_n, x_1) + \\text{cov}(x_n, x_2) + \\cdots + \\text{cov}(x_n, x_n)] \\\\\n&= \\frac{1}{n^2} [n\\gamma_0 + 2(n-1)\\gamma_1 + 2(n-2)\\gamma_1 + 2(n-2)\\gamma_2 + \\dots] \\\\\n&= \\frac{1}{n} \\left[2\\sum_{k=1}^n \\gamma_k\\left(1-\\frac{k}{n}\\right) + \\gamma_0 \\right].\n\\end{aligned}\n\\]\nThe argument for the \\(iid\\) does not work with the presence of serial correlations. If we assume absolute summability, \\(\\sum_{j=-\\infty}^{\\infty} |\\gamma_j| <\\infty\\), then\n\\[\n\\lim_{n\\to\\infty} \\frac{1}{n} \\left[2\\sum_{k=1}^n \\gamma_k\\left(1-\\frac{k}{n}\\right) + \\gamma_0 \\right] =0.\n\\]\nIn this case, we still have the LLN holds. Otherwise, as the variance may not converge. Remember Theorem 4.1, absolute summability implies the series is ergodic.\n\nProposition 11.1 If \\(x_t\\) is a covariance stationary time series with absolutely summable auto-covariances, then a Law of Large Numbers holds.\n\nFrom the new proof of LLN one can guess that the variance in a Central Limit Theorem should also change. The serially correlated \\(x_t\\), the liming variance is given by\n\\[\n\\begin{aligned}\n\\text{var}\\left(\\frac{1}{\\sqrt n}\\sum_{i=1}^{n}x_i\\right)\n&= 2\\sum_{k=1}^n \\gamma_k\\left(1-\\frac{k}{n}\\right) + \\gamma_0 \\\\\n&\\to 2\\sum_{k=1}^{\\infty} \\gamma_k + \\gamma_0 = \\sum_{k=-\\infty}^{\\infty}\\gamma_k = S.\n\\end{aligned}\n\\]\nWe call \\(S\\) the long-run variance. There are many CLTs for serially correlated observations. We give the two mostly commonly cited versions: one applies to MA(\\(\\infty\\)) processes, the other one is more general.\n\nTheorem 11.1 Let \\(y_t\\) be an MA process: \\(y_t = \\mu + \\sum_{j=0}^\\infty c_j\\epsilon_{t-j}\\) where \\(\\epsilon_t\\) is independent white noise and \\(\\sum_{j=0}^{\\infty} |c_j|<\\infty\\) (this implies ergodic), then\n\\[\n\\sqrt{T} \\bar y_t \\overset{d}\\to N(0, S),\n\\]\nwhere \\(S = \\sum_{k=-\\infty}^{\\infty}\\gamma_k\\) is the long-run variance.\n\n\nTheorem 11.2 (Gordin’s CLT) Assume we have a strictly stationary and ergodic series \\(\\{y_t\\}\\) with \\(\\mathbb{E}(y_t^2) <\\infty\\) satisfying: \\(\\sum_j\\{\\mathbb{E}[\\mathbb{E}[y_t|I_{t-j}]-\\mathbb{E}[y_t|I_{t-j-1}]]^2\\}^{1/2} <\\infty\\) and \\(\\mathbb{E}[y_t|I_{t-j}]\\to 0\\) as \\(j\\to\\infty\\), then\n\\[\n\\sqrt{T} \\bar y_t \\overset{d}\\to N(0, S),\n\\]\nwhere \\(S = \\sum_{k=-\\infty}^{\\infty}\\gamma_k\\) is the long-run variance.\n\nThe Gordin’s conditions are intended to make the dependence between distant observations to decrease to 0. ARMA process is a special case of Gordin series. The essence of these theorems is that we need some restrictions on dependencies for LLN and CLT to hold. We allow serial correlations as long as they are not too strong. If the observations become almost independent as they are far away in time, the can still apply the asymptotic theorems."
  },
  {
    "objectID": "ols2.html#ols-for-time-series",
    "href": "ols2.html#ols-for-time-series",
    "title": "11  OLS for Time Series",
    "section": "11.2 OLS for Time Series",
    "text": "11.2 OLS for Time Series\n\nDefinition 11.1 Given a time series regression model\n\\[y_t = x_t'\\beta + \\epsilon_t, \\]\n\\(x_t\\) is weakly exogenous if\n\\[ \\mathbb{E}(\\epsilon_t | x_t, x_{t-1}, ...) = 0;\\]\n\\(x_t\\) is strictly exogenous if\n\\[ \\mathbb{E}(\\epsilon_t | \\{x_t\\}_{t=-\\infty}^{\\infty}) = 0. \\]\n\nStrictly exogeneity requires innovations being exogenous from all past and future regressors; while weakly exogeneity only requires being exogenous from past regressors. In practice, strict exogeneity is too strong as an assumption. The weak exogenous is more practical and it is enough to ensure the consistency of the OLS estimator.\nThe OLS estimator is as usual:\n\\[\\hat\\beta = \\beta + \\left(\\frac{1}{n}\\sum_t x_t x_t'\\right)^{-1} \\left(\\frac{1}{n}\\sum_t x_t\\epsilon_t\\right).\\]\nAssuming LLN holds and \\(x_t\\) is weakly exogenous, we have\n\\[\n\\begin{aligned}\n\\frac{1}{n}\\sum_t x_t x_t' &\\to \\mathbb{E}(x_t x_t') = Q,\\\\\n\\frac{1}{n}\\sum_t x_t\\epsilon_t &\\to \\mathbb{E}(x_t\\epsilon_t) = \\mathbb{E}[x_t\\mathbb{E}[\\epsilon_t|x_t]] = 0.\n\\end{aligned}\n\\]\nTherefore, \\(\\hat\\beta \\to \\beta\\). The OLS estimator is consistent.\nAssuming the Gordin’s conditions hold for \\(z_t=x_t\\epsilon_t\\), the CLT gives\n\\[\n\\frac{1}{\\sqrt n}\\sum_t x_t\\epsilon_t \\to N(0,S),\n\\]\nwhere \\(S = \\sum_{-\\infty}^{\\infty}\\gamma_j\\) is the long-run variance for \\(z_t\\). Thus, we have the asymptotic normality for the OLS estimator\n\\[\n\\sqrt{T}(\\hat\\beta - \\beta) \\to N(0,Q^{-1}SQ^{-1}).\n\\]\nNote how the covariance matrix \\(S\\) is different from the one in the \\(iid\\) case where \\(S=\\sigma^2\\mathbb{E}(x_ix_i')\\). The long-run variance \\(S\\) takes into account the auto-dependencies between observations. The auto-dependencies usually arise from the serially correlated error terms. It may also arise from \\(x_t\\) being autocorrelated and from conditional heteroskedasticity of the error terms. Because of the auto-covariance structure, \\(S\\) cannot be estimated in the same way as in the \\(iid\\) case. The estimator for \\(S\\) is called HAC (heteroskedasticity autocorrelation consistent) standard errors."
  },
  {
    "objectID": "ols2.html#hac-standard-errors",
    "href": "ols2.html#hac-standard-errors",
    "title": "11  OLS for Time Series",
    "section": "11.3 HAC Standard Errors",
    "text": "11.3 HAC Standard Errors\n\\(S\\) can be estimated with truncated autocovariances,\n\\[\n\\hat S = \\sum_{j=-h(T)}^{h(T)} \\hat\\gamma_j.\n\\]\n\\(h(T)\\) is a function of \\(T\\) and \\(h(T)\\to\\infty\\) as \\(T\\to\\infty\\), but more slowly. Because we don’t want to include too many imprecisely estimated covariances. Another problem is the estimated \\(\\hat S\\) might be negative. The solution is weight the covariances in a way to ensure positiveness:\n\\[\n\\hat S = \\sum_{j=-h(T)}^{h(T)} k_T(j) \\hat\\gamma_j.\n\\]\n\\(k_T(\\cdot)\\) is called a kernel. The weights are chosen to guarantee positive-definiteness by weighting down high lag covariances. Also we need \\(k_T(\\cdot)\\to 1\\) for consistency.\nA popular HAC estimator is the Newey-West variance estimator, in which \\(h(T) = 0.75 T^{1/3}\\) and \\(k_T(j) = \\frac{h-j}{h}\\), so that\n\\[\n\\hat S = \\sum_{j=-h}^{h} \\left(\\frac{h-j}{h}\\right)\\hat\\gamma_j.\n\\]"
  },
  {
    "objectID": "ols2.html#example",
    "href": "ols2.html#example",
    "title": "11  OLS for Time Series",
    "section": "11.4 Example",
    "text": "11.4 Example\nNote that all of our discussions in this chapter apply only to stationary time series. Without stationarity, even the autocovariance \\(\\gamma_j\\) might not be well-defined. In the following example, we generate artificial data from an AR(2) process, and recover the parameters by regression \\(y_t\\) on its lags.\n\nlibrary(lmtest)\ny = arima.sim(list(ar = c(0.5, 0.3)), n = 1000)\nmod = lm(y ~ ., data = cbind(y, lag(y,-1), lag(y,-2)))\ncoeftest(mod, vcov. = sandwich::NeweyWest(mod))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.025182   0.031793 -0.7921   0.4285    \n`lag(y, -1)`  0.536739   0.030622 17.5281   <2e-16 ***\n`lag(y, -2)`  0.265160   0.031017  8.5489   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "mle.html",
    "href": "mle.html",
    "title": "12  MLE for ARMA Models",
    "section": "",
    "text": "OLS can only be used to estimate AR models, but not MA models. MA models or ARMA models in general can be estimated using maximum likelihood approach. Maximum likelihood estimation (MLE) starts with an assumed distribution of the random variables. The parameters are chosen to maximize the likelihood of observing the data under the distribution.\nConsider an ARMA(\\(p\\), \\(q\\)) model\n\\[\ny_t = \\phi_1 y_{t-1} +\\dots + \\phi_p y_{t-p} + u_t + \\theta_1 u_{t-1} + \\dots + \\theta_q u_{t-q}\n\\]\nWrite in the form of data matrix:\n\\[\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\y_T\n\\end{bmatrix}=\n\\underbrace{\n\\begin{bmatrix}\ny_0 & y_{-1} & \\dots & y_{1-p} \\\\\ny_1 & y_{0} & \\dots & y_{2-p} \\\\\ny_2 & y_{1} & \\dots & y_{3-p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_T & y_{T-1} & \\dots & y_{T-p} \\\\\n\\end{bmatrix}\n}_{\\boldsymbol X}\n\\begin{bmatrix}\n\\phi_1 \\\\ \\phi_2 \\\\ \\phi_3 \\\\ \\vdots \\\\ \\phi_p\n\\end{bmatrix} +\n\\underbrace{\n\\begin{bmatrix}\n1 & 0 & 0 & \\dots & 0 \\\\\n\\theta_1 & 1 & 0 & \\dots & 0 \\\\\n\\theta_2 & \\theta_1 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & & \\ddots & \\vdots \\\\\n0  & \\dots & \\theta_2 & \\theta_1 & 1\n\\end{bmatrix}\n}_{\\boldsymbol\\Gamma}\n\\begin{bmatrix}\nu_1 \\\\ u_2 \\\\ u_3 \\\\ \\vdots \\\\ u_T\n\\end{bmatrix}\n\\]\nOr compactly,\n\\[\n\\boldsymbol{y = X\\phi + \\Gamma u}.\n\\]\nWe assume the innovations are jointly normal \\(\\boldsymbol u \\sim N(0,\\sigma^2\\boldsymbol I)\\). We also assume the first \\(p\\) observations are known initial values \\(y_0, y_{-1},\\dots,y_{1-p}\\) and \\(u_0 = u_{-1} = \\dots = u_{1-q} = 0\\). Therefore, the observed data are jointly normal given the initial condition,\n\\[\n\\boldsymbol{y|y_0} \\sim N(\\boldsymbol{X\\phi}, \\sigma^2\\boldsymbol{\\Gamma\\Gamma'}).\n\\]\nThe probability density function for multivariate normal is\n\\[\nf(\\boldsymbol{y} | \\boldsymbol{y_0}, \\boldsymbol{\\phi}, \\boldsymbol{\\Gamma},\\sigma^2) = (2\\pi)^{-T/2}|\\sigma^2\\boldsymbol{\\Gamma\\Gamma'}|^{-1/2}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{y-X\\phi})' (\\sigma^2\\boldsymbol{\\Gamma\\Gamma'})^{-1} (\\boldsymbol{y-X\\phi}) \\right)\n\\]\nTo simplify computation, take logarithm to get the log-likelihood function\n\\[\n\\ell(\\boldsymbol{\\phi},\\boldsymbol{\\Gamma},\\sigma^2|\\boldsymbol{y},\\boldsymbol{y_0}) = -\\frac{T}{2}\\ln(2\\pi) -\\frac{1}{2}\\ln|\\sigma^2\\boldsymbol{\\Gamma\\Gamma'}|-\\frac{1}{2\\sigma^2}(\\boldsymbol{y-X\\phi})' (\\boldsymbol{\\Gamma\\Gamma'})^{-1} (\\boldsymbol{y-X\\phi}).\n\\]\nThe parameters are then chosen to maximize this log-likelihood function, i.e. the probability of observing the data under the assumed distribution. This can be done by conducting a grid search over the parameter space using a computer. To reduce the seach dimensions, we may concentrate the log-likelihood by computing the first-order conditions:\n\\[\n\\frac{\\partial\\ell}{\\partial\\boldsymbol\\phi}=0 \\implies \\boldsymbol{\\hat\\phi} = (\\boldsymbol X'(\\boldsymbol{\\hat\\Gamma\\hat\\Gamma'})^{-1}\\boldsymbol X)^{-1}\\boldsymbol X'(\\boldsymbol{\\hat\\Gamma\\hat\\Gamma'})^{-1}\\boldsymbol y\n\\]\n\\[\n\\frac{\\partial\\ell}{\\partial\\sigma^2}=0 \\implies \\hat\\sigma^2 =\\frac{1}{T} (\\boldsymbol{y-X\\hat\\phi})' (\\boldsymbol{\\hat\\Gamma\\hat\\Gamma'})^{-1} (\\boldsymbol{y-X\\hat\\phi})\n\\]\nThia allows us to focus our search only on \\(\\boldsymbol\\phi\\)."
  },
  {
    "objectID": "forecast.html#intuitive-approach",
    "href": "forecast.html#intuitive-approach",
    "title": "13  Forecasting",
    "section": "13.1 Intuitive Approach",
    "text": "13.1 Intuitive Approach\nSuppose we have an AR(1) process,\n\\[\ny_t = \\phi y_{t-1} + \\epsilon_t, \\quad\\epsilon_t\\sim\\text{WN}(0,\\sigma^2).\n\\]\nWhat would be the reasonable forecast for \\(y_{T+1}\\) given \\(y_1,...,y_T\\)? It seems sensible to simply drop the white noise, as it is something completely unpredictable and it has mean zero. Thus,\n\\[\n\\hat y_{T+1|T} = \\phi y_t.\n\\]\nThis is 1-period ahead forecast. But how do we forecast \\(k\\)-period ahead? Heuristically, we can simply iterate over to the future:\n\\[\n\\begin{aligned}\n\\hat y_{T+2|T} &= \\phi\\hat y_{T+1|T} = \\phi^2 y_T, \\\\\n\\hat y_{T+h|T} &= \\phi\\hat y_{T+h-1} = \\cdots = \\phi^h y_T.\n\\end{aligned}\n\\]\nWe will leave the heuristic solutions here and justify them later. If we accept this heuristic approach, we can easily generalize it to AR(\\(p\\)) processes:\n\\[\n\\begin{aligned}\ny_t &= \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t. \\\\\n\\hat y_{T+1|T} &= \\phi_1 y_{T} + \\phi_2 y_{T-1} + \\dots + \\phi_p y_{T-p+1}, \\\\\n\\hat y_{T+2|T} &= \\phi_1\\hat y_{T+1|T} + \\phi_2 y_{T} + \\dots + \\phi_p y_{T-p+2}, \\\\\n&\\vdots \\\\\n\\hat y_{T+h|T} &= \\phi_1\\hat y_{T+h-1|T} + \\phi_2\\hat y_{T+h-2|T} + \\dots + \\phi_p y_{T-p+h}. \\\\\n\\end{aligned}\n\\]\nFor MA(\\(q\\)) processes\n\\[\ny_t = \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2} + \\cdots + \\theta_q\\epsilon_{t-q}  \n\\] Suppose we know the past innovations until \\(T\\): \\(\\epsilon_{T}, \\epsilon_{T-1}, ...\\) The best way to forecast \\(\\hat y_{T+h|T}\\) looks to simply discard \\(\\epsilon_{T+1},...,\\epsilon_{T+h}\\). Since we have no knowledge about future innovations given the information at time \\(T\\). Therefore,\n\\[\n\\begin{array}{llll}\n\\hat y_{T+1|T} &= \\theta_1\\epsilon_T + &\\theta_2\\epsilon_{T-1} + &\\theta_3\\epsilon_{T-2} + \\cdots \\\\\n\\hat y_{T+2|T} &=  &\\theta_2\\epsilon_T + &\\theta_3\\epsilon_{T-1} + \\cdots \\\\\n&\\vdots & & \\\\\n\\hat y_{T+h|T} &=  & & \\theta_h\\epsilon_T + \\theta_{h+1}\\epsilon_{T-1} + \\cdots\n\\end{array}\n\\]"
  },
  {
    "objectID": "forecast.html#best-linear-predictor",
    "href": "forecast.html#best-linear-predictor",
    "title": "13  Forecasting",
    "section": "13.2 Best Linear Predictor",
    "text": "13.2 Best Linear Predictor\nWe now justify our heuristic solutions by the theory of best linear predictor. Suppose we want to forecast \\(y\\) give the information set \\(X\\).\n\nDefinition 13.1 The best linear predictor (BLP) is defined as\n\\[\n\\mathcal{F}(y|X)=x'\\beta^*\n\\] which is a linear function of \\(X=(x_1,x_2,...,x_p)\\) such that\n\\[\n\\beta^* =\\text{argmin}\\ \\mathbb{E}(y-x'\\beta)^2.\n\\]\n\nTaking first-order condition with respect to \\(\\beta\\) gives\n\\[\n\\beta^* = [\\mathbb{E}(xx')]^{-1} \\mathbb{E}(xy).\n\\]\nTherefore, the BLP is given by\n\\[\n\\hat y = \\mathcal{F}(y|X)=x'\\beta^* = x'[\\mathbb{E}(xx')]^{-1} \\mathbb{E}(xy).\n\\]\nThe prediction error is\n\\[\nr_{y|X} = y - \\hat y = y - x'[\\mathbb{E}(xx')]^{-1} \\mathbb{E}(xy).\n\\]\nThe BLP is the linear projection of \\(y\\) onto \\(X\\). Because \\(\\mathbb{E}[x(y-x'\\beta)]=0\\). The forecast error is orthogonal to \\(X\\).\n\nProposition 13.1 BLP has the following properties:\n\n\\(\\mathcal{F}[ax + by| z_1...z_k] = a\\mathcal{F}[x|z_1...z_k] + b\\mathcal{F}[y|z_1...z_k]\\);\nIf \\(x = a_1z_1 + \\cdots + a_kz_k\\) is already a linear combination of \\(z_1...z_k\\), then \\(\\mathcal{F}[x|z_1...z_k]=x\\);\nIf for all \\(1\\leq j\\leq k\\), \\(\\text{cov}(x,z_j)=\\mathbb{E}(xz_j)=0\\), then \\(\\mathcal{F}[x|z_1...z_k]=0\\)."
  },
  {
    "objectID": "forecast.html#forecasting-with-arma-models",
    "href": "forecast.html#forecasting-with-arma-models",
    "title": "13  Forecasting",
    "section": "13.3 Forecasting with ARMA Models",
    "text": "13.3 Forecasting with ARMA Models\nARMA model is a basic yet powerful tool for forecasting. Given all stationary time series can be approximated by ARMA processes, it makes sense to model a stationary time series with ARMA, and then make forecast based on that model. We will see our heuristic solutions in the first part can be easily justified with the theory of BLP.\n\n13.3.1 Forecasting with AR(p)\nWe have said that, for an AR(\\(p\\)) process\n\\[\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t,\n\\]\nThe one-step-ahead forecast is simply\n\\[\n\\hat y_{T+1|T} = \\phi_1 y_{T} + \\phi_2 y_{T-1} + \\dots + \\phi_p y_{T-p+1}.\n\\]\nThis is the BLP immediately from Property 2 of Proposition 13.1. We can also justify the iterated \\(h\\)-step-ahead forecast by Property 1 (assuming \\(h<p\\)):\n\\[\n\\begin{aligned}\n\\hat y_{T+h|T} &= \\phi_1\\hat y_{T+h-1|T} + \\phi_2\\hat y_{T+h-2|T} + \\cdots + \\phi_h y_{T} + \\dots + \\phi_p y_{T+h-p} \\\\\n&= \\phi_1\\mathcal{F}[y_{T+h-1}|y_T, y_{T-1}...] + \\cdots + \\phi_p\\mathcal{F}[y_{T+h-p}|y_T, y_{T-1}...] \\\\\n&= \\mathcal{F}[\\phi_1 y_{T+h-1} + \\cdots + \\phi_p y_{T+h-p} | y_T,y_{T-1},...] \\\\\n&= \\mathcal{F}[y_{T+h}|y_T,y_{T-1},...]\n\\end{aligned}\n\\]\nThis is assuming all the forecast before \\(h\\) are BLPs, which can be justified recursively. Also note that for the values readily observed: \\(y_T, y_{T-1},...\\) , the BLP is the value itself.\n\n\n13.3.2 Forecasting with MA(q)\nFor the MA(\\(q\\)) process\n\\[\ny_t = \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2} + \\cdots + \\theta_q\\epsilon_{t-q}  \n\\]\nThe BLP for \\(h\\)-step-ahead forecast is (assuming \\(h<q\\))\n\\[\n\\begin{aligned}\n\\hat y_{T+h|T} &= \\mathcal{F}(y_{T+h}|\\epsilon_T,\\epsilon_{T-1}...) \\\\\n&= \\mathcal{F}(\\epsilon_{T+h}|\\epsilon_T,\\epsilon_{T-1}...) + \\theta_1\\mathcal{F}(\\epsilon_{T+h-1}|\\epsilon_T,\\epsilon_{T-1}...) + \\cdots + \\theta_q\\mathcal{F}(\\epsilon_{T+h-q}|\\epsilon_T,\\epsilon_{T-1}...) \\\\\n&= 0 + \\cdots + 0 + \\theta_h\\epsilon_T + \\cdots + \\theta_q\\epsilon_{T+h-q}\n\\end{aligned}\n\\]\nWe make use of Property 3 of Proposition 13.1 with the knowledge that \\(\\text{cov}(\\epsilon_i,\\epsilon_j)=0\\) for \\(i \\neq j\\). This result is also consistent with our intuition, because we have no knowledge of future innovations, the best thing we can do is assuming they are zeros. If \\(h>q\\), then all \\(\\mathcal{F}(\\epsilon_{T+h-q}|\\epsilon_T,\\epsilon_{T-1}...)\\) are zero, which yields \\(\\hat y_{T+h|T} = 0\\).\nIn practice, we do not observe \\(\\{\\epsilon_{t}\\}\\). If we have an estimated MA model and we want to make forecast based on the model, we need to back out \\(\\{\\epsilon_{t}\\}\\) from \\(\\{y_t\\}\\) by inverting the MA process: \\(\\epsilon_t = \\theta^{-1}(L)y_t\\).\nWith the MA specification, we can easily compute the Mean Squared Forecast Error (MSFE) as follows\n\\[\nQ_{T+h|T} = \\mathbb{E}(y_{T+h} - \\hat y_{T+h|T})^2 = \\mathbb{E}\\left(\\sum_{j=0}^{h-1}\\theta_j\\epsilon_{T+h-j}\\right)^2 = \\sigma^2\\sum_{j=0}^{h-1}\\theta_j^2.\n\\]\n\n\n13.3.3 Forecasting with ARMA(p,q)\nConsider the ARMA(\\(p\\), \\(q\\)) process\n\\[\n(1-\\phi_1L-\\cdots-\\phi_pL^p)y_t = (1+\\theta_1L+\\cdots+\\theta_qL^q)\\epsilon_t\n\\]\nWe assume the process is causal and invertible. We can transform it to an AR(\\(\\infty\\)) process or MA(\\(\\infty\\)) process.\nCausal form:\n\\[\ny_t = \\phi^{-1}(L)\\theta(L)\\epsilon_t = \\sum_{j=0}^{\\infty} \\psi_j\\epsilon_{t-j}\n\\]\nInvertible form:\n\\[\n\\epsilon_t = \\theta^{-1}(L)\\phi(L)y_t = \\sum_{j=0}^{\\infty} \\pi_j y_{t-j}\n\\]\nor\n\\[\ny_t = -\\sum_{j=1}^{\\infty} \\pi_j y_{t-j} + \\epsilon_t\n\\]\nAs we have seen so far, it is relatively easier to compute the mean forecast with AR models, and the MSFE with MA models. So we make forecast with the AR representation:\n\\[\n\\hat y_{T+h|T} = -\\sum_{j=1}^{h-1}\\pi_j\\hat y_{T+h-j} - \\sum_{j=h}^{\\infty}\\pi_jy_{T+h-j}\n\\]\nHowever, we do not observe infinite past values in real world. We can only use the truncated values, discarding past values that we do not observe \\(y_0,y_{-1}, y_{-1},...\\)\n\\[\n\\hat y_{T+h|T} = -\\sum_{j=1}^{h-1}\\pi_j\\hat y_{T+h-j} - \\sum_{j=h}^{T+h-1}\\pi_jy_{T+h-j}\n\\]\nWe compute the MSFE with the MA representation:\n\\[\nQ_{T+h|T} = \\mathbb{E}(y_{T+h} - \\hat y_{T+h})^2 =  \\sigma^2\\sum_{j=0}^{h-1}\\psi_j^2\n\\]\nIf we can compute the prediction interval if we assume some probability distributions for the innovations. If we assume \\(\\epsilon_t\\overset{iid}\\sim N(0, \\sigma^2)\\), then \\((y_1,...,y_{T+h})'\\) is jointly normal. Therefore,\n\\[\ny_{T+h} - \\hat y_{T+h|T} \\sim N(0, Q_{T+h|T})\n\\]\nThe prediction interval is thus given by \\(\\hat y_{T+h|T} \\pm z_{\\alpha/2}\\sqrt{Q_{T+h|T}}\\)."
  },
  {
    "objectID": "forecast.html#applications",
    "href": "forecast.html#applications",
    "title": "13  Forecasting",
    "section": "13.4 Applications",
    "text": "13.4 Applications\nThe following examples use ARMA models to forecast inflation rate and stock market index. The parameters of the ARMA models are chosen automatically. We can see for the inflation rate, the model produces some patterns in the forecast. But for the stock market index, the forecast is an uninformative flat line, indicating there is no useful patterns in the past data can be extrapolated by the ARMA model.\n\nlibrary(forecast)\ndata = readRDS(\"data/md.Rds\")\ndata$CPI |>\n  auto.arima() |> \n  forecast(h=20) |>\n  autoplot()\n\n\n\n\nFigure 13.1: ARMA forecast for monthly inflation\n\n\n\n\n\ndata$SHSE |>\n  auto.arima() |> \n  forecast(h=20) |>\n  autoplot()\n\n\n\n\nFigure 13.2: ARMA forecast for stock market index"
  },
  {
    "objectID": "dce.html",
    "href": "dce.html",
    "title": "14  Dynamic Causal Effect",
    "section": "",
    "text": "As in all fields of science, we are perpetually interested in understanding the causal effect of one thing on another. In economics, we want to understand how monetary policy affects output and inflation, how exchange rate affects import and export, and so on. However, causality is something much easier said than done. In reality, there are multiple forces at work simultaneously that leads to the consequences we observed. It is challenging both conceptually and statistically to isolate the causality of a variable of particular interest.\nIn cross-sectional analysis, causality is defined counterfactually. That is, the causal effect of a treatment is defined as the difference between the treated outcome and the untreated outcome assuming that they would be otherwise the same without the treatment. In practice, that involves working with a large number of \\(iid\\) observations that are similar on average only differentiated by the status of the treatment. This approach, however, does not work well with many macroeconomic studies. For example, suppose we want to figure out the causal effect of monetary policy on inflation rate. The cross-sectional approach would entail finding a large number of almost identical countries, each with independent monetary policy. And a random subset of them tighten their monetary policies while others do not. Then we work out the different economic outcomes between these two groups. This is clearly infeasible. The question we posed concerns only one country with inflation and interest rates observed through time. We would need a definition of causal effect that encompasses observations over time not across individuals.\nSuppose \\(\\epsilon_t\\) denote a random treatment happened at time \\(t\\). Then the causal effect on an outcome variable \\(y_{t+h}\\), \\(h\\) periods ahead, of a unit shock in \\(\\epsilon\\) is defined as\n\\[\n\\mathbb{E}[y_{t+h}|\\epsilon_t=1]-\\mathbb{E}[y_{t+h}|\\epsilon_t=0].\n\\tag{14.1}\\]\nWe require the randomness of the treatment \\(\\epsilon_t\\) in a sense that it is uncorrelated with any other variables that could possible have an impact on the outcome. Therefore, \\(\\epsilon_t\\) happens or not does not affect other forces that shape the outcome. The difference in the outcomes is solely attributable to \\(\\epsilon_t\\). It is this randomness that guarantees a causal interpretation.\nOur example of monetary policy above clearly does not meet this requirement. The monetary authority does not set the interest rate randomly, but based on the economic conditions of the time, which makes it correlated with other economic variables that could also have an impact on inflation. A qualified random shock may be a change in weather conditions. Weather has huge impact on agricultural production, but it is determined independent of any human activity. If \\(\\epsilon_t\\) denotes a rainy day at time \\(t\\), and \\(y_{t+h}\\) be the agricultural production, Equation 14.1 could be a plausible causal effect. However, most variables of interest in economics are endogenously determined. How to estimate the causal effect in such cases is an art in itself. We will come back to this point later.\nThe conceptual definition of Equation 14.1 can not be computed directly as the counterfactual is not observed. What we have is a sample of experiments over time, in which the treatment happens randomly at some points but not others, \\(\\{\\epsilon_1=0, \\epsilon_2=1, \\epsilon_3=0,\\dots\\}\\). We could envision that if we have long enough observations, by comparing the outcomes when the shock happens and when it does not, it gives us an reasonable estimation of the causal effect because all other factors that contributing to the outcome, despite they are changing over time, would be averaged out provided the randomness of the treatment.\nAssuming linearity and stationarity, the causal effect of Equation 14.1 can be effectively captured by a regression framework,\n\\[\ny_{t+h} = \\theta_h\\epsilon_t + u_{t+h},\n\\]\nwhere \\(u_{t+h}\\) represents all other factors contributing to the outcome variable. Since \\(\\epsilon_t\\) is random, it holds that \\(\\mathbb{E}(u_{t+h}|\\epsilon_t) = 0\\). Therefore,\n\\[\n\\theta_h = \\mathbb{E}(y_{t+h}|\\epsilon_t=1)-\\mathbb{E}(y_{t+h}|\\epsilon_t=0).\n\\]\nThus, \\(\\theta_h\\) captures the causal effect of one unit shock of \\(\\epsilon_t\\) on \\(y_{t+h}\\). The path of the causal effects mapped out by \\(\\{\\theta_0, \\theta_1, \\theta_2, \\dots\\}\\) is called the dynamic causal effect, in a sense that it is the causal effects through time."
  },
  {
    "objectID": "ssf.html",
    "href": "ssf.html",
    "title": "15  The Structural Shock Framework",
    "section": "",
    "text": "The counterfactual framework introduced in the last section defines the dynamic causal effect of any variable on another. As economists, we are more interested in understanding the causal relationships between important forces that drive the economy. We now introduce the structural shock framework, or the Slutzky-Frisch paradigm. This paradigm is explicitly or implicitly embedded in virtually every mainstream macroeconomic models or econometric models. It is not an essential component of time series analysis. But, as we would like to approach the topic from an economist’s perspective, it is good to have this framework in mind for many of our applications.\nThe structural shock framework envisions our economy as a complex system driven by a set of fundamental structural forces and coordinated by numerous price signals that automatically balance the demand and supply of all goods and services. The structural forces could be technology progress, climate change, policy changes and so on. These structural shocks are the primitive forces underlying our economy. When a structural shock happens, it triggers a reallocation of economic resources guided by market forces. In theoretical works, we are interested in modelling the system as a whole, particularly how resources are allocated optimally by market forces. In empirical works, we are interested how to recover the underlying structural shocks and estimate their causal effect on other economic variables.\nIn the language of time series analysis, we can envision our economy as an MA process, in which the observable variables (output, employment, inflation, etc) are the outcomes of accumulated past and current structural shocks:\n\\[\n\\boldsymbol y_t = \\boldsymbol\\Theta(L) \\boldsymbol\\epsilon_t,\n\\]\nor\n\\[\n\\begin{bmatrix}\ny_{1t}\\\\ y_{2t}\\\\ \\vdots\\\\ y_{nt}\n\\end{bmatrix} =\n\\sum_{j=0}^{\\infty}\n\\begin{bmatrix}\n\\theta_{j,11} & \\theta_{j,12} & \\cdots & \\theta_{j,1m}\\\\\n\\theta_{j,21} & \\theta_{j,22} & \\cdots & \\theta_{j,2m}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\theta_{j,n1} & \\theta_{j,n2} & \\cdots & \\theta_{j,nm}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\epsilon_{1,t-j}\\\\ \\epsilon_{2,t-j}\\\\ \\vdots\\\\ \\epsilon_{m,t-j}\n\\end{bmatrix}.\n\\]\n\\(\\boldsymbol y_t\\) represents the vector of economic variables of concern. The space of \\(\\boldsymbol y_t\\) are spanned by \\(m\\) structural shocks (current and past): \\(\\{\\boldsymbol \\epsilon_{t-j}\\}_{j=0}^{\\infty}\\).\nStructural shocks are conceptual constructions that are primitive, unforeseeable, and uncorrelated underlying forces. Whether structural shocks do exist or not is an open question. But they are useful constructions that enable econometricians to disentangle different driving forces of the outcome variable.\nIn reality, almost every economic variable is endogenous. For example, monetary policy (interest rate) is set by the monetary authority based on their assessment of the economic conditions. But we can also imagine, there is a “genuine” component of the monetary policy, which may come from the personality of the policymaker and his mental conditions when he make the decision, that is not predictable from other variables. This genuine component is what we deem as the “monetary policy shock”. It is a shock in a sense that it is not predictable. It speaks for its own sake and contribute to the economic outcomes independently.\nWe do not observe the structural shocks directly. The observable variables are linear combinations of the structural shocks. For example, we may think of the observed interest rate as a linear combination of the monetary policy shock together with supply-side shocks and others.\n\\[\ni_t = \\theta_{1}(L)\\epsilon_{t}^{\\text{MP}} + \\theta_{2}(L)\\epsilon_{t}^{\\text{SS}} + \\cdots\n\\]\nTherefore, regressing inflation or output on interest rate will not give the causal effect of the monetary policy. Because interest rate does not represent the “genuine” monetary policy shock. It is determined by other economic variables and there are multiple structural forces at work. There are numerous literature that works on methods to isolate the “monetary policy shock” from the observed interest rates. Such way of constructing the structural shocks is not only a conceptual idea, but also a prerequisite for meaningful interpretation of the coefficients of econometric models."
  },
  {
    "objectID": "adl.html#distributed-lags",
    "href": "adl.html#distributed-lags",
    "title": "16  Estimating Dynamic Multipliers",
    "section": "16.1 Distributed Lags",
    "text": "16.1 Distributed Lags\nThe easiest approach to estimate dynamic causal effect is to include lags in the specification:\n\\[\ny_t = \\beta_0\\epsilon_t + \\beta_1\\epsilon_{t-1} + \\cdots + \\beta_p\\epsilon_{t-p} + u_t,\n\\]\nwhere \\(\\epsilon_t\\) is the structural shock, \\(u_t\\) is everything that otherwise influences \\(y_t\\). Since \\(\\epsilon_t\\) happens randomly, we have \\(\\mathbb E(u_t |\\epsilon_{t-j}) = 0\\). Thus, the \\(\\beta\\)s, which capture the dynamic causal effect, would be consistently estimated by OLS.\nNote that we call it a specification, in a sense that the joint distribution of the random variables is unknown, which distinguishes itself from the DGP model in Chapter 6. But it does not stop us from uncovering the causal effect, as long as the exogenous condition holds.\nThe effect of a unit change in \\(\\epsilon\\) on \\(y\\) after \\(h\\) periods, which is \\(\\beta_{h}\\), is also called the \\(h\\)-period dynamic multiplier. Sometimes, we are interested in the accumulated effect over time, \\(\\beta_0+\\beta_1+\\cdots+\\beta_h\\), which is called cumulative dynamic multiplier.\nBecause \\(u_t\\) is the linear combination of all other current and past shocks, it is likely serially correlated. So HAC standard errors are required for robust inferences.\n\nProposition 16.1 Assumptions for a consistent estimation of dynamic causal effects with distributed lag models:\n\n\\(\\epsilon\\) is an exogenous shock, \\(\\mathbb E(u_t|\\epsilon_t,\\epsilon_{t-1},...)=0\\);\nAll variables are stationary;\nRegular conditions for OLS to work.\n\n\nTo reduce the serial correlations \\(\\{u_t\\}\\), and also allow for slow adjustment of \\(y_t\\), we can also include lagged dependent variables in the specification, which becomes an autoregressive distributed lag (ADL) specification:\n\\[\ny_t = \\phi_1 y_{t-1} + \\cdots + \\phi_p y_{t-p} + \\beta_0\\epsilon_t + \\beta_1\\epsilon_{t-1} + \\cdots + \\beta_p\\epsilon_{t-p} + u_t,\n\\]\nor\n\\[\n\\phi(L) y_t = \\beta(L) \\epsilon_t + u_t.\n\\]\nWhen lags of the dependent variable are included as regressors, strict exogeneity fails for sure, because \\(X=\\{y_{t-1},\\dots,\\epsilon_t, \\epsilon_{t-1},\\dots\\}\\) is correlated with past errors \\(u_{t-1}\\), despite it is uncorrelated with the contemporary error \\(u_t\\). The OLS is consistent so long as \\(\\{u_t\\}\\) are not serially correlated. Otherwise, \\(u_t\\) would be correlated with \\(X\\) through \\(u_{t-1}\\). The serial correlation can be tested with Durbin-Watson test or Breusch-Godfrey test.\nThe dynamic causal effect is more convoluted with the ADL specification though,\n\\[\n\\hat\\theta(L) = \\hat\\phi^{-1}(L) \\hat\\beta(L).\n\\]\nADL also require truncated lags. \\(p\\) and \\(q\\) are chosen as an increasing function of the sample size. In general, choosing \\(p\\) and \\(q\\) to be of order \\(T^{1/3}\\) would be sufficient for consistency."
  },
  {
    "objectID": "adl.html#local-projections",
    "href": "adl.html#local-projections",
    "title": "16  Estimating Dynamic Multipliers",
    "section": "16.2 Local Projections",
    "text": "16.2 Local Projections\nDynamic causal effect can also be estimated by projecting future outcomes directly on the shock. Jordà (2005) named it local projections (LP).\n\\[\ny_{t+h} = \\theta_h \\epsilon_t + u_{t+h}.\n\\]\nBy assumption, \\(\\mathbb E(u_{t+h}|\\epsilon_t)=0\\). So \\(\\hat\\theta_h\\) is a consistent estimate of the \\(h\\)-period dynamic multiplier. HAC standard errors are also required in local projections, as \\(u_{t+h}\\) in are usually serially correlated.\nReaders may wonder, since ADL and LP both give consistent estimates of the dynamic multipliers, what is the difference between them. There are two obvious differences:\n\nLagged shocks do not appear in LP specifications as they do in distributed lag specifications.\nThe LP method requires running separate regressions for each \\(h\\). The dynamic response \\(\\{\\theta_0,\\theta_1,\\theta_2, \\dots\\}\\) are estimated through multiple regressions rather than one.\n\nThe error structure is also different. To see this, suppose the DGP is an MA(\\(\\infty\\)) process\n\\[\ny_t = \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2} +\\cdots\n\\]\nIf we estimate it with a DL specification with two lags,\n\\[\ny_t = \\beta_0\\epsilon_t + \\beta_1\\epsilon_{t-1} + u_t,\n\\]\nwhere \\(u_t = \\sum_{j=2}^{\\infty}\\theta_j\\epsilon_{t-j}\\). Exogeneity would ensure \\(\\hat\\beta_1 \\to \\theta_1\\).\nWe can also estimate it with a local projection (suppose we are interested in the one-step-ahead dynamic multiplier):\n\\[\ny_{t+1} = \\psi_1\\epsilon_t + u_{t+1}.\n\\]\nAgain, we have consistency \\(\\hat\\psi_1\\to\\theta_1\\). But the error structure is different \\(u_{t+1} = \\epsilon_{t+1} + \\sum_{j=2}^{\\infty}\\theta_j\\epsilon_{t-j}\\).\nBoth the DL and LP specifications may include additional control variables, which can reduce the variance of the residuals and improve the efficiency of the estimators."
  },
  {
    "objectID": "adl.html#example-of-observable-exogenous-shocks",
    "href": "adl.html#example-of-observable-exogenous-shocks",
    "title": "16  Estimating Dynamic Multipliers",
    "section": "16.3 Example of Observable Exogenous Shocks",
    "text": "16.3 Example of Observable Exogenous Shocks\nDirectly observable exogenous shocks are rare. Here we use an example from Stock and Watson (2020), which explores the dynamic causal effect of cold weather on orange juice prices. Cold weather is bad for orange production. Orange trees cannot withstand freezing temperatures that last for more than a few hours. Florida accounts for more than 98 percent of U.S. production of frozen concentrated orange juice. Therefore, the frozen weather in Florida would reduce the supply and orange juice and raise the price. The dataset includes the number of freezing degree days in Florida and the average producer price for orange juice. Cold weather is plausibly exogenous, which allows us the utilize the regression framework above to estimate the dynamic causal effect.\n\nlibrary(AER)\nlibrary(dynlm)\nlibrary(lmtest)\n\ndata(\"FrozenJuice\") # load data\n\n# compute percentage change on price\npchg = 100*diff(log(FrozenJuice[, 'price']))\nsample = ts.union(fdd = FrozenJuice[,'fdd'], pchg)\n\n# distributed lag model\nmod = dynlm(pchg ~ L(fdd, 0:6), data = sample)\n# confidence interval\nci = coefci(mod, vcov. = NeweyWest)\n\n# plot dynamic multiplier\n{\n  plot(mod$coefficients[-1], # remove intercept\n       type = \"l\", \n       col = 2, \n       ylim = c(-0.4,1), \n       xlab = \"Lag\", \n       ylab = \"Dynamic Multiplier\")\n  abline(h = 0, lty = 2)\n  lines(ci[-1,1], col = 4)\n  lines(ci[-1,2], col = 4)\n}\n\n\n\n\nFigure 16.1: Dynamic Effect of Freezing Days on Orange Juice Price\n\n\n\n\nWe can also use local projections. Note that local projections require estimating multiple regressions. The coefficients from each of the regressions constitute the dynamic multiplier.\n\n# apply local projection for horizons 0-6\nlps = sapply(0:6, function(h) {\n  lp = dynlm(L(pchg, -h) ~ fdd, data = sample)\n  ci = coefci(lp, vcov. = NeweyWest)\n  c(lp$coefficients[-1], ci[-1,]) # remove intercept\n}) |> t() # transpose it\n\n# plot the LP coefficients\n{\n  plot(lps[,'fdd'], \n       type = \"l\", \n       col = 2, \n       ylim = c(-0.4,1), \n       xlab = \"Horizon\", \n       ylab = \"LP Coefficient\")\n  abline(h = 0, lty = 2)\n  lines(lps[,'2.5 %'], col = 4)\n  lines(lps[,'97.5 %'], col = 4)\n}\n\n\n\n\nFigure 16.2: Local Projections of Freezing Days on Orange Juice Price"
  },
  {
    "objectID": "adl.html#example-of-constructed-structural-shocks",
    "href": "adl.html#example-of-constructed-structural-shocks",
    "title": "16  Estimating Dynamic Multipliers",
    "section": "16.4 Example of Constructed Structural Shocks",
    "text": "16.4 Example of Constructed Structural Shocks\nMost structural shocks in economics are not directly observed, such as monetary policy shocks, or fiscal policy shocks, yet they are of profound interest of researchers. As we have explained before, regressing output or inflation on interest rate does not give a plausible estimation of the causal effect of monetary policy, due to the endogeneity problem. Thus, we need to isolate the exogenous part of the monetary policy from observed variables. The method to achieve this is an active research field in itself. We here demonstrate the monetary policy shock for China constructed by Das and Song (2023).\nThe authors utilize the high-frequency price changes of interest rate swap around the window of monetary policy announcement to approximate the monetary policy shock. The rationale of this construction is that, the price of the financial instrument reflects the expected interest rate by market participants based on the economic conditions. Therefore, the sudden change of the price in the tiny window of monetary policy announcement captures the unexpected part of the monetary policy.\n\nlibrary(zoo)\n\nmp = readRDS(\"data/mpshocks.Rds\") # monetary policy shock\nmd = readRDS(\"data/md.Rds\") # monthly data\n\nplot(na.exclude(cbind(mp$shock_1y, md$LoanRate1Y)), \n     main=\"\", xlab=\"\", ylab=c(\"Loan Rate\", \"MP Shock\"))\n\n\n\n\nFigure 16.3: Monetary Policy Shock and Lending Rate\n\n\n\n\nWe estimate the dynamic causal effect of monetary policy shock on inflation using the constructed MP shocks. It shows that a tightening of monetary policy implies a gradual cooling down of inflation. The price level starts to decline roughly half a year after the initial tightening shock. However, the confidence interval is wide, suggesting an insignificant estimation of the policy effect. The result does not provide very strong evidence underlining the effectiveness of monetary policy to control inflation.\n\nsample = na.exclude(cbind(cpi=md$CPI, mp))\n\n# distributed lag model\nmod = dynlm(cpi ~ L(shock_1y, 0:12), sample)\n# confidence interval\nci = coefci(mod, vcov. = NeweyWest)\n\n# plot dynamic multiplier\n{\n  plot(mod$coefficients[-1], # remove intercept\n       type = \"l\", \n       col = 2, \n       ylim = c(-1,1.5), \n       xlab = \"Lag\", \n       ylab = \"Dynamic Multiplier\")\n  abline(h = 0, lty = 2)\n  lines(ci[-1,1], col = 4)\n  lines(ci[-1,2], col = 4)\n}\n\n\n\n\nFigure 16.4: Dynamic response of inflation on monetary policy shocks\n\n\n\n\n\n\n\n\nStock, James H, and Mark W Watson. 2020. Introduction to Econometrics. Pearson."
  },
  {
    "objectID": "lpiv.html",
    "href": "lpiv.html",
    "title": "17  Instrument Variables",
    "section": "",
    "text": "If a structural shock is not directly observable, neither can it be constructed through observable variables, we can identify it using an instrument variable approach if an instrument is available.\nSuppose our observable space \\(\\boldsymbol y=(y_1,y_2,\\dots)'\\) is spanned by multiple structural shocks \\(\\epsilon = (\\epsilon_1,\\epsilon_2,\\dots)'\\). We want to identify the causal effect of structural shock \\(\\epsilon_1\\). An instrument variable \\(z\\) satisfies the following conditions:\n\n\\(\\mathbb E(\\epsilon_{1t}z_t) =\\alpha\\neq 0\\) (relevance);\n\\(\\mathbb E(\\epsilon_{2:n}z_t) = 0\\) (contemporaneous exogeneity);\n\\(\\mathbb E(\\boldsymbol\\epsilon_{t+j}z_t) = 0\\) for \\(j\\neq 0\\) (lead-lag exogeneity).\n\n\\(\\epsilon_{2:n}\\) denotes all other structural shocks except \\(\\epsilon_1\\). The lead-lag exogeneity is unique to time series. To understand this, consider an local projection: \\(y_{t+h} = \\theta_h\\epsilon_t +u_{t+h}\\). As illustrated in the last section, \\(u_{t+h}\\) is a linear combination of the entire history of structural shocks. If \\(z_t\\) is to identify the causal effect of shock \\(\\epsilon_{1t}\\) alone, it must be uncorrelated with all leads and lags. The requirement that \\(z_t\\) be uncorrelated with future \\(\\epsilon\\)’s is generally not restrictive — by definition, future shocks are unanticipated. To the contrary, the requirement that \\(z_t\\) be uncorrelated with past \\(\\epsilon\\)’s is more restrictive and hard to meet.\nSuppose we want to estimate the causal effect of \\(\\epsilon_{1,t}\\) on \\(y_{2,t+h}\\), where \\(\\epsilon_{1,t}\\) is only observable through \\(y_{1,t}\\). Suppose we have an instrument variable \\(z_t\\) that satisfies the above conditions. The local projection\n\\[\ny_{2,t+h} = \\theta_{h,21} y_{1,t} + u_{t+h}\n\\]\ncannot be consistently estimated because \\(y_{1,t}\\) and \\(u_{t+h}\\) are correlated. However, with the help with \\(z_t\\) as an instrument, we can consistently estimate the dynamic multiplier \\(\\theta_{h,21}\\):\n\\[\n\\begin{aligned}\n\\beta_{\\text{LP-IV}} &= \\frac{\\mathbb E(y_{2,t+h}z_t)}{\\mathbb E(y_{1,t}z_t)}\\\\\n&= \\frac{\\mathbb E[(\\theta_{h,21}y_{1,t} + u_{t+h})z_t]}{\\mathbb E(y_{1,t}z_t)}\\\\\n&= \\frac{\\theta_{h,21}\\alpha}{\\alpha} = \\theta_{h,21}.\n\\end{aligned}\n\\]\nLead-lag exogeneity implies \\(z_t\\) being unforecastable in a regression of \\(z_t\\) on lags of \\(y_t\\). If the exogeneity fails, LP-IV is not consistent. This problem can be partially addressed by including control variables in the regression:\n\\[\ny_{2,t+h} = \\theta_{h,21} y_{1,t} + \\boldsymbol{\\gamma_h'w_t} + u_{t+h}^{\\perp}.\n\\]\nWe could also include lagged values of \\(y_t\\) or other lagged variables. The IV estimator is consistent if \\(\\boldsymbol w_t\\) absorbs all past shocks that could potentially correlated with \\(z_t\\). In a broad sense, the validity of the instrument variable with additional controls requires that the controls span the space of all structural shocks."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Epilogue",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Stock, James H, and Mark W Watson. 2020. Introduction to\nEconometrics. Pearson."
  }
]