# Granger Causality

## Definition

Granger causality conceptualizes the usefulness of some variables in
forecasting other variables.

::: {#def-gc}
## Granger Causality

$x$ fails to Granger-cause $y$ if for all $s >0$, the MSE of a forecast
$\hat y_{t+s}$ based on $(y_t,y_{t-1},...x_t,x_{t-1},...)$ is the same
as the MSE of the forecast based on $(y_t,y_{t-1},...)$:

$$
\text{MSE}[\hat y_{t+s}|y_t,y_{t-1},...] = 
\text{MSE}[\hat y_{t+s}|y_t,y_{t-1},...x_t,x_{t-1},...].
$$

Equivalently, we say $x$ is exogenous to $y$, or $x$ is not linearly
informative about $y$.
:::

It must be noted that Granger causality has nothing to do with the
causality defined by counterfactuals. The word "causality" is
unfortunately misleading. It would be better named as "Granger
predictability".

## Granger Causality Test

Consider the Granger causality test in a single equation setup:

$$
y_t = \alpha + \phi_1 y_{t-1}+\cdots+\phi_p y_{t-p} + 
\beta_1 x_{t-1} + \cdots +\beta_q x_{t-q} + u_t
$$

Testing whether $x$ Granger-causes $y$ is equivalent to test

$$
H_0: \beta_1 = \beta_2=\cdots=\beta_q=0
$$

The test can be done by comparing the residual sum of squares (RSS) with
and without $x$ as the regressors. Assuming the $H_0$ holds, we would
have the restricted model:

$$
y_t = \alpha + \phi_1 y_{t-1} + \cdots + \phi_p y_{t-p} + v_t
$$

Compute RSS for the restricted model:

$$
\text{RSS}_0 = \sum_{t=1}^{T} \hat v_t^2
$$

Also compute the RSS for the unrestricted model, i.e. including all $x$
as the regressors:

$$
\text{RSS}_1 = \sum_{t=1}^{T} \hat u_t^2
$$

The joint significance can be tested by the $F$ ratio:

$$
S = \frac{(\text{RSS}_0 - \text{RSS}_1)/p}{\text{RSS}_1/(T-2p-1)} \sim F(p, T-2p-1).
$$

## Granger Causality in VAR

In a VAR setting, $x$ does not Granger-cause $y$ if the coefficient
matrix are lower triangular for all $j$:

$$
\begin{aligned}
\begin{bmatrix}y_t \\ x_t\end{bmatrix} &= 
\begin{bmatrix}\alpha_1 \\ \alpha_2\end{bmatrix} +
\begin{bmatrix}\phi_{1,11} & 0 \\ \phi_{1,21} & \phi_{1,22} \end{bmatrix}
\begin{bmatrix}y_{t-1} \\ x_{t-1}\end{bmatrix} + 
\begin{bmatrix}\phi_{2,11} & 0 \\ \phi_{2,21} & \phi_{2,22}\end{bmatrix}
\begin{bmatrix}y_{t-2} \\ x_{t-2}\end{bmatrix} + \cdots \\[1em]
&= 
\begin{bmatrix}\alpha_1 \\ \alpha_2\end{bmatrix} +
\begin{bmatrix}\phi_{11}(L) & 0 \\ \phi_{21}(L) & \phi_{22}(L)\end{bmatrix}
\begin{bmatrix}y_{t-1} \\ x_{t-1} \end{bmatrix} +
\begin{bmatrix}\epsilon_{1t} \\ \epsilon_{2t} \end{bmatrix}
\end{aligned}
$$

In the MA representation,

$$
\begin{bmatrix}y_t \\ x_t\end{bmatrix} = 
\begin{bmatrix}\mu_1 \\ \mu_2\end{bmatrix} +
\begin{bmatrix}\theta_{11}(L) & 0 \\ \theta_{21}(L) & \theta_{22}(L)\end{bmatrix}
\begin{bmatrix}\epsilon_{1t} \\ \epsilon_{2t}\end{bmatrix}
$$

To test Granger causality in a VAR setting, we need to introduce the
likelihood ratio (LR) test.

## Likelihood Ratio Test

Let's quickly derive the maximum likelihood estimator for the VAR. The
joint probability density function is

$$
\begin{aligned}
f(y_T,y_{T-1},\dots,y_1 | y_0,\dots,y_{1-p};\theta)
&= f(y_T|y_{T-1}\dots)f(y_{T-1}|y_{T-2}\dots)\cdots f(y_1|y_0\dots) \\[1em]
&= \prod_{t=1}^{T} f(y_t|y_{t-1},y_{t-2},... ,y_{1-p};\theta)
\end{aligned}
$$

Define $x_t'=\begin{bmatrix}1 & y_{t-1} & \dots & y_{t-p}\end{bmatrix}$
the collection of regressors, and
$B'=\begin{bmatrix}\alpha & \Phi_1 & \dots & \Phi_p\end{bmatrix}$ the
collection of parameters. The log-likelihood function can be written as

$$
\begin{aligned}
\ell(\theta) &= \sum_{t=1}^T \log f(y_t|y_{t-1},y_{t-2},... ,y_{1-p};\theta) \\
&= -\frac{T}{2}\log |2\pi\Omega^{-1}| - \frac{1}{2}\sum_{t=1}^{T}[(y_t - B'x_t)'\Omega^{-1}(y_t-B'x_t)]
\end{aligned}
$$

where $\Omega$ is the variance-covariance matrix of the residuals.
Maximizing the log-likelihood function gives the ML estimator:

$$
\begin{aligned}
\hat B'_{n \times (np+1)} &= 
\left[\sum_{t=1}^{T} y_tx_t'\right]\left[\sum_{t=1}^{T} x_tx_t'\right]^{-1} \\
\hat\Omega_{n \times n} &= \frac{1}{T} \sum_{t=1}^{T}\hat u_t \hat u_t'
\end{aligned}
$$

The likelihood ratio test is motivated by the fact that different
specifications give different likelihood values. By comparing the
likelihood difference, we can test one VAR specification versus the
alternative. The null hypothesis ($H_0$) could be a specification with a
particular lag length, or a particularization with certain exogeneity
restrictions.

Then we estimate the covariance matrix under the null hypothesis ($H_0$)
and the alternative ($H_1$):

$$
\begin{aligned}
\hat\Omega_0 &= \frac{1}{T} \sum_t \hat\epsilon_t(H_0) \hat\epsilon_t(H_0)' \\
\hat\Omega_1 &= \frac{1}{T} \sum_t \hat\epsilon_t(H_1) \hat\epsilon_t(H_1)' \\
\end{aligned}
$$

The corresponding likelihood ratios are

$$
\begin{aligned}
\ell_0^* &= -\frac{T}{2} \log |2\pi\hat\Omega_0^{-1}| - \frac{Tn}{2} \\
\ell_1^* &= -\frac{T}{2} \log |2\pi\hat\Omega_1^{-1}| - \frac{Tn}{2} \\
\end{aligned}
$$

The LR statistics

$$
2(\ell_1^* - \ell_0^*) = T(\log |\hat\Omega_0| - \log |\hat\Omega_1|)
$$

has a $\chi^2$ distribution with degree of freedom $n^2(p_1 - p_0)$.

## Multivariate

In a multivariate setting,

$$
\begin{bmatrix}x_t \\ y_t\end{bmatrix} = 
\begin{bmatrix}c_1 \\ c_2\end{bmatrix} +
\begin{bmatrix}A_1(L) & A_2(L) \\ B_1(L) & B_2(L)\end{bmatrix}
\begin{bmatrix}x_t \\ y_t\end{bmatrix} +
\begin{bmatrix}u_{1,t} \\ u_{2,t}\end{bmatrix} 
$$

Testing $x$ fails to Granger-cause $y$ is equivalent to test $A_2=0$.
The restricted regression under $H_0$ is

$$
x_t = c_1 + A_1(L) x_t + u_{1,t}^{R}
$$

The unrestricted regression is

$$
x_t = c_1 + A_1(L) x_t + A_2(L) y_t + u_{1,t}^{U}
$$

Estimate the variance-covariance matrix

$$
\begin{aligned}
\hat\Omega^U &= \frac{1}{T}\sum_t u_t^U {u_t^U}' \\
\hat\Omega^R &= \frac{1}{T}\sum_t u_t^R {u_t^R}' \\
\end{aligned}
$$

Form the LR test statistics

$$
\text{LR} = T(\log |\hat\Omega^U| - \log |\hat\Omega^R|) \sim \chi^2.
$$
