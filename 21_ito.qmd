# Brownian Motion

## Continuous random walk

**Brownian motion**, or **Wiener process**, is the continuous-time
extension of a discrete-time random walk. To define the continuous
version of a random walk, we cannot simply sum up infinite number of
white noises, which will certainly explode. Instead, we chop up a finite
interval into infinitely many small intervals, each one corresponding to
a tiny Gaussian white noise. The following table shows how a discrete
random walk extends to a continuous function.

<!-- WARNING: switch to visual model breaks the table -->

|                     | Random Walk                                 | Brownian Motion                                                 |
|-------------------|-----------------------|------------------------------|
| Innovation          | $\epsilon_i\sim\text{WN}(0,1)$              | ![](fig/epsilon.png){width="200px"}                             |
| Stochastic process  | $y_t = \epsilon_1 + \cdots + \epsilon_t$    | $W(t)_{t\in[0,1]} = \lim_{n\to\infty}\sum_{i=1}^{nt}\epsilon_i$ |
| Expectation         | $\mathbb E[y_t]=0$                          | $\mathbb E[W(t)] = 0$                                           |
| Variance            | $\text{Var}[y_t]=t\text{Var}[\epsilon_i]=t$ | $\text{Var}[W(t)]=nt\text{Var}[\epsilon_i]=t$                   |
| Quadratic variation | $\mathbb E\sum_{i=1}^{t}(y_i-y_{i-1})^2=t$  | $\int_0^t(dW)^2 = t$                                            |

: Random Walk and Brownian Motion

Brownian motion $W(t)$ is a stochastic function. Its realized path is
different each time we take a draw from it. But every piece of it
follows a tiny Gaussian process. **The function is continuous, but
nowhere differentiable.** It is hard to imagine such a function at first
glance. But as we proceed, we will appreciate its amazing properties.
Despite its path is random, the area under the curve integrates to a
well-defined probability distribution. The squared changes (quadratic
variation) even sum up to a deterministic constant.

```{r}
#| echo: false
#| fig-cap: "Multiple Realizations of Brownian Motion"
#| label: fig-simbm
N = 1000
set.seed(1)
brownian = function(r, n=1000) {
  cumsum(rnorm(n*r,0,1/n))
}
# plot multiple realizations of Brownian motion
{
  x = seq(0,.999,1/N)
  plot(x, brownian(1), type = "l", ylim = c(-.1,.1))
  for(i in 1:20) 
    lines(x, brownian(1), col = rgb(0,0,0,runif(1,0,.3)))
}
```

## Some properties

The **quadratic variation** is one of the most important properties of
Brownian motions. Intuitively, it says the squared tiny changes sum up
to a constant with probability $1$ no matter which realized path it
takes. Smooth functions will not have such property, because $(dX)^2$
diminishes much faster than $dX$, surely we get $\int_0^t(dX)^2 =0$.
Because Brownian motion fluctuations too much, the squared changes do
not diminish away. To see this, imagine summing up the squares of
infinitely many small increments up to $t$:

$$
\lim_{n\to\infty}\sum_{i=1}^{nt}\left[ 
  X\left(\frac{i}{n}\right) - X\left(\frac{i-1}{n}\right)
  \right]^2 = \lim_{n\to\infty}\sum_{i=1}^{nt}\epsilon_i^2,
$$

with $\epsilon_i\sim N\left(0,\frac{1}{n}\right)$. Therefore,
$\mathbb E(\epsilon_i^2)=\frac{1}{n}$. Let $z_i = \epsilon_i^2$. The sum
above can be rewritten as

$$
\sum_{i=1}^{nt} z_i = nt\left(\frac{1}{nt}\sum_{i=1}^{nt} z_i\right) 
\overset{\text{LLN}}\to nt\mathbb{E}(z_i) = t.
$$

The integral version is the quadratic variation

$$
\int_0^t(dW)^2 = t,
$$ {#eq-quad2}

or written in differential form

$$
(dW)^2 = dt.
$$ {#eq-quad}

It should be stressed, $W$ is not differentiable. We use the notation
$dW$, but it is not the same as conventional differentials. The calculus
for Brownian motions, the Itô calculus, which will be introduced below,
is a different class of calculus specifically designed for stochastic
functions. Before we get to that, let's first have a look at some
additional properties of Brownian motions.

By Central Limit Theorem, it holds that

$$
\frac{1}{\sqrt{nt}} W(t) = \sqrt{nt}\frac{1}{nt}\sum_{i=1}^{nt}\epsilon_i
\to N\left(0,\frac{1}{n}\right)
$$

Therefore,

$$
W(t) \sim \sqrt{nt} N\left(0,\frac{1}{n}\right) \sim N(0,t);
$$ {#eq-brnorm}

It follows that, for any $r<s$,

$$
W(s) - W(t) \sim N(0, s-t);
$$ {#eq-brdiff}

As $s$ and $t$ become arbitrarily close, we have

$$
dW \sim N(0, dt).
$$ {#eq-brdr}

In essence, Brownian motion is the accumulation of tiny independent
Gaussian innovations. We give the formal definition of Brownian motions
below.

::: {.callout-important icon="false"}
## Brownian motion

A Brownian motion (Wiener process) is a stochastic function $W(t)$ such
that

1.  $W(0)=0$;
2.  For $0 \leq t \leq s \leq 1$, $W(s)-W(t) \sim N(0, s-t)$;
3.  For any realization, $W(t)$ is continuous in $t$.
:::

## Itô calculus

::: {#thm-ito1}
Let $F(W)$ be a "smooth" function of a Brownian motion $W(t)$. Then

$$
dF = F'dW + \frac{1}{2}F''dt.
$$
:::

::: proof
For an informal proof, it immediately follows from the Taylor expansion

$$
F(W(t+h))-F(W(t))=F'(W(t+h)-W(t))+\frac{1}{2}F''(W(t+h)-W(t))^2+\cdots
$$

As $h\to 0$,

$$
dF = F'dW + \frac{1}{2}F''(dW)^2.
$$

By @eq-quad, $(dW)^2= dt$. Therefore,

$$
dF = F'dW + \frac{1}{2}F''dt.
$$
:::

Note that how this differs from the differential formula for normal
functions: $dF = F'dW$. The second-order term does not disappear
precisely because the quadratic variation does not go to zero.

::: {#exm-1}
$F(W) = W^2 \implies dF = 2WdW + dt$
:::

An extension of @thm-ito1 include an additional variable $t$. It follows
immediately from the the partial differentiation rule and the result
above.

::: {#thm-ito2}
Let $F(t, W)$ be a function of real-valued $t$ and Brownian motion
$W(t)$. Then

$$
dF = (F_t + F_{ww}) dt + F_w dW
$$

where $F_t$ and $F_w$ denote partial derivatives.
:::

::: {#exm-2}
$F(t, W)=tW \implies d(tW) = W dt + t dW$
:::

Stochastic integrals are defined as the reverse operation of the
stochastic differentiation. The following rules are immediately from the
fundamental theorem of calculus assuming $W(0)=0$. Note that we change
$W(t)$ to $W(s)$ when it enters as the integrand.

::: {#thm-itoint}
1.  $\int_{0}^{t} dW = W(t)$;
2.  $F(W(t))=\int_0^t f(W(s))dW$ , if $dF = f\ dW$;
3.  $F(t, W(t)) = \int_0^t f(s, W(s))dW + \int_0^t g(s, W(s))ds$ , if
    $dF = f\ dW + g\ dt$.
:::

::: {#exm-3}
Given $dW^2 = 2W dW + dt$, take integral on both sides:

$$
W^2(t) = 2\int_0^t W(s) dW + \int_0^t ds
$$

$$
\implies \int_0^t W dW = \frac{1}{2}[W^2(t) - t].
$$

Setting $t=1$, it follows that

$$
\int_0^1 W dW = \frac{1}{2}[W^2(1)-1].
$$

Note that $W(1) \sim N(0,1)$. So $W^2(1) \sim \chi^2(1)$ with
expectation 1. Thus $\int_0^1 W dW$ is centered at 0 but skewed.
:::

::: {#exm-4}
Given $d(tW) = Wdt + t\ dW$, we have

$$
tW(t) = \int_0^t W(s)ds + \int_0^t s\ dW
$$

$$
\begin{aligned}
\implies \int_0^t W(s)ds &= tW(t) - \int_0^t s\ dW \\
&=t\int_0^t dW - \int_0^t s\ dW \\
&=\int_0^t (t-s) dW.
\end{aligned}
$$
:::

::: {.callout-tip icon="false"}
## Making Sense of Itô Calculus

Let $F(t)$ be a continuous-time trading strategy that holds an amount of
$F(t)$ of a stock at time $t$. The stock price is a Brownian motion
$W(t)$. $dW$ represents the movement of stock price. Consider the Itô's
integral

$$
Y_t = \int_0^t F dW
$$

The stochastic integral represents the payoff of the trading strategy up
to time $t$. Note that the integral always evaluates *at the left*. So
the strategy can only make decision based on available information.
:::

::: {#thm-isometry}
Let $W(t)$ be a Brownian motion. Let $f(t)$ be a nonrandom function of
time. Then the following holds

1.  $\mathbb{E}\left[\int_0^t f(s)dW\right]=0$;
2.  $\mathbb{E}\left[\left(\int_0^t f(s)dW\right)^2\right]=\mathbb{E}\left[\int_0^t f^2 ds\right]$
    (Itô isometry);
3.  $\int_0^t f(s) dW \sim N\left(0, \int_0^t f^2 ds\right)$.
:::

If $f(t)$ represents a trading strategy and the stock price follows a
Brownian motion, the theorem tells us the expected payoff of this
strategy is zero; more precisely, the payoff follows a Gaussian
distribution.

::: {#exm-5}
Following the last example,

$$
\int_0^t W ds = \int_0^t (t-s) dW
$$

$f(s) = t-s$ is a nonrandom function, apply the theorem above

$$
\text{var}\left[\int_0^t W ds\right]=\int_0^t (t-s)^2ds = \frac{1}{3}t^3
$$

Therefore,

$$
\int_0^t W(s) ds \sim N\left(0, \frac{1}{3}t^3\right).
$$

Thus, the integral, the area under the curve of a Brownian motion,
follows a Gaussian distribution.
:::

## Unit root process

Consider a unit root process,

$$
y_t = y_{t-1} + \epsilon_t = \sum_{j=1}^{t}\epsilon_j,
$$

where $\epsilon_t\sim\text{WN}(0,1)$, $t=1,2,\dots,T$. It is not
surprising to see the unit root process converges to Brownian motion if
stabilizing it by $T^{-1/2}$ :

$$
\begin{aligned}
\frac{1}{\sqrt{T}}y_t &= \frac{1}{\sqrt{T}}\sum_{j=1}^{t}\epsilon_j \\
&= \frac{1}{\sqrt{T}}\sum_{j=1}^{Tr}\epsilon_j \quad(r=t/T) \\
&= \sqrt{r}\left(\frac{1}{\sqrt{Tr}}\sum_{j=1}^{Tr}\epsilon_j\right) \\[1em]
&\to \sqrt{r} N(0, 1) \sim N(0,r) \quad(\text{by CLT}) \\[1em]
&\to W(r) \quad(\text{by definition}).
\end{aligned}
$$

If $\epsilon_t$ has variance $\sigma^2$, we would have
$T^{-1/2}y_t \to \sigma W(t/T)$. Note if $y_t$ is stationary, $y_t$ will
not deviate too far from $\mathbb E(y_t)$, we would have
$T^{-1/2} y_t \to 0$.

Now let's consider the behaviour of the mean:
$\bar y=\frac{1}{T}\sum_{t=1}^T y_t$. Define $\xi_T(r)=T^{-1/2}y_t$ ,
where $r=t/T$. We have

$$
\begin{aligned}
\frac{1}{\sqrt T} \bar{y} &= \frac{1}{\sqrt T}\left(\frac{1}{T}\sum_{t=1}^T y_t\right) \\
&= \frac{1}{T}\sum_{t=1}^T \left(\frac{1}{\sqrt T}y_t\right)=\frac{1}{T}\sum_{t=1}^T \xi\left(\frac{t}{T}\right) \\
&= \Delta r \sum_{r=0}^{1} \xi(r) \quad (r=t/T, \Delta r = 1/T) \\
&\to \int_0^1 \sigma W(r) dr \quad (\Delta r \to 0)
\end{aligned}
$$Remember for stationary process, we would have
$\bar y \to \mathbb E(y_t)$ . With unit root process, the mean no longer
converges to a constant, but to a distribution
$\int_0^1 W(r)dr\sim N(0,\frac{1}{3})$.

For higher orders of $y_t$, we have

$$
\begin{aligned}
\frac{1}{T^2}\sum_{t=1}^{T}y_t^2 &= \frac{1}{T}\sum_{t=1}^{T}\left(\frac{1}{\sqrt T}y_t\right)^2 \\
&= \frac{1}{T}\sum_{t=1}^{T}\left[\xi\left(\frac{t}{T}\right)\right]^2 \\
&= \sum_{r=0}^{1} [\xi(r)]^2 \Delta r \\
&\to \int_0^1 \sigma^2 W^2(r) dr
\end{aligned}
$$

By continuous mapping theorem, it can be shown, in general

$$
\frac{1}{T^{1+k/2}}\sum_{i=1}^T y_t^k \to \sigma^k\int_0^1 W^k(r)dr.
$$

Consider the numerator of the OLS estimator of the unit root process,

$$
\begin{aligned}
\sum_{t=1}^{T}y_{t-1}\epsilon_t &= \sum_{i=1}^{T} (\epsilon_1+\cdots+\epsilon_{t-1})\epsilon_t = \sum_{s<t}^{T}\epsilon_s\epsilon_t \\
&= \frac{1}{2}\left[(\epsilon_1+\cdots+\epsilon_T)^2-\sum_{t=1}^{T}\epsilon_t^2\right] \\
&= \frac{1}{2}y_T^2 - \frac{1}{2}\sum_{t=1}^{T}\epsilon_t^2
\end{aligned}
$$

Divide it by $T$, we have

$$
\begin{aligned}
\frac{1}{T}\sum_{t=1}^{T}y_{t-1}\epsilon_t 
&= \frac{1}{2}\left(\frac{1}{\sqrt T}y_T\right)^2 - \frac{1}{2T}\sum_{t=1}^{T}\epsilon_t^2 \\
&= \frac{1}{2}[\xi^2(1)-\hat\sigma^2] \\
&\to \frac{1}{2}[\sigma^2 W^2(1) - \sigma^2] \\
&= \frac{1}{2}\sigma^2 [ W^2(1) - 1] \\
&= \sigma^2 \int_0^1 W dW.
\end{aligned}
$$

We summarize the important results below.

::: {.callout-important icon="false"}
## Key Rules Summary

1.  $\int_0^T W dt = N(0, \frac{T^3}{3})$
2.  $\int_0^T W dW = \frac{1}{2}[ W^2(T) - T]$
3.  $\frac{1}{T^{3/2}}\sum_{t=1}^T y_t \to \sigma \int_0^1 W(r)dr$
4.  $\frac{1}{T^{1+k/2}}\sum_{i=1}^T y_t^k \to \sigma^k\int_0^1 W^k(r)dr$
5.  $\frac{1}{T}\sum_{t=1}^{T}y_{t-1}\epsilon_t \to \sigma^2\int_0^1 WdW$
6.  $\frac{1}{T^2}\sum_{t=1}^{T}y_{1t}y_{2t} \to \sigma_1\sigma_2\int_0^1 W_1(r)W_2(r)dr$
:::

The last rule was given without proof, as we will need it in the
following chapters.
