# Introduction to Bayes

We have completed our venture on classical time series topics covering
univariate and multivariate, stationary and non-stationary models. This
chapter taps into the Bayesian approach to time series analysis, which
is not an essential component for classical treatment of the subject.
But given its rising popularity and importance, it is an exciting topic
that cannot be missed. Bayesian statistics is a whole new world for
frequentist statisticians. We introduce the Bayesian approach with an
example.

## The Sunrise Problem

What is the probability that the sun will rise tomorrow?

We do not consider the physics here. Suppose we want to answer this
question by purely statistics. What we need to do is to observe how many
days the sun had risen in the past, and make some inference about the
future. If we had collect the data on the past $n$ days, we would have
observed the sun had risen everyday for sure (the sun rises even in
cloudy or rainy days). If we want to calculate the probability of a
sunrise event, denoted by $A$, the frequentist approach would give
$P(A) = \frac{n}{n} = 1$. The probability is always $1$ no matter how
many observations we have. That's a bit quirky, even though we haven't
looked at the confidence interval. The 100 percent probability does not
sound correct, as nothing can be so certain.

Let's have a look at how Laplace in the 18th century solves this
problem. Let $x_t$ be an random variable such that

$$
x_t = \begin{cases}
1, \quad\text{the sun rise in day } t \text{ with probability }\theta\\
0, \quad\text{otherwise}
\end{cases}
$$

In other words, $x_t$ follows a Bernoulli distribution
$x_t\sim\text{Bern}(\theta)$. There is an unknown parameter $\theta$,
which is our goal to estimate. Before we have observed any data, we have
no knowledge about this $\theta$. We assume it is distributed uniformly,
$\theta\sim\text{Unif}(0,1)$. That is, it can be any value between $0$
and $1$.

Suppose we have observed the data for $n$ days: $x_1,x_2,\dots,x_n$.
Assume these events are $i.i.d$. Define $S_n$ as the total number of
sunrises that had happened:

$$
S_n = x_1 + x_2 + \dots + x_n
$$

We know $S_n$ follows a Binomial distribution
$S_n\sim\text{Bin}(n,\theta)$, with the probability mass function

$$
P(S_n = k | \theta) = \binom{n}{k}\theta^k (1-\theta)^{n-k}
$$

Our goal is to find: $\theta | S_n=?$ An estimation of the probability
of a sunrise after observing the data.

Recall that the Bayesian rule allows us to invert the conditional
probability:

$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$

Using this formula, we have

$$
\begin{aligned}
P(\theta|S_n=k) &=\frac{P(S_n=k|\theta) P(\theta)}{P(S_n=k)}=
\frac{P(S_n=k|\theta) P(\theta)}{\int_0^1 P(S_n=k|\theta) P(\theta)d\theta}\\[1em]
&= \frac{\binom{n}{k}\theta^k (1-\theta)^{n-k}\cdot\mathbb{I}(0\leq\theta\leq 1)}
{\int_0^1\binom{n}{k}\theta^k (1-\theta)^{n-k}\cdot\mathbb{I}(0\leq\theta\leq 1)d\theta}\\[1em]
&= \begin{cases}\frac{\binom{n}{k}\theta^k (1-\theta)^{n-k}}
{\int_0^1\binom{n}{k}\theta^k (1-\theta)^{n-k}d\theta}&\quad\text{if}\ 0\leq\theta\leq 1\\
0&\quad\text{otherwise}
\end{cases}
\end{aligned}
$$

If $k=n$, we have

$$
P(\theta|S_n=n) = \frac{\theta^n}{\int_0^1\theta^n d\theta}=(n+1)\theta^n
$$

for $0\leq\theta\leq 1$. Now we are ready to calculate the probability
of the sun rising tomorrow after observing $n$ sunrises:

$$
\begin{aligned}
P(x_{n+1}=1|S_n=n) &= \int_0^1 P(x_{n+1}=1|\theta)P(\theta|S_n=n)d\theta \\
&=\int_0^1 \theta\cdot(n+1)\theta^n d\theta \\
&= \frac{n+1}{n+2}
\end{aligned}
$$

As $n\to\infty$, the probability approaches $1$. Personally, I think
this is much more reasonable answer than the frequentist approach, in
which you always get probability $1$.

## The Bayesian Approach

This illustration literally shows every tenet of the Bayesian approach.
We start with an **prior distribution** about an unknown parameter
$\theta$. Because our ignorance before we see the data, we model it as a
uniform distribution. Note how this contrasts with the frequentist
approach, in which the parameter is a fixed unknown number.

When we have seen the data, we update the prior distribution with the
data. This is achieved by the Bayesian formula:

$$
\underbrace{P(\theta|\text{data})}_{\text{posterior}} = \frac{\overbrace{P(\text{data}|\theta)}^{\text{likelihood}} \times 
\overbrace{P(\theta)}^{\text{prior}}}
{\underbrace{P(\text{data})}_{\text{a scalar not depend on }\theta}}
$$

The updated distribution is called the **posterior distribution**. The
probability function of the data conditioned on the parameter is called
the **likelihood function**. The posterior distribution summarizes the
(updated) uncertainty over the value of the parameters. The more data we
collect, the less impact the prior exerts on the posterior distribution.

## Frequentist vs Bayesian 

Frequentists and Bayesians hold different philosophy about statistics.
Frequentists view our sample as the result of one of an infinite number
of exactly repeated experiments. The data are **randomly sampled** from
a fixed population distribution. The unknown parameters are properties
of the population, and therefore are fixed. The purpose of statistics is
to make inference about the population parameters (the ultimate truth)
with limited samples. The uncertainty associated with this process
arises from sampling. Because we do not have the entire population, each
sample only tells partial truth about the population. Therefore our
inference about the parameters can never be perfect due to sampling
errors. Frequentists conduct hypothesis tests assuming a hypothesis
(about the population parameter) is true and calculating the probability
of obtaining the observed sample data.

In Bayesians' world view, probability is an expression of **subjective
beliefs** (a measure of certainty in a belief), which can be updated in
light of new data. Parameters are probabilistic rather than fixed, which
reflects the uncertainties about the parameters. The essence of Bayesian
inference is to update the probability of a 'hypothesis' given the data
we have obtained. The Bayes' rule is all we need. All information is
summarized in the posterior probability and there is not need for
explicit hypothesis testing.

| Frequentist                                 | Bayesian                        |
|---------------------------------------------|---------------------------------|
| Probability is the limit of frequency       | Probability is uncertainty      |
| Parameters are fixed unknown numbers        | Parameters are random variables |
| Data is a random sample from the population | Data is fixed/given             |
| LLN/CLT                                     | Bayes' rule                     |

In time series analysis, there are good reasons to be Bayesian. Perhaps
the frequentist perspective makes sense in a cross section, where it is
intuitive to image taking different samples from the population.
However, in time series we have only one realization. It is difficult to
imagine where we would obtain another sample. It is more natural to take
a Bayesian perspective. For example, we have some prior belief on how
inflation and unemployment might be related (the Phillips curve), then
we update our belief with data.

Frequentists often criticize Bayesians' priors as entirely subjective.
Bayesians would respond that frequentists also have prior assumptions
that they are not even aware of. Frequentist inference utilizes the LLN
and CLT, which inevitably assumes the speed of convergence. In settings
like VAR models, where there are a large number of parameters to
estimate but only a limited amount of observations. Are the
asymptotically properties really plausible? Bayesians believe it would
be better to make our assumptions explicit.

Apart from the philosophical difference, in practice Frequentists and
Bayesians might well give similar results (though the results should be
interpreted differently). After all, if the data is plenty, the
influence of priors would diminish to zero.
